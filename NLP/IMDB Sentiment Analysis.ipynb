{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDB Sentiment Analysis","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5Egv6m4MBnSy","colab_type":"code","colab":{}},"source":["import os\n","import cv2\n","import time\n","import numpy as np\n","import random\n","from string import punctuation\n","from collections import Counter\n","from statistics import mean, stdev\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from PIL import Image\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3HBXVkNmMZ2","colab_type":"text"},"source":["###Download the IMDB Reviews dataset"]},{"cell_type":"code","metadata":{"id":"8X7emYdr_WTl","colab_type":"code","colab":{}},"source":["%%capture\n","if not os.path.isdir('aclImdb'):\n","    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","    !tar -xvf 'aclImdb_v1.tar.gz'\n","    !sudo rm -r 'aclImdb_v1.tar.gz'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfgvQaUNErPL","colab_type":"text"},"source":["### The Readme"]},{"cell_type":"code","metadata":{"id":"fysvv80ZDl0y","colab_type":"code","outputId":"d0615cb6-8e4b-4435-e18b-cf74dcf20a67","executionInfo":{"status":"ok","timestamp":1587370459074,"user_tz":-330,"elapsed":2519,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["with open('aclImdb/README', 'r') as f:\n","    readme = f.read()\n","    print(readme)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Large Movie Review Dataset v1.0\n","\n","Overview\n","\n","This dataset contains movie reviews along with their associated binary\n","sentiment polarity labels. It is intended to serve as a benchmark for\n","sentiment classification. This document outlines how the dataset was\n","gathered, and how to use the files provided. \n","\n","Dataset \n","\n","The core dataset contains 50,000 reviews split evenly into 25k train\n","and 25k test sets. The overall distribution of labels is balanced (25k\n","pos and 25k neg). We also include an additional 50,000 unlabeled\n","documents for unsupervised learning. \n","\n","In the entire collection, no more than 30 reviews are allowed for any\n","given movie because reviews for the same movie tend to have correlated\n","ratings. Further, the train and test sets contain a disjoint set of\n","movies, so no significant performance is obtained by memorizing\n","movie-unique terms and their associated with observed labels.  In the\n","labeled train/test sets, a negative review has a score <= 4 out of 10,\n","and a positive review has a score >= 7 out of 10. Thus reviews with\n","more neutral ratings are not included in the train/test sets. In the\n","unsupervised set, reviews of any rating are included and there are an\n","even number of reviews > 5 and <= 5.\n","\n","Files\n","\n","There are two top-level directories [train/, test/] corresponding to\n","the training and test sets. Each contains [pos/, neg/] directories for\n","the reviews with binary labels positive and negative. Within these\n","directories, reviews are stored in text files named following the\n","convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n","the star rating for that review on a 1-10 scale. For example, the file\n","[test/pos/200_8.txt] is the text for a positive-labeled test set\n","example with unique id 200 and star rating 8/10 from IMDb. The\n","[train/unsup/] directory has 0 for all ratings because the ratings are\n","omitted for this portion of the dataset.\n","\n","We also include the IMDb URLs for each review in a separate\n","[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n","have its URL on line 200 of this file. Due the ever-changing IMDb, we\n","are unable to link directly to the review, but only to the movie's\n","review page.\n","\n","In addition to the review text files, we include already-tokenized bag\n","of words (BoW) features that were used in our experiments. These \n","are stored in .feat files in the train/test directories. Each .feat\n","file is in LIBSVM format, an ascii sparse-vector format for labeled\n","data.  The feature indices in these files start from 0, and the text\n","tokens corresponding to a feature index is found in [imdb.vocab]. So a\n","line with 0:7 in a .feat file means the first word in [imdb.vocab]\n","(the) appears 7 times in that review.\n","\n","LIBSVM page for details on .feat file format:\n","http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n","\n","We also include [imdbEr.txt] which contains the expected rating for\n","each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n","rating is a good way to get a sense for the average polarity of a word\n","in the dataset.\n","\n","Citing the dataset\n","\n","When using this dataset please cite our ACL 2011 paper which\n","introduces it. This paper also contains classification results which\n","you may want to compare against.\n","\n","\n","@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n","  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n","  title     = {Learning Word Vectors for Sentiment Analysis},\n","  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n","  month     = {June},\n","  year      = {2011},\n","  address   = {Portland, Oregon, USA},\n","  publisher = {Association for Computational Linguistics},\n","  pages     = {142--150},\n","  url       = {http://www.aclweb.org/anthology/P11-1015}\n","}\n","\n","References\n","\n","Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n","David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n","636-659.\n","\n","Contact\n","\n","For questions/comments/corrections please contact Andrew Maas\n","amaas@cs.stanford.edu\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IY3nftn7FYNW","colab_type":"text"},"source":["###Dataset Generation and preprocessing"]},{"cell_type":"code","metadata":{"id":"uZtbuJ-LQ40o","colab_type":"code","colab":{}},"source":["class dataset_from_directory(Dataset):\n","    \n","    def __init__(self,root_dir, seq_length = 250):\n","        \n","        self.reviews = []\n","        self.labels = []\n","\n","        for filename in os.listdir(os.path.join(root_dir,'pos')):\n","            review = open(os.path.join(os.path.join(root_dir,'pos'),filename), 'r').read().lower().replace('<br />',' ')\n","            review = \"\".join([char for char in review if char not in punctuation])\n","            self.reviews.append(review)\n","            self.labels.append(1)\n","\n","        for filename in os.listdir(os.path.join(root_dir,'neg')):\n","            review = open(os.path.join(os.path.join(root_dir,'neg'),filename), 'r').read().lower().replace('<br />',' ')\n","            review = \"\".join([char for char in review if char not in punctuation])\n","            self.reviews.append(review)\n","            self.labels.append(0)\n","\n","        # Tokenize â€” Create Vocab to Int mapping dictionary\n","        words = ' '.join(self.reviews).split()\n","        self.count_words = Counter(words)\n","        total_words = len(words)\n","        self.sorted_words = self.count_words.most_common(total_words)\n","\n","        self.tokens = {word:i+1 for i, (word,count) in enumerate(self.sorted_words)}\n","\n","        # Encode the words using tokens\n","        self.reviews_tokenized = []\n","        for review in self.reviews:\n","            token = [self.tokens[word] for word in review.split()]\n","            self.reviews_tokenized.append(token)\n","\n","        self.reviews_len = [len(review) for review in self.reviews_tokenized]\n","\n","        # Removing Outliers (getting rid of reviews with lenth 0)\n","        self.reviews_tokenized = [self.reviews_tokenized[i] for i, l in enumerate(self.reviews_len) if l>0]\n","        self.labels = [self.labels[i] for i, l in enumerate(self.reviews_len) if l>0]\n","\n","        # To deal with both short and long reviews, we will pad or truncate all our reviews to a specific length.\n","        self.reviews_tokenized = self.pad_features(self.reviews_tokenized, seq_length)\n","\n","    def pad_features(self,reviews_int, seq_length):\n","    # Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n","    \n","        features = np.zeros((len(reviews_int), seq_length), dtype = int)\n","        \n","        for i, review in enumerate(reviews_int):\n","            review_len = len(review)\n","            \n","            if review_len <= seq_length:\n","                zeroes = list(np.zeros(seq_length-review_len))\n","                padded_review = zeroes+review\n","            elif review_len > seq_length:\n","                padded_review = review[0:seq_length]\n","            \n","            features[i,:] = np.array(padded_review)\n","        return features\n","    \n","    def __len__(self):\n","        return len(self.reviews_tokenized)\n","    \n","    def __getitem__(self,index):  \n","        return self.reviews_tokenized[index], np.array(self.labels[index])\n","\n","    def word_count(self):\n","        print(self.count_words)  \n","\n","    def vocab_to_int(self):\n","        return self.tokens\n","\n","    def get_reviews(self,index):\n","        return self.reviews[index] \n","\n","    def get_vocab_len(self):\n","        return len(self.count_words)\n","\n","    def analyze_reviews(self):\n","        print('Max: {}'.format(max(self.reviews_len)))\n","        print('Min: {}'.format(min(self.reviews_len)))\n","        print('Mean: {}'.format(mean(self.reviews_len)))\n","        print('Std Dev: {}'.format(stdev(self.reviews_len)))\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RvlMrv7U6uw","colab_type":"code","outputId":"693c30a1-0fe9-4a14-85a0-c1e24578d18e","executionInfo":{"status":"ok","timestamp":1587370466701,"user_tz":-330,"elapsed":10030,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["train_dataset = dataset_from_directory('aclImdb/train', seq_length = 250)\n","\n","print('Total number of Reviews: {}'.format(len(train_dataset)))\n","print('Number of Positives: {}'.format(train_dataset.labels.count(0)))\n","print('Number of Negatives: {}'.format(train_dataset.labels.count(1)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Total number of Reviews: 25000\n","Number of Positives: 12500\n","Number of Negatives: 12500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1bMCyyYeSqPI","colab_type":"code","outputId":"ccad1122-a8d0-40d7-ac17-4fe2c0af4af7","executionInfo":{"status":"ok","timestamp":1587370466704,"user_tz":-330,"elapsed":8886,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["review, label = train_dataset[0:5]\n","print(review)\n","print(label)\n","print(review.shape)\n","print(label.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[    0     0     0 ...   592  1951    94]\n"," [    0     0     0 ...   408    10    27]\n"," [    0     0     0 ...  7783  1100   183]\n"," [    0     0     0 ...    44    46 11227]\n"," [    0     0     0 ...     2 13364  3449]]\n","[1 1 1 1 1]\n","(5, 250)\n","(5,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u6dWeWcbXPdw","colab_type":"code","outputId":"e39863e3-d50f-4101-a553-051c4c423fad","executionInfo":{"status":"ok","timestamp":1587370466705,"user_tz":-330,"elapsed":8500,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["train_dataset.get_reviews(2)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'this first part of the brd trilogy has more passion and plot density than lola but less of the magic of veronica voss the political musings have point to them we see the shortages after the war how the blackmarketers were able to control so much of the daytoday life delicious moment when fassbinder playing a grifter tries to sell a complete set of kleist to schygulla who remarks that burning books dont provide much warmth she really wants firewood  theres some clumsiness in the first hour the scene in marias room with the black soldier interrupted by hermanns appearance should go quicker the train scene when maria meets karl oswald falls flat when she insults the gii cringed it was so bad but as the story develops and the years go by i was drawn more and more into this glossy cold world'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"F9Hv82DPpehm","colab_type":"code","colab":{}},"source":["vocab_to_int = train_dataset.vocab_to_int()\n","vocab_len = train_dataset.get_vocab_len()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5-AIDHbnz8N","colab_type":"code","outputId":"9ccc6f77-6610-4f74-dd70-0d5f25551a87","executionInfo":{"status":"ok","timestamp":1587370466707,"user_tz":-330,"elapsed":7430,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["train_dataset.analyze_reviews()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Max: 2459\n","Min: 10\n","Mean: 230.58692\n","Std Dev: 171.33392780621796\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IzNWw6JvlMIr","colab_type":"code","outputId":"e2deb453-d882-441a-acc0-8f7d46696e9c","executionInfo":{"status":"ok","timestamp":1587370466708,"user_tz":-330,"elapsed":6770,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["len_valid_set = int(0.2*len(train_dataset))\n","len_train_set = len(train_dataset) - len_valid_set\n","\n","print(\"The length of Train set is {}\".format(len_train_set))\n","print(\"The length of Valid set is {}\".format(len_valid_set))\n","\n","train_dataset , valid_dataset = torch.utils.data.random_split(train_dataset, [len_train_set, len_valid_set])  \n","\n","batch_size = 50\n","\n","# shuffle and batch the datasets\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size,shuffle=True, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size,shuffle=True, drop_last=True)\n","\n","reviews, labels = next(iter(train_loader))\n","\n","print(reviews.shape)\n","print(labels.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["The length of Train set is 20000\n","The length of Valid set is 5000\n","torch.Size([50, 250])\n","torch.Size([50])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yczpOpoiF1Gl","colab_type":"code","outputId":"d69d3326-65e3-46b2-878d-5fffea591939","executionInfo":{"status":"ok","timestamp":1587370466709,"user_tz":-330,"elapsed":6448,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["reviews, labels = next(iter(valid_loader))\n","\n","print(reviews.shape)\n","print(labels.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["torch.Size([50, 250])\n","torch.Size([50])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LiRXajeBoFNr","colab_type":"text"},"source":["###Creating LSTM Network"]},{"cell_type":"code","metadata":{"id":"U3BR7LGMoC7I","colab_type":"code","colab":{}},"source":["class SentimentLSTM(nn.Module):\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers, drop_prob=0.5):\n","        super().__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=drop_prob, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.5)\n","        \n","        # linear and sigmoid layers\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","\n","    def forward(self, x, h):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # embeddings and lstm_out\n","        x = self.embedding(x)\n","\n","        lstm_out, h = self.lstm(x, h)   # lstm_out: [batch, len_review, hidden_dim]\n","    \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)    # [batch * len_review, hidden_dim]\n","\n","        # dropout and fully-connected layer\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)                   # [batch * len_review, 1]\n","        sig_out = self.sigmoid(out)\n","\n","        # reshape to be batch_size first\n","        sig_out = sig_out.view(batch_size, -1)  # [batch, len_review]\n","\n","        # get the last prediction for the complete batch\n","        pred = sig_out[:, -1]                   # [batch]\n","\n","        return pred, h\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcK7O297qyWW","colab_type":"code","outputId":"7596207d-810a-452f-b940-43b3aefcd050","executionInfo":{"status":"ok","timestamp":1587370469401,"user_tz":-330,"elapsed":7850,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["vocab_size = vocab_len + 1 # +1 for the 0 padding\n","output_size = 1\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 1\n","\n","network = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).cuda()\n","print(network)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["SentimentLSTM(\n","  (embedding): Embedding(112012, 400)\n","  (lstm): LSTM(400, 256, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=256, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dS8yuuVmjMDz","colab_type":"code","colab":{}},"source":["def find_acc(pred, label):\n","    pred = torch.round(pred.squeeze())  # rounds to the nearest integer\n","    correct = pred.eq(label.view_as(pred))\n","    accuracy = correct.to(torch.float32).mean().item()\n","    return accuracy * 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CV0LCToYJp94","colab_type":"code","outputId":"ec2f8052-ebbb-44bb-a661-03e75f2a5f0e","executionInfo":{"status":"error","timestamp":1587370679219,"user_tz":-330,"elapsed":216488,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":523}},"source":["num_epochs = 10\n","clip=5 # gradient clipping\n","loss_min = np.inf\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n","\n","for epoch in range(1,num_epochs+1):\n","    \n","    loss_train = 0\n","    loss_valid = 0\n","    running_loss = 0\n","    \n","    # set the network into train mode\n","    network.train()\n","    h = network.init_hidden(batch_size)\n","\n","    for step in range(1,len(train_loader)+1):\n","    \n","        reviews, labels = next(iter(train_loader))\n","\n","        reviews = reviews.cuda()\n","        labels = labels.cuda()\n","\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        network.zero_grad()\n","\n","        # get the output from the model\n","        output, h = network(reviews,h)\n","\n","        # calculate the loss and perform backprop\n","        loss_train_step = criterion(output.squeeze(), labels.float())\n","        loss_train_step.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(network.parameters(), clip)\n","\n","        # Update the parameters\n","        optimizer.step()\n","        \n","        loss_train += loss_train_step.item()\n","        \n","    network.eval() \n","    val_h = network.init_hidden(batch_size)\n","    \n","    # turn the gradients off for validation\n","    with torch.no_grad():\n","        \n","        for step in range(1,len(valid_loader)+1):\n","            \n","            reviews, labels = next(iter(valid_loader))\n","\n","            reviews = reviews.cuda()\n","            labels = labels.cuda()\n","\n","            val_h = tuple([each.data for each in val_h])\n","\n","            output, val_h = network(reviews,val_h)\n","\n","            loss_valid_step = criterion(output.squeeze(), labels.float())\n","\n","            loss_valid += loss_valid_step.item()\n","    \n","    loss_train /= len(train_loader)\n","    loss_valid /= len(valid_loader)\n","    \n","    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n","    \n","    if loss_valid < loss_min:\n","        loss_min = loss_valid\n","        torch.save(network.state_dict(), 'sentiment_analysis.pth') \n","        print(\"\\nMinimum validation loss of {} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n","        print('Model Saved\\n')\n","\n","print('Training Complete')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch: 1  Train Loss: 0.5801  Valid Loss: 0.5496\n","\n","Minimum validation loss of 0.549594295322895 at epoch 1/10\n","Model Saved\n","\n","Epoch: 2  Train Loss: 0.3960  Valid Loss: 0.4762\n","\n","Minimum validation loss of 0.4762261989712715 at epoch 2/10\n","Model Saved\n","\n","Epoch: 3  Train Loss: 0.2520  Valid Loss: 0.4552\n","\n","Minimum validation loss of 0.4552463109791279 at epoch 3/10\n","Model Saved\n","\n","Epoch: 4  Train Loss: 0.1509  Valid Loss: 0.5217\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-1b946c0afdcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_train_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"V33D1tc6j1La","colab_type":"text"},"source":["## Load the best network"]},{"cell_type":"code","metadata":{"id":"zo6Vpe7idsh1","colab_type":"code","outputId":"b3f352af-0877-4180-ffd8-e89b3a24a772","executionInfo":{"status":"ok","timestamp":1587370685893,"user_tz":-330,"elapsed":2212,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["best_network = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).cuda()\n","best_network.load_state_dict(torch.load('sentiment_analysis.pth')) "],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"qKEzJfxqj5Z5","colab_type":"text"},"source":["## Test on valid data"]},{"cell_type":"code","metadata":{"id":"G3uEJ-iY6DgK","colab_type":"code","outputId":"b295611a-06ef-446e-aea7-804542f85d21","executionInfo":{"status":"ok","timestamp":1587370692125,"user_tz":-330,"elapsed":4339,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["losses = []\n","accuracies = []\n","\n","best_network.eval() \n","h = best_network.init_hidden(batch_size)\n","\n","# turn the gradients off for validation\n","for reviews, labels in valid_loader:\n","\n","    reviews = reviews.cuda()\n","    labels = labels.cuda()\n","\n","    h = tuple([each.data for each in h])\n","\n","    output, h = best_network(reviews,h)\n","\n","    loss = criterion(output, labels.float())\n","    acc = find_acc(output, labels.float())\n","\n","    losses.append(loss.item())\n","    accuracies.append(acc)\n","\n","print('Loss: {:.4f}  Accuracy: {:.4f}'.format(np.mean(losses), np.mean(accuracies)))\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Loss: 0.4564  Accuracy: 82.1400\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4x7WgiK3lJ8o","colab_type":"text"},"source":["## Test on custom reviews"]},{"cell_type":"code","metadata":{"id":"xcrLL0YZYvPo","colab_type":"code","colab":{}},"source":["def tokenize_review(review):\n","    # lowercase\n","    review = review.lower() \n","\n","    # scan the review char by char and get rid of punctuations\n","    text = ''.join([char for char in review if char not in punctuation])\n","    \n","    # splitting by spaces to get words\n","    words = text.split()\n","    \n","    # Encode the review\n","    encoded_review = []\n","    encoded_review.append([vocab_to_int[word] for word in words])\n","    \n","    return encoded_review\n","\n","def pad_features(encoded_reviews, seq_length):\n","        features = np.zeros((len(encoded_reviews), seq_length), dtype = int)\n","        for i, review in enumerate(encoded_reviews):\n","            review_len = len(review)\n","            \n","            if review_len <= seq_length:\n","                zeroes = list(np.zeros(seq_length-review_len))\n","                padded_review = zeroes + review\n","            elif review_len > seq_length:\n","                padded_review = review[0:seq_length]\n","            \n","            features[i,:] = np.array(padded_review)\n","        return features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"axUZJ4mCpmw6","colab_type":"code","colab":{}},"source":["def predict(network, review, seq_length=250):\n","    ''' Prints out whether a give review is predicted to be \n","        positive or negative in sentiment, using a trained model.\n","        \n","        review - a review made of normal text and punctuation\n","        sequence_length - the padded length of a review\n","        '''\n","    \n","    network.eval()\n","    \n","    encoded_review = tokenize_review(review)\n","\n","    # Pad the review\n","    features = torch.from_numpy(pad_features(encoded_review, seq_length)).cuda()\n","    \n","    batch_size = features.size(0)\n","    \n","    # initialize hidden state\n","    h = network.init_hidden(batch_size)\n","      \n","    # get the output from the model\n","    output, h = network(features, h)\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze()).item()\n","    \n","    # print custom response based on whether test_review is pos/neg\n","    if(pred==1):\n","        print('Positive Review: Confidence: {:.3f} %'.format((output.item())*100))\n","    else:\n","        print('Negative Review: Confidence: {:.3f} %'.format((1-output.item())*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIB-pSP1pmxB","colab_type":"code","outputId":"d4a53d7b-2b71-4a0e-cb06-ee3aaa082f9c","executionInfo":{"status":"ok","timestamp":1587370944208,"user_tz":-330,"elapsed":2731,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["review = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n","predict(best_network, review, seq_length = 250)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Negative Review: Confidence: 99.371 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VKIIMmDVc-9F","colab_type":"code","outputId":"7b4fb230-793b-4947-d5bf-f7cea4e6eca5","executionInfo":{"status":"ok","timestamp":1587370944209,"user_tz":-330,"elapsed":1775,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["review = 'I did not like the ending. The ending was bad, but overall the movie was amazing.'\n","predict(best_network, review, seq_length = 250)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Positive Review: Confidence: 87.453 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"avxabTdorlFf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"661a6163-2f68-455e-c880-ebeaeea52ec1","executionInfo":{"status":"ok","timestamp":1587371352306,"user_tz":-330,"elapsed":1416,"user":{"displayName":"abdur rahman","photoUrl":"","userId":"00136007448438231118"}}},"source":["review = 'I have no words to say. It was amazing.'\n","predict(best_network, review, seq_length = 250)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Positive Review: Confidence: 90.586 %\n"],"name":"stdout"}]}]}