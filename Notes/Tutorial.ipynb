{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"19HvFXDmErF1","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFcWXGu5FBWf","colab_type":"text"},"source":["### # Define a tensor"]},{"cell_type":"code","metadata":{"id":"ldT55kXQEVON","colab_type":"code","outputId":"175102ef-8ba4-481f-dc61-e118d547a4cf","executionInfo":{"status":"ok","timestamp":1561179806143,"user_tz":-330,"elapsed":2632,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["t = torch.Tensor()\n","\n","print(t)\n","print(type(t))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor([])\n","<class 'torch.Tensor'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oC73rMWGE-mR","colab_type":"text"},"source":["This creates an empty tensor (tensor with no data), but we'll get to adding data in just a moment.\n","\n","### Tensor attributes\n","First, letâ€™s look at a few tensor attributes. Every torch.Tensor has these attributes:\n","\n","* torch.dtype\n","* torch.device\n","* torch.layout (specifies how tensors are stored in the memory)"]},{"cell_type":"code","metadata":{"id":"6nkGeh6mEnEk","colab_type":"code","outputId":"74c6edf6-5913-4a00-b779-e6900bda326f","executionInfo":{"status":"ok","timestamp":1561179806145,"user_tz":-330,"elapsed":2616,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["print(t.dtype)\n","print(t.device)\n","print(t.layout)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["torch.float32\n","cpu\n","torch.strided\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eBmeBl_dFtCi","colab_type":"text"},"source":["One thing to keep in mind about tensor data types is that tensor operations between tensors must happen between tensors with the same type of data.\n","\n","### Tensors have a torch.device\n","The device, cpu in our case, specifies the device (CPU or GPU) where the tensor's data is allocated. This determines where tensor computations for the given tensor will be performed.\n","\n","**Note:** One thing to keep in mind about using multiple devices is that tensor operations between tensors must happen between tensors that exists on the same device."]},{"cell_type":"code","metadata":{"id":"BALnBpItF6za","colab_type":"code","outputId":"57b0be1f-b810-4f47-dfe9-aae1e0efe649","executionInfo":{"status":"ok","timestamp":1561179806147,"user_tz":-330,"elapsed":2589,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["device = torch.device('cuda:0')\n","print(device)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AaolUDBoI1vN","colab_type":"text"},"source":["## Creating tensors using data\n","These are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch:"]},{"cell_type":"code","metadata":{"id":"k0O1KCwAI3dD","colab_type":"code","outputId":"d89c7f25-b706-4e9e-ff82-12074b1e2b46","executionInfo":{"status":"ok","timestamp":1561179806147,"user_tz":-330,"elapsed":2571,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["data = np.array([1,2,3])\n","type(data)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Y4NdtJiDJJHh","colab_type":"code","outputId":"c898d502-8755-4237-c18e-62156bafa9af","executionInfo":{"status":"ok","timestamp":1561179806148,"user_tz":-330,"elapsed":2551,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# creating tensors using constructors\n","torch.Tensor(data)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 2., 3.])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"C-8OsKYVLHYo","colab_type":"text"},"source":["### # creating tensors using factory function"]},{"cell_type":"code","metadata":{"id":"pM_vs_5dJRnf","colab_type":"code","outputId":"5be860ed-20c3-4af6-b606-028747e14679","executionInfo":{"status":"ok","timestamp":1561179806149,"user_tz":-330,"elapsed":2535,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.tensor(data)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"eemaLWNRJUQL","colab_type":"code","outputId":"945391b8-57d5-484a-d917-b6861e80d31f","executionInfo":{"status":"ok","timestamp":1561179806150,"user_tz":-330,"elapsed":2521,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.as_tensor(data)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"-LE-O9zfJWdR","colab_type":"code","outputId":"6af5921c-57e3-4146-9ea7-2689622674bb","executionInfo":{"status":"ok","timestamp":1561179806152,"user_tz":-330,"elapsed":2503,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.from_numpy(data)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"nnFzpgJ9JzYP","colab_type":"text"},"source":["### Identity tensor"]},{"cell_type":"code","metadata":{"id":"0nXcC-6LJy19","colab_type":"code","outputId":"5ba89958-2a63-4198-a015-58fa7b4963bd","executionInfo":{"status":"ok","timestamp":1561179806154,"user_tz":-330,"elapsed":2485,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["torch.eye(3)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"hCuJwTbNJ-b_","colab_type":"text"},"source":["### Other tensors"]},{"cell_type":"code","metadata":{"id":"3BX57J2bKA1h","colab_type":"code","outputId":"cbc7807e-9ee8-4bd3-8e74-a27cf3c062fe","executionInfo":{"status":"ok","timestamp":1561179806156,"user_tz":-330,"elapsed":2466,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["torch.zeros(3,4)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"fPWfiEZbKOOd","colab_type":"code","outputId":"e94b584a-2536-4c18-dbf5-29368739cc10","executionInfo":{"status":"ok","timestamp":1561179806157,"user_tz":-330,"elapsed":2448,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["torch.ones(4,3)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"gLAHGlndKRbW","colab_type":"code","outputId":"0f317f49-e8ce-4e35-ab53-6d5f34789064","executionInfo":{"status":"ok","timestamp":1561179806158,"user_tz":-330,"elapsed":2430,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.rand(3)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.6192, 0.1051, 0.7069])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"cZa_9FDrLehF","colab_type":"text"},"source":["# Creating tensors from data"]},{"cell_type":"code","metadata":{"id":"_QXkM2ROLkzw","colab_type":"code","outputId":"b9ff11c6-1f9a-4d28-e971-7abe9abe223c","executionInfo":{"status":"ok","timestamp":1561179806162,"user_tz":-330,"elapsed":2418,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.tensor(np.array([1,2,3]))"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"JJkePKaRLryC","colab_type":"code","outputId":"e145953d-ee81-4844-c0b6-7b9a5349ed7b","executionInfo":{"status":"ok","timestamp":1561179806163,"user_tz":-330,"elapsed":2397,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.tensor(np.array([1.,2.,3.]))"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 2., 3.], dtype=torch.float64)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"qgyE9nh-LxTL","colab_type":"code","outputId":"7194ed93-f27d-4bb3-be5c-4a515cfca729","executionInfo":{"status":"ok","timestamp":1561179806165,"user_tz":-330,"elapsed":2379,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["torch.tensor(np.array([1,2,3]), dtype = torch.float32)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 2., 3.])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"2jdLDUZhM48F","colab_type":"text"},"source":["## Memory Sharing and Copying between the data and the tensor"]},{"cell_type":"code","metadata":{"id":"yHj2IIH5M4mT","colab_type":"code","outputId":"25d15647-bd07-4718-93db-fdbc23f9c930","executionInfo":{"status":"ok","timestamp":1561179806166,"user_tz":-330,"elapsed":2364,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["data = np.array([1,2,3])\n","data"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"RHEXKyeUNHXA","colab_type":"code","colab":{}},"source":["t1 = torch.Tensor(data)\n","t2 = torch.tensor(data)\n","t3 = torch.as_tensor(data)\n","t4 = torch.from_numpy(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTa_hm4JOZvl","colab_type":"text"},"source":["Modifying the  data but not the tensor"]},{"cell_type":"code","metadata":{"id":"b_1KseCMNxjN","colab_type":"code","colab":{}},"source":["data[0] = 0\n","data[1] = 0\n","data[2] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5c9z294yN01K","colab_type":"code","outputId":"bb78ffc9-5730-4674-d7d7-91c1a7b5c647","executionInfo":{"status":"ok","timestamp":1561179806175,"user_tz":-330,"elapsed":2356,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["print(t1)\n","print(t2)\n","print(t3)\n","print(t4)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([1., 2., 3.])\n","tensor([1, 2, 3])\n","tensor([0, 0, 0])\n","tensor([0, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zb2A9sPYOnss","colab_type":"text"},"source":["This concludes that 1st two methods create a copy of the data, whereas the last two methods share the same memory for both data and tensor which means the code will run faster since no copying is needed.\n","\n","## Flatten, Reshape and Squeeze Explained"]},{"cell_type":"code","metadata":{"id":"o7hAk__aPyYB","colab_type":"code","colab":{}},"source":["t = torch.tensor([\n","    [1,1,1,1],\n","    [2,2,2,2],\n","    [3,3,3,3]\n","], dtype=torch.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-SQ2b_WRAYK","colab_type":"code","outputId":"e64e1d02-d252-4e4b-ff2b-c2e05f8d2714","executionInfo":{"status":"ok","timestamp":1561179806180,"user_tz":-330,"elapsed":2342,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["t.shape"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 4])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"w6MH0dc_RDcc","colab_type":"code","outputId":"3e5483e7-a074-4642-f25d-33d9563a3313","executionInfo":{"status":"ok","timestamp":1561179806181,"user_tz":-330,"elapsed":2327,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["t.size()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 4])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"huCGcIqPRNER","colab_type":"code","outputId":"85e677ba-8292-4158-852f-af66ccdc4a9c","executionInfo":{"status":"ok","timestamp":1561179806181,"user_tz":-330,"elapsed":2312,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# find number of elements in the tensor\n","t.numel()"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"UUxy7mtkRVUd","colab_type":"code","outputId":"5faff39e-9ac1-416b-f812-d91dad2e7b0e","executionInfo":{"status":"ok","timestamp":1561179806182,"user_tz":-330,"elapsed":2298,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# find no. of axes\n","len(t.shape)"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"oeQalBvzRn5d","colab_type":"code","outputId":"640eb320-a310-465c-d6c9-467f4fbb1cbe","executionInfo":{"status":"ok","timestamp":1561179806184,"user_tz":-330,"elapsed":2285,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["t.reshape(-1)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"nJ3LVRQsRsro","colab_type":"code","outputId":"b5ae98eb-d0f0-4170-b0ee-0d62b27c36c4","executionInfo":{"status":"ok","timestamp":1561179806185,"user_tz":-330,"elapsed":2275,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["t.reshape(-1,2)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1.],\n","        [1., 1.],\n","        [2., 2.],\n","        [2., 2.],\n","        [3., 3.],\n","        [3., 3.]])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"czsw9A4uR6Y0","colab_type":"code","outputId":"bb252e33-613e-4cb3-e69a-9908d9ad1bf6","executionInfo":{"status":"ok","timestamp":1561179806186,"user_tz":-330,"elapsed":2265,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["t.reshape(2,2,3)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1.],\n","         [1., 2., 2.]],\n","\n","        [[2., 2., 3.],\n","         [3., 3., 3.]]])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"VsgFdW3bSAFh","colab_type":"code","outputId":"bf291e54-6487-4ae9-8d85-e138d37c6c15","executionInfo":{"status":"ok","timestamp":1561179806187,"user_tz":-330,"elapsed":2250,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# squeeze removes the axis with length of 1\n","t.reshape(1,12).squeeze()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"kRAJkEClSR-w","colab_type":"code","outputId":"75312fab-11a6-4dab-ecc7-38b4e9a6b5d9","executionInfo":{"status":"ok","timestamp":1561179806188,"user_tz":-330,"elapsed":2237,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# squeeze adds an axis with length of 1 along the specified dimension\n","t.reshape(12).unsqueeze(dim = 0)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"zDYWbFpsSZv5","colab_type":"code","outputId":"6e779d74-25cc-42cc-9bda-48adec4c372d","executionInfo":{"status":"ok","timestamp":1561179806189,"user_tz":-330,"elapsed":2227,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["# squeeze adds an axis with length of 1 along the specified dimension\n","t.reshape(12).unsqueeze(dim = 1)"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [2.],\n","        [2.],\n","        [2.],\n","        [2.],\n","        [3.],\n","        [3.],\n","        [3.],\n","        [3.]])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"mvPxX_-sVbtf","colab_type":"code","outputId":"f51497e3-40dd-4061-a54a-92c06d77c5d4","executionInfo":{"status":"ok","timestamp":1561179806193,"user_tz":-330,"elapsed":2219,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["t.flatten()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"z3evupJjV2nt","colab_type":"code","outputId":"80409902-335f-4c11-c081-d8a051de9ad4","executionInfo":{"status":"ok","timestamp":1561179806194,"user_tz":-330,"elapsed":2208,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# concatenating\n","\n","a = torch.tensor([\n","    [1,2,3],\n","    [1,2,3]\n","])\n","\n","b = torch.tensor([\n","    [4,5,6],\n","    [4,5,6]\n","])\n","\n","torch.cat((a,b), dim = 0)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [1, 2, 3],\n","        [4, 5, 6],\n","        [4, 5, 6]])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"bnwmpPk8Wam5","colab_type":"code","outputId":"0c654acc-907d-4bc5-97d6-ae0b25d18ec6","executionInfo":{"status":"ok","timestamp":1561179806195,"user_tz":-330,"elapsed":2192,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["torch.cat((a,b), dim = 1)"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3, 4, 5, 6],\n","        [1, 2, 3, 4, 5, 6]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"kDGRw8P5WwxL","colab_type":"text"},"source":["# CNN Flatten Operation Visualized - Tensor Batch Processing for Deep Learning"]},{"cell_type":"code","metadata":{"id":"LJOPowtSWyRN","colab_type":"code","outputId":"5a82c918-0cce-4786-b4d4-73b7f1173946","executionInfo":{"status":"ok","timestamp":1561179806195,"user_tz":-330,"elapsed":2176,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["a = torch.ones(3,4)\n","b = a + 1\n","c = b + 1\n","\n","print(a)\n","print(b)\n","print(c)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])\n","tensor([[2., 2., 2., 2.],\n","        [2., 2., 2., 2.],\n","        [2., 2., 2., 2.]])\n","tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jp_tlu18YIyV","colab_type":"code","outputId":"f4360c23-824a-4333-8e7e-2bdee486bae7","executionInfo":{"status":"ok","timestamp":1561179807367,"user_tz":-330,"elapsed":3337,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["# stack the tensors on top of each other\n","torch.stack([a,b,c])"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]],\n","\n","        [[2., 2., 2., 2.],\n","         [2., 2., 2., 2.],\n","         [2., 2., 2., 2.]],\n","\n","        [[3., 3., 3., 3.],\n","         [3., 3., 3., 3.],\n","         [3., 3., 3., 3.]]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"VgyUYRkMYYVQ","colab_type":"code","outputId":"7146b01e-57bb-4ce7-8216-b187e95a1d47","executionInfo":{"status":"ok","timestamp":1561179807368,"user_tz":-330,"elapsed":3326,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["# often we need to unsqueeze grayscale images before feeding them to a neural net\n","\n","torch.unsqueeze(torch.stack([a,b,c]), dim = 1)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[1., 1., 1., 1.],\n","          [1., 1., 1., 1.],\n","          [1., 1., 1., 1.]]],\n","\n","\n","        [[[2., 2., 2., 2.],\n","          [2., 2., 2., 2.],\n","          [2., 2., 2., 2.]]],\n","\n","\n","        [[[3., 3., 3., 3.],\n","          [3., 3., 3., 3.],\n","          [3., 3., 3., 3.]]]])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"Zca8LdF6Y0Gp","colab_type":"text"},"source":["Brackets are arranged in the order [batch, channel, height, width]"]},{"cell_type":"code","metadata":{"id":"IX6xtPtRZAgI","colab_type":"code","outputId":"ade46107-f551-44c4-abe6-3d9c34bd4855","executionInfo":{"status":"ok","timestamp":1561179807369,"user_tz":-330,"elapsed":3316,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["t = torch.unsqueeze(torch.stack([a,b,c]), dim = 1)\n","print(t.shape)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["torch.Size([3, 1, 3, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_WdMt5FjZ8st","colab_type":"code","outputId":"8d3dc47b-6b26-490c-a43a-41406b4a5d99","executionInfo":{"status":"ok","timestamp":1561179807370,"user_tz":-330,"elapsed":3305,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# first image in the batch\n","t[0]"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"e7R9-XRJaBFF","colab_type":"code","outputId":"4e2a34a1-527e-4ded-b3f2-ef7b0d04c1c7","executionInfo":{"status":"ok","timestamp":1561179807372,"user_tz":-330,"elapsed":3296,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# first color channel of the first image\n","t[0][0]"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"Oi-gKHCuaJZH","colab_type":"code","outputId":"bf132abd-66e8-44c7-ab5e-7b51768d993f","executionInfo":{"status":"ok","timestamp":1561179807373,"user_tz":-330,"elapsed":3285,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# first row of first color channel of the first image\n","t[0][0][0]"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1.])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"s_59sFQdaqeR","colab_type":"code","outputId":"abb9ce81-4e5d-4faf-8bfa-592a3881b77b","executionInfo":{"status":"ok","timestamp":1561179807374,"user_tz":-330,"elapsed":3276,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# when flattening images to feed to our CNN we don't want to flatten in batch axis\n","# this is specified using start_dim which tells the function which axis to start flattening \n","t.flatten(start_dim = 1)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n","        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n","        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"HM5iZSChYEoI","colab_type":"text"},"source":["# Tensor Reduction Operations\n","A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor."]},{"cell_type":"code","metadata":{"id":"OQY-6bAdS-4Y","colab_type":"code","colab":{}},"source":["t = torch.tensor([\n","    [1,2,3],\n","    [4,5,6],\n","    [7,8,9]\n","], dtype = torch.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Be6QV5WY4Mc","colab_type":"code","outputId":"3c9c814e-0172-4234-bf09-814dff105b2d","executionInfo":{"status":"ok","timestamp":1561179807378,"user_tz":-330,"elapsed":3267,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# sum of all the elements in the tensor\n","t.sum()"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(45.)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"_FiwgZ5lZByI","colab_type":"code","outputId":"4c42472e-7bcc-4010-a8ca-c398a93ee390","executionInfo":{"status":"ok","timestamp":1561179807379,"user_tz":-330,"elapsed":3256,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# product of all the elements in the tensor\n","t.prod()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(362880.)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"-DvjhWtbZGN9","colab_type":"code","outputId":"8cfcbd16-ad62-4f24-95eb-df35f389941b","executionInfo":{"status":"ok","timestamp":1561179807380,"user_tz":-330,"elapsed":3247,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# mean of all the elements in the tensor\n","t.mean()"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"TSp0TJuAZRs6","colab_type":"code","outputId":"42b208d0-faa4-4920-94de-f969a4a8a462","executionInfo":{"status":"ok","timestamp":1561179807381,"user_tz":-330,"elapsed":3236,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# standard deviation of all the elements in the tensor\n","t.std()"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.7386)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"H_LDWddOZb9G","colab_type":"code","outputId":"c92b06af-d17d-422c-f620-8cbe964bba6e","executionInfo":{"status":"ok","timestamp":1561179807382,"user_tz":-330,"elapsed":3218,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# number of elements in the tensor\n","t.numel()"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"dsCQP9IDaGwq","colab_type":"text"},"source":["All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor's elements.\n","\n","### We can select axes for reduction operation\n"]},{"cell_type":"code","metadata":{"id":"L85yCIhOaVx5","colab_type":"code","outputId":"7fa4754d-11cb-446d-9d7b-117eca2c4030","executionInfo":{"status":"ok","timestamp":1561179807383,"user_tz":-330,"elapsed":3206,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(t.mean(dim = 0))\n","print(t.mean(dim = 1))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["tensor([4., 5., 6.])\n","tensor([2., 5., 8.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cm3ncJjEajaS","colab_type":"code","outputId":"9d9a5613-e707-4ed3-9eda-0d30a9a06699","executionInfo":{"status":"ok","timestamp":1561179807383,"user_tz":-330,"elapsed":3194,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(t.sum(dim = 0))\n","print(t.sum(dim = 1))"],"execution_count":50,"outputs":[{"output_type":"stream","text":["tensor([12., 15., 18.])\n","tensor([ 6., 15., 24.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OGhnQx7gbwUd","colab_type":"code","outputId":"bc961ff9-f27d-4a25-a836-dc32feb7d528","executionInfo":{"status":"ok","timestamp":1561179807384,"user_tz":-330,"elapsed":3183,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# finding maximum value\n","t.max()"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(9.)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"O1ck46jNb0lw","colab_type":"code","outputId":"55e3c582-e4a0-4158-f943-d33d5871c7b7","executionInfo":{"status":"ok","timestamp":1561179807385,"user_tz":-330,"elapsed":3174,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# finding argmax\n","t.argmax()"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(8)"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"Er_wXPm8qAIa","colab_type":"text"},"source":["## Preparing dataset(Fashion MNIST) \n","\n","Here, we will learn ETL pipeline (extract, transform, load)"]},{"cell_type":"code","metadata":{"id":"XdHMMN1IqD_-","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VdQXZAxsW9v","colab_type":"text"},"source":["torch: The top-level PyTorch package and tensor library.\n","\n","torch.nn:\tA subpackage that contains modules and extensible classes for building neural networks.\n","\n","torch.optim:\tA subpackage that contains standard optimization operations like SGD and Adam.\n","\n","torch.nn.functional: A functional interface that contains typical operations used for building neural networks like loss functions and convolutions.\n","\n","torchvision:\tA package that provides access to popular datasets, model architectures, and image transformations for computer vision.\n","\n","torchvision.transforms:\tAn interface that contains common transforms for image processing.\n"]},{"cell_type":"code","metadata":{"id":"aChwQNwssTqL","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix\n","#from plotcm import plot_confusion_matrix\n","\n","import pdb\n","\n","torch.set_printoptions(linewidth=120)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5lCZZhLt09m","colab_type":"text"},"source":["torch.utils.data.Dataset:\tAn abstract class for representing a dataset.\n","torch.utils.data.DataLoader:\tWraps a dataset and provides access to the underlying data.\n","\n","An abstract class is a Python class that has methods we must implement, so we can create a custom dataset by creating a subclass that extends the functionality of the Dataset class.\n","\n","To create a custom dataset using PyTorch, we extend the Dataset class by creating a subclass that implements these required methods. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object."]},{"cell_type":"code","metadata":{"id":"WlfhCJISu3YV","colab_type":"code","outputId":"fb71fc53-c81f-4991-e01f-b8b7f6c99a27","executionInfo":{"status":"ok","timestamp":1561179811124,"user_tz":-330,"elapsed":6898,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":280}},"source":["# Since we want our images to be transformed into tensors,\n","# we use the built-in transforms.ToTensor() transformation, \n","# and since this dataset is going to be used for training,\n","# weâ€™ll name the instance train_set.\n","\n","# download the dataset if it doesn't exist on the disk\n","\n","train_set = torchvision.datasets.FashionMNIST(\n","    root='./data/FashionMNIST'\n","    ,train=True\n","    ,download=True\n","    ,transform=transforms.Compose([\n","        transforms.ToTensor()\n","    ])\n",")"],"execution_count":55,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["26427392it [00:01, 14401103.50it/s]                             \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 102044.28it/s]           \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["4423680it [00:01, 4387293.42it/s]                            \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 37165.71it/s]            "],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"p018e-E8wSTK","colab_type":"text"},"source":["All subclasses of the Dataset class must override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive.\n","\n","Specifically, there are two methods that are required to be implemented. The __len__ method which returns the length of the dataset, and the __getitem__ method that gets an element from the dataset at a specific index location within the dataset."]},{"cell_type":"markdown","metadata":{"id":"6TKb3vObw9Yj","colab_type":"text"},"source":["## Exploring the data"]},{"cell_type":"code","metadata":{"id":"OtZr7seZw8qm","colab_type":"code","outputId":"3a88e35b-de91-4d06-faf9-89e80a0fb0b9","executionInfo":{"status":"ok","timestamp":1561179811126,"user_tz":-330,"elapsed":6889,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["len(train_set)"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60000"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"OW_vJC9SxJqV","colab_type":"code","outputId":"5ba31546-2c9e-4d06-f71e-16cb8f07b21d","executionInfo":{"status":"ok","timestamp":1561179811127,"user_tz":-330,"elapsed":6879,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# print the target labels for all the 60000 images\n","train_set.targets"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([9, 0, 0,  ..., 3, 0, 5])"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"sq2BZYhRxjpV","colab_type":"code","outputId":"86adf08f-de82-468b-981c-4764bb50d880","executionInfo":{"status":"ok","timestamp":1561179811128,"user_tz":-330,"elapsed":6869,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# finding unique labels \n","torch.unique(train_set.targets)"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"gGtrTHLZx6X6","colab_type":"code","outputId":"d2eff9f9-c278-4649-c3ff-bdb5c81af065","executionInfo":{"status":"ok","timestamp":1561179811129,"user_tz":-330,"elapsed":6860,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# find how many of each labels exist in the dataset\n","train_set.targets.bincount()"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"tXDhxiUeyGP4","colab_type":"text"},"source":["This shows us that the Fashion-MNIST dataset is uniform with respect to the number of samples from each class. This means we have 6000 samples for each class. As a result, this dataset is said to be balanced. If the classes had a varying number of samples, we would call the set an unbalanced dataset.\n","\n","## Accessing data in the training set\n","To access an individual element from the training set, we first pass the train_set object to Pythonâ€™s iter() built-in function, which returns an object representing a stream of data.\n","\n","With the stream of data, we can use Python built-in next() function to get the next data element in the stream of data. From this we are expecting to get a single sample, so weâ€™ll name the result accordingly:"]},{"cell_type":"code","metadata":{"id":"SEuF6jUgyHFu","colab_type":"code","outputId":"165051c9-b71d-40fc-c4ed-421c9d22e5bd","executionInfo":{"status":"ok","timestamp":1561179811129,"user_tz":-330,"elapsed":6846,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":356}},"source":["# get the next element (not batch)\n","image, label = next(iter(train_set))\n","\n","print(type(image))\n","print(type(label))\n","\n","print(image.shape)\n","print(label)\n","\n","img = image.squeeze()\n","\n","plt.imshow(img, cmap = 'gray')"],"execution_count":60,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","<class 'int'>\n","torch.Size([1, 28, 28])\n","9\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fae4736cac8>"]},"metadata":{"tags":[]},"execution_count":60},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEfJJREFUeJzt3W2M1eWZx/HfJfjEgyAiOCARrbjS\nGBfXEY2iqVaMmkatGqwvNhq1NKYm26Qma9wXa+ILiW7b9AVpQq0prl3bJtWo8amu2cTdgJXRsIDO\ntoJiHMQBBZFnGLz2xRyaEflf13jOmXMOvb+fhDBzrrnn3HOGH+fMXP/7vs3dBaA8R7V7AgDag/AD\nhSL8QKEIP1Aowg8UivADhSL8QKEIP1Aowg8UanQr78zMuJwQGGHubsP5uIae+c3sajP7s5mtNbP7\nGvlcAFrL6r2238xGSfqLpPmS+iStkHSru78TjOGZHxhhrXjmnytprbu/5+77JP1W0vUNfD4ALdRI\n+KdL+nDI+321277EzBaaWY+Z9TRwXwCabMR/4efuSyQtkXjZD3SSRp75N0iaMeT9U2u3ATgCNBL+\nFZJmmdnpZnaMpO9JerY50wIw0up+2e/uA2Z2j6SXJY2S9Ji7v920mQEYUXW3+uq6M37mB0ZcSy7y\nAXDkIvxAoQg/UCjCDxSK8AOFIvxAoQg/UCjCDxSK8AOFIvxAoQg/UCjCDxSK8AOFaunW3Wg9s3iB\nV6OrOsePHx/W582bV1l78cUXG7rv7GsbNWpUZW1gYKCh+25UNvdIs1bi8swPFIrwA4Ui/EChCD9Q\nKMIPFIrwA4Ui/ECh6PP/jTvqqPj/9wMHDoT1M888M6zfddddYX337t2VtZ07d4Zj9+zZE9bfeOON\nsN5ILz/rw2ePaza+kblF1y9k38+heOYHCkX4gUIRfqBQhB8oFOEHCkX4gUIRfqBQDfX5zWy9pO2S\nDkgacPfuZkwKzRP1hKW8L3zFFVeE9SuvvDKs9/X1VdaOPfbYcOyYMWPC+vz588P6o48+Wlnr7+8P\nx2Zr5r9OP/1wxo0bV1n74osvwrG7du1q6L4PasZFPpe7+ydN+DwAWoiX/UChGg2/S/qjmb1pZgub\nMSEArdHoy/557r7BzKZIesXM/s/dXxv6AbX/FPiPAegwDT3zu/uG2t+bJD0tae5hPmaJu3fzy0Cg\ns9QdfjMba2bjD74t6SpJa5o1MQAjq5GX/VMlPV1bujha0n+4+0tNmRWAEVd3+N39PUl/38S5YATs\n27evofEXXHBBWJ85c2ZYj64zyNbEv/zyy2H9vPPOC+sPP/xwZa2npyccu3r16rDe29sb1ufO/cpP\nwF8SPa7Lli0Lxy5fvryytmPHjnDsULT6gEIRfqBQhB8oFOEHCkX4gUIRfqBQ1qzjfod1Z2atu7OC\nRNtEZ9/fbFls1C6TpIkTJ4b1/fv3V9aypauZFStWhPW1a9dW1hptgXZ1dYX16OuW4rnffPPN4djF\nixdX1np6evT5558P6/xvnvmBQhF+oFCEHygU4QcKRfiBQhF+oFCEHygUff4OkB3n3Ijs+/v666+H\n9WzJbib62rJjqhvtxUdHfGfXGLz11lthPbqGQMq/tquvvrqydsYZZ4Rjp0+fHtbdnT4/gGqEHygU\n4QcKRfiBQhF+oFCEHygU4QcK1YxTetGgVl5rcaitW7eG9Wzd+u7du8N6dAz36NHxP7/oGGsp7uNL\n0vHHH19Zy/r8l156aVi/+OKLw3q2LfmUKVMqay+91JrjL3jmBwpF+IFCEX6gUIQfKBThBwpF+IFC\nEX6gUGmf38wek/QdSZvc/ZzabZMk/U7STEnrJS1w97hhjI40ZsyYsJ71q7P6rl27Kmvbtm0Lx376\n6adhPdtrILp+IttDIfu6ssftwIEDYT26zmDGjBnh2GYZzjP/ryUduvPAfZJedfdZkl6tvQ/gCJKG\n391fk7TlkJuvl7S09vZSSTc0eV4ARli9P/NPdfeNtbc/ljS1SfMB0CINX9vv7h7tzWdmCyUtbPR+\nADRXvc/8/WbWJUm1vzdVfaC7L3H3bnfvrvO+AIyAesP/rKTbam/fJumZ5kwHQKuk4TezJyUtl/R3\nZtZnZndKWiRpvpm9K+nK2vsAjiDpz/zufmtF6dtNnkuxGu05Rz3lbE38tGnTwvrevXsbqkfr+bN9\n+aNrBCRp4sSJYT26TiDr0x9zzDFhffv27WF9woQJYX3VqlWVtex71t1d/RP0O++8E44diiv8gEIR\nfqBQhB8oFOEHCkX4gUIRfqBQbN3dAbKtu0eNGhXWo1bfLbfcEo495ZRTwvrmzZvDerQ9thQvXR07\ndmw4NlvamrUKozbj/v37w7HZtuLZ133SSSeF9cWLF1fW5syZE46N5vZ1jnvnmR8oFOEHCkX4gUIR\nfqBQhB8oFOEHCkX4gUJZK4+Hjrb7KlnWUx4YGKj7c1944YVh/fnnnw/r2RHcjVyDMH78+HBsdgR3\ntrX30UcfXVdNyq9ByI42z0Rf2yOPPBKOfeKJJ8K6uw+r2c8zP1Aowg8UivADhSL8QKEIP1Aowg8U\nivADhTqi1vNHa5WzfnO2/XW2Djpa/x2tWR+ORvr4mRdeeCGs79y5M6xnff5si+voOpJsr4Dse3rc\ncceF9WzNfiNjs+95Nvdzzz23spYdXd4sPPMDhSL8QKEIP1Aowg8UivADhSL8QKEIP1CotM9vZo9J\n+o6kTe5+Tu22ByR9X9LBRu397h43lIehkbXhI9krH2mXXXZZWL/pppvC+iWXXFJZy465ztbEZ338\nbC+C6HuWzS379xDtyy/F1wFk+1hkc8tkj9uOHTsqazfeeGM49rnnnqtrTocazjP/ryVdfZjbf+bu\nc2p/Gg4+gNZKw+/ur0na0oK5AGihRn7mv8fMVpnZY2Z2YtNmBKAl6g3/LyR9Q9IcSRsl/aTqA81s\noZn1mFlPnfcFYATUFX5373f3A+7+haRfSpobfOwSd+929+56Jwmg+eoKv5l1DXn3u5LWNGc6AFpl\nOK2+JyV9S9JkM+uT9K+SvmVmcyS5pPWSfjCCcwQwAorZt3/SpElhfdq0aWF91qxZdY/N+rZnnXVW\nWN+7d29Yj/YqyNalZ+fMf/TRR2E92/8+6ndnZ9jv27cvrI8ZMyasL1u2rLI2bty4cGx27UW2nj9b\nkx89bv39/eHY2bNnh3X27QcQIvxAoQg/UCjCDxSK8AOFIvxAoTqq1XfRRReF4x988MHK2sknnxyO\nnThxYliPlp5K8fLSzz77LBybLTfOWlZZyyvadjzberu3tzesL1iwIKz39MRXbUfHcJ94YrwkZObM\nmWE9895771XWsuPBt2/fHtazJb9ZCzVqNZ5wwgnh2OzfC60+ACHCDxSK8AOFIvxAoQg/UCjCDxSK\n8AOFanmfP+qXL1++PBzf1dVVWcv69Fm9ka2asy2ms157oyZMmFBZmzx5cjj29ttvD+tXXXVVWL/7\n7rvDerQkeM+ePeHY999/P6xHfXwpXobd6HLibClzdh1BND5bLnzaaaeFdfr8AEKEHygU4QcKRfiB\nQhF+oFCEHygU4QcK1dI+/+TJk/26666rrC9atCgcv27duspathVzVs+Oe45kPd+oDy9JH374YVjP\nts+O9jKItvWWpFNOOSWs33DDDWE9OgZbitfkZ9+T888/v6F69LVnffzsccuO4M5EezBk/56ifS8+\n/vhj7du3jz4/gGqEHygU4QcKRfiBQhF+oFCEHygU4QcKNTr7ADObIelxSVMluaQl7v5zM5sk6XeS\nZkpaL2mBu2+NPtfAwIA2bdpUWc/63dEa6ewY6+xzZz3nqK+b7bO+ZcuWsP7BBx+E9Wxu0X4B2Zr5\n7EyBp59+OqyvXr06rEd9/uzY9KwXn52XEB1Pnn3d2Zr6rBefjY/6/Nk1BNGR7tljMtRwnvkHJP3Y\n3b8p6SJJPzSzb0q6T9Kr7j5L0qu19wEcIdLwu/tGd3+r9vZ2Sb2Spku6XtLS2octlRRfCgago3yt\nn/nNbKak8yT9SdJUd99YK32swR8LABwhhh1+Mxsn6Q+SfuTunw+t+eACgcMuEjCzhWbWY2Y92c9w\nAFpnWOE3s6M1GPzfuPtTtZv7zayrVu+SdNjf5Ln7EnfvdvfuRhdDAGieNPw2+GvJX0nqdfefDik9\nK+m22tu3SXqm+dMDMFLSVp+kSyT9o6TVZraydtv9khZJ+r2Z3SnpA0nxWc4abN1s2LChsp4tL+7r\n66usjR07NhybbWGdtUg++eSTytrmzZvDsaNHxw9ztpw4aytFy2qzLaSzpavR1y1Js2fPDus7d+6s\nrGXt161bw85x+rhFc4/agFLeCszGZ0d0R0upt23bFo6dM2dOZW3NmjXh2KHS8Lv7/0iqakp+e9j3\nBKCjcIUfUCjCDxSK8AOFIvxAoQg/UCjCDxRqOH3+ptm9e7dWrlxZWX/qqacqa5J0xx13VNay7a2z\n45yzpa/RstqsD5/1fLMrH7MjwKPlzNnR5Nm1FdnR5Rs3bgzr0efP5pZdH9HI96zR5cKNLCeW4usI\nTj/99HBsf39/3fc7FM/8QKEIP1Aowg8UivADhSL8QKEIP1Aowg8UqqVHdJtZQ3d2zTXXVNbuvffe\ncOyUKVPCerZuPerrZv3qrE+f9fmzfnf0+aMtoqW8z59dw5DVo68tG5vNPRONj3rlw5F9z7Ktu6P1\n/KtWrQrHLlgQb53h7hzRDaAa4QcKRfiBQhF+oFCEHygU4QcKRfiBQrW8zx/tE5/1Rhtx+eWXh/WH\nHnoorEfXCUyYMCEcm+2Nn10HkPX5s+sMItGR6VJ+HUB0DoMUf0937NgRjs0el0w092zde7aPQfY9\nfeWVV8J6b29vZW3ZsmXh2Ax9fgAhwg8UivADhSL8QKEIP1Aowg8UivADhUr7/GY2Q9LjkqZKcklL\n3P3nZvaApO9LOng4/f3u/kLyuVp3UUELnX322WF98uTJYT3bA/7UU08N6+vXr6+sZf3sdevWhXUc\neYbb5x/OoR0Dkn7s7m+Z2XhJb5rZwSsYfubu/1bvJAG0Txp+d98oaWPt7e1m1itp+khPDMDI+lo/\n85vZTEnnSfpT7aZ7zGyVmT1mZidWjFloZj1m1tPQTAE01bDDb2bjJP1B0o/c/XNJv5D0DUlzNPjK\n4CeHG+fuS9y92927mzBfAE0yrPCb2dEaDP5v3P0pSXL3fnc/4O5fSPqlpLkjN00AzZaG3wa3QP2V\npF53/+mQ27uGfNh3Ja1p/vQAjJThtPrmSfpvSaslHVyfeb+kWzX4kt8lrZf0g9ovB6PP9TfZ6gM6\nyXBbfUfUvv0AcqznBxAi/EChCD9QKMIPFIrwA4Ui/EChCD9QKMIPFIrwA4Ui/EChCD9QKMIPFIrw\nA4Ui/EChhrN7bzN9IumDIe9Prt3WiTp1bp06L4m51auZczttuB/Y0vX8X7lzs55O3duvU+fWqfOS\nmFu92jU3XvYDhSL8QKHaHf4lbb7/SKfOrVPnJTG3erVlbm39mR9A+7T7mR9Am7Ql/GZ2tZn92czW\nmtl97ZhDFTNbb2arzWxlu48Yqx2DtsnM1gy5bZKZvWJm79b+PuwxaW2a2wNmtqH22K00s2vbNLcZ\nZvZfZvaOmb1tZv9Uu72tj10wr7Y8bi1/2W9moyT9RdJ8SX2SVki61d3faelEKpjZeknd7t72nrCZ\nXSZph6TH3f2c2m0PS9ri7otq/3Ge6O7/3CFze0DSjnaf3Fw7UKZr6MnSkm6QdLva+NgF81qgNjxu\n7Xjmnytprbu/5+77JP1W0vVtmEfHc/fXJG055ObrJS2tvb1Ug/94Wq5ibh3B3Te6+1u1t7dLOniy\ndFsfu2BebdGO8E+X9OGQ9/vUWUd+u6Q/mtmbZraw3ZM5jKlDTkb6WNLUdk7mMNKTm1vpkJOlO+ax\nq+fE62bjF35fNc/d/0HSNZJ+WHt525F88Ge2TmrXDOvk5lY5zMnSf9XOx67eE6+brR3h3yBpxpD3\nT63d1hHcfUPt702SnlbnnT7cf/CQ1Nrfm9o8n7/qpJObD3eytDrgseukE6/bEf4VkmaZ2elmdoyk\n70l6tg3z+AozG1v7RYzMbKykq9R5pw8/K+m22tu3SXqmjXP5kk45ubnqZGm1+bHruBOv3b3lfyRd\nq8Hf+K+T9C/tmEPFvM6Q9L+1P2+3e26SntTgy8D9GvzdyJ2STpL0qqR3Jf2npEkdNLd/1+Bpzqs0\nGLSuNs1tngZf0q+StLL259p2P3bBvNryuHGFH1AofuEHFIrwA4Ui/EChCD9QKMIPFIrwA4Ui/ECh\nCD9QqP8HS8xVdqsDFvAAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"kW-t6rn71c0x","colab_type":"text"},"source":["## PyTorch DataLoader: Working with batches of data\n","We get a batch from the loader in the same way that we saw with the training set. We use the iter() and next() functions.\n","\n","There is one thing to notice when working with the data loader. If shuffle=True, then the batch will be different each time a call to next occurs. With shuffle=True, the first samples in the training set will be returned on the first call to next. The shuffle functionality is turned off by default."]},{"cell_type":"code","metadata":{"id":"daC-FLKYvzVM","colab_type":"code","colab":{}},"source":["# To create a DataLoader wrapper for our training set, we do it like this:\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUR0oIci1-_2","colab_type":"code","outputId":"c0e19801-3e8f-475c-99c7-6ccc3d578095","executionInfo":{"status":"ok","timestamp":1561179811135,"user_tz":-330,"elapsed":6837,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["images, labels = next(iter(train_loader))\n","print(images.shape)\n","print(labels.shape)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["torch.Size([10, 1, 28, 28])\n","torch.Size([10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1excRu_K4O7y","colab_type":"text"},"source":["(batch size, number of color channels, image height, image width)\n","\n","To plot a batch of images, we can use the torchvision.utils.make_grid() function to create a grid that can be plotted like so:"]},{"cell_type":"code","metadata":{"id":"Q2NSM9wM4QL9","colab_type":"code","outputId":"71900ef4-5f05-4e65-bb11-3ae41f4e200b","executionInfo":{"status":"ok","timestamp":1561179811730,"user_tz":-330,"elapsed":7420,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["# make a grid with the images \n","grid = torchvision.utils.make_grid(images, nrow=10)\n","\n","plt.figure(figsize = (15,4))\n","plt.imshow(np.transpose(grid,(1,2,0)))\n","#we can also use: grid.permute(1,2,0)\n","\n","print('Labels: ',labels)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Labels:  tensor([2, 4, 9, 5, 1, 5, 0, 5, 6, 3])\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA2wAAAB7CAYAAAAIVwPvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<Figure size 1080x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"HjjVppQB6N2j","colab_type":"text"},"source":["# Build PyTorch CNN - Object Oriented Neural Networks\n","To build neural networks in PyTorch, we extend the torch.nn.Module PyTorch class. This means we need to utilize a little bit of object oriented programming (OOP) in Python.\n","\n","Weâ€™ll do a quick OOP review in this post to cover the details needed for working with PyTorch neural networks, but if you find that you need more, the Python docs have an overview tutorial [here.](https://https://docs.python.org/3/tutorial/classes.html)\n","\n","## PyTorchâ€™s torch.nn package\n","To build neural networks in PyTorch, we use the torch.nn package, which is PyTorchâ€™s neural network (nn) library. We typically import the package like so:\n","\n","\n","```\n","import torch.nn as nn\n","```\n","\n","## PyTorch's nn.Module class\n","As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components:\n","\n","A transformation (code)\n","A collection of weights (data)\n","Like many things in life, this fact makes layers great candidates to be represented as objects using OOP. OOP is short for object oriented programming.\n","\n","In fact, this is the case with PyTorch. Within the nn package, there is a class called Module, and it is the base class for all of neural network modules which includes layers.\n","\n","This means that all of the layers in PyTorch extend the nn.Module class and inherit all of PyTorchâ€™s built-in functionality within the nn.Module class. In OOP this concept is known as inheritance.\n","\n","Even neural networks extend the nn.Module class. This makes sense because neural networks themselves can be thought of as one big layer \n","\n","### PyTorch nn.Modules have a forward() method\n","Forward pass is defined in this method. When we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation.\n","\n","### PyTorchâ€™s nn.functional package\n","When we implement the forward() method of our nn.Module subclass, we will typically use functions from the nn.functional package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the nn.Module layer classes use nn.functional functions to perform their operations.\n","\n","## Steps to create a model in PyTorch\n","* Create a neural network class that extends the nn.Module base class.\n","* In the class constructor, define the networkâ€™s layers as class attributes using pre-built layers from torch.nn.\n","* Use the networkâ€™s layer attributes as well as operations from the nn.functional API to define the networkâ€™s forward pass.\n","\n","## Step 1 "]},{"cell_type":"code","metadata":{"id":"cRkD8Psk6NS2","colab_type":"code","colab":{}},"source":["# create a simple class\n","class Network:\n","    def __init__(self):\n","        self.layer = None\n","\n","    def forward(self, t):\n","        t = self.layer(t)\n","        return t"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVWZDnniVWW0","colab_type":"text"},"source":["This gives us a simple network class that has a single dummy layer inside the constructor and a dummy implementation for the forward function.\n","\n","The implementation for the forward() function takes in a tensor t and transforms it using the dummy layer. After the tensor is transformed, the new tensor is returned.\n","\n","This is a good start, but the class hasnâ€™t yet extended the nn.Module class. To make our Network class extend nn.Module, we must do two additional things:\n","\n","* Specify the nn.Module class in parentheses on line 1.\n","* Insert a call to the super class constructor on line 3 inside the constructor."]},{"cell_type":"code","metadata":{"id":"ZVkY_BEGVXOd","colab_type":"code","colab":{}},"source":["class Network(nn.Module):\n","    def __init__(self):\n","        super(Network,self).__init__()\n","        self.layer = None\n","        \n","    def forward(self,t):\n","        t = self.Layer(t)\n","        return t"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9WGnDQlWWAd","colab_type":"text"},"source":["These changes transform our simple neural network into a PyTorch neural network because we are now extending PyTorch's nn.Module base class.\n","\n","With this, we are done! Now we have a Network class that has all of the functionality of the PyTorch nn.Module class.\n","\n","## Step 2"]},{"cell_type":"code","metadata":{"id":"UMRauA6GWsfl","colab_type":"code","colab":{}},"source":["class Network(nn.Module):\n","    def __init__(self):\n","        super(Network,self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n","        \n","        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n","        self.fc2 = nn.Linear(in_features=120, out_features=60)\n","        self.out = nn.Linear(in_features=60, out_features=10)\n","        \n","    def forward(self,t): \n","        # implement the forward pass\n","        return t"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZXPdct5Y1-A","colab_type":"text"},"source":["Each of our layers extends PyTorch's neural network Module class. For each layer, there are two primary items encapsulated inside, a forward function definition and a weight tensor.\n","\n","The weight tensor inside each layer contains the weight values that are updated as the network learns during the training process, and this is the reason we are specifying our layers as attributes inside our Network class.\n","\n","PyTorch's neural network Module class keeps track of the weight tensors inside each layer. The code that does this tracking lives inside the nn.Module class, and since we are extending the neural network module class, we inherit this functionality automatically.\n","\n","Remember, inheritance is one of those object oriented concepts that we talked about last time. All we have to do to take advantage of this functionality is assign our layers as attributes inside our network module, and the Module base class will see this and register the weights as learnable parameters of our network.\n","\n","#### Parameter vs Argument\n","While parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function.\n","\n","In our network's case, the names are the parameters and the values that we have specified are the arguments.\n","\n","### Getting an instance of the Network\n","Remember, to get an object instance of our Network class, we type the class name followed by parentheses. When this code executes, the code inside the __init__ class constructor will run, assigning our layers as attributes before the object instance is returned."]},{"cell_type":"code","metadata":{"id":"WwX5x-SbZSZJ","colab_type":"code","outputId":"d6d92da8-3d88-443f-f6f7-75cee9fa8006","executionInfo":{"status":"ok","timestamp":1561179811743,"user_tz":-330,"elapsed":7418,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["network = Network()\n","print(network)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["Network(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=192, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=60, bias=True)\n","  (out): Linear(in_features=60, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bMp1vorhgG7c","colab_type":"text"},"source":["This nice string representation comes from nn.Module base class. We can override this functionality using __repr__() "]},{"cell_type":"code","metadata":{"id":"PVfJNjI7gYIR","colab_type":"code","outputId":"0e17c8d2-8351-4a73-c808-1b34368d7373","executionInfo":{"status":"ok","timestamp":1561179811744,"user_tz":-330,"elapsed":7405,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["class Network(nn.Module):\n","    \n","    # constructor function constructs objects\n","    def __init__(self):\n","        super(Network,self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n","        \n","        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n","        self.fc2 = nn.Linear(in_features=120, out_features=60)\n","        self.out = nn.Linear(in_features=60, out_features=10)\n","        \n","    def forward(self,t): \n","        # implement the forward pass\n","        return t\n","    \n","    def __repr__(self):\n","        return 'Simple Model'\n","    \n","network = Network()\n","print(network)    "],"execution_count":68,"outputs":[{"output_type":"stream","text":["Simple Model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D2zaccejhn-F","colab_type":"text"},"source":["### Accessing specific layers of the model\n","we access attributes and methods of objects using dot notation."]},{"cell_type":"code","metadata":{"id":"JwPmaOYchnli","colab_type":"code","outputId":"f4444ec5-5dc0-425f-8caa-d49c65f64690","executionInfo":{"status":"ok","timestamp":1561179811745,"user_tz":-330,"elapsed":7394,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["network.conv1"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"j0SuGT26iE0r","colab_type":"text"},"source":["### Accessing weights of a layer in the network\n","This is a good example that showcases how objects are nested. We first access the conv layer object that lives inside the network object.\n","\n","Then, we access the weight tensor object that lives inside the conv layer object, so all of these objects are chained or linked together.\n","\n"]},{"cell_type":"code","metadata":{"id":"h823-pZeiJHf","colab_type":"code","outputId":"7072a8bc-c979-4739-f5eb-a485e445b92c","executionInfo":{"status":"ok","timestamp":1561179811746,"user_tz":-330,"elapsed":7383,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":745}},"source":["print(network.conv1.weight.shape)\n","print(network.conv1.weight)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["torch.Size([6, 1, 5, 5])\n","Parameter containing:\n","tensor([[[[-1.2327e-01,  4.8687e-02,  6.7146e-02,  1.2331e-01, -2.2676e-02],\n","          [-1.5241e-01,  7.4533e-02, -7.5088e-02, -4.3989e-02, -1.8396e-01],\n","          [-3.7828e-02,  7.4208e-02,  1.3061e-01,  1.3842e-01,  1.1060e-01],\n","          [ 1.1955e-01,  1.6281e-01,  1.9613e-01, -1.1820e-01,  7.7322e-05],\n","          [-1.5651e-01, -4.1623e-02,  1.3419e-01,  8.6564e-02,  4.7187e-02]]],\n","\n","\n","        [[[-8.8144e-02,  4.5578e-02,  1.9309e-01,  1.9857e-01, -3.4807e-02],\n","          [ 1.1247e-01, -1.5200e-01, -1.4739e-01, -7.0977e-02, -1.4732e-01],\n","          [-3.0903e-02,  1.6030e-01,  9.6916e-02,  1.8270e-01, -8.0652e-02],\n","          [ 2.3350e-04,  1.0217e-01, -8.6990e-02, -1.7729e-01,  1.3270e-01],\n","          [ 7.4908e-02, -1.0606e-03, -1.7848e-02,  6.2075e-02,  1.0076e-01]]],\n","\n","\n","        [[[-5.9415e-02, -1.9033e-01, -2.5441e-02, -8.4010e-02,  1.1576e-01],\n","          [-2.8890e-02, -1.5430e-01, -1.5346e-01, -1.0298e-01, -1.1794e-01],\n","          [-1.5470e-01,  1.8514e-01,  1.8644e-01,  8.1992e-03,  1.0945e-01],\n","          [-1.0710e-01, -5.5889e-02, -1.9836e-01, -4.7208e-02,  8.6151e-02],\n","          [-8.9773e-02, -6.6982e-02, -1.7465e-01, -1.8359e-01, -3.3861e-02]]],\n","\n","\n","        [[[-8.7009e-02,  1.1338e-02, -1.6626e-01,  3.8672e-03, -1.7454e-01],\n","          [ 8.6183e-03,  9.0949e-03, -5.9380e-02, -9.5515e-02,  1.6436e-01],\n","          [-1.7770e-01,  1.3650e-01,  1.0732e-02,  5.9434e-02, -9.4483e-02],\n","          [-1.4116e-01,  1.6269e-01, -1.5049e-01, -3.3419e-02, -1.4212e-02],\n","          [-1.7192e-01, -1.3595e-03,  1.8528e-01,  9.9832e-02, -9.0929e-02]]],\n","\n","\n","        [[[-1.7881e-02, -4.3859e-02, -9.6842e-03, -1.0868e-02,  7.9900e-02],\n","          [ 8.9568e-02,  1.4697e-02,  1.4159e-01, -7.1276e-02,  7.2456e-02],\n","          [ 1.1327e-01, -7.0666e-02, -7.1449e-02, -1.4064e-01,  1.7282e-02],\n","          [ 9.0106e-02, -1.6800e-01, -1.7337e-01,  1.0005e-01,  1.9874e-01],\n","          [-1.8206e-01,  1.2563e-01, -2.6084e-02,  1.7693e-01, -1.8907e-01]]],\n","\n","\n","        [[[ 4.7869e-02,  8.0709e-03,  5.7454e-02, -8.6234e-02,  8.1739e-02],\n","          [-1.5186e-01,  8.3564e-02, -1.4463e-02,  9.6562e-02,  2.4710e-02],\n","          [-1.3676e-01, -1.9181e-01,  1.0685e-01,  1.7113e-01,  5.9982e-02],\n","          [-1.7166e-01, -1.9922e-01, -1.6621e-01,  6.6444e-02,  1.9828e-01],\n","          [ 4.5961e-02, -1.4871e-02,  1.3931e-01, -1.7702e-01,  1.1581e-01]]]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hjgj7wIGioHJ","colab_type":"text"},"source":["**Note:** One thing to notice about the weight tensor output is that it says parameter containing at the top of the output. This is because this particular tensor is a special tensor because its values or scalar components are learnable parameters of our network.\n","\n","### PyTorch Parameter Class\n","To keep track of all the weight tensors inside the network. PyTorch has a special class called Parameter. The Parameter class extends the tensor class, and so the weight tensor inside every layer is an instance of this Parameter class. This is why we see the Parameter containing text at the top of the string representation output.\n","### Accessing the Networks Parameters\n","\n"]},{"cell_type":"code","metadata":{"id":"RF65ahjuis9-","colab_type":"code","outputId":"bbbf3325-49b6-4ced-83b6-4c38cd684913","executionInfo":{"status":"ok","timestamp":1561179811747,"user_tz":-330,"elapsed":7372,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["for param in network.parameters():\n","    print(param.shape)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["torch.Size([6, 1, 5, 5])\n","torch.Size([6])\n","torch.Size([12, 6, 5, 5])\n","torch.Size([12])\n","torch.Size([120, 192])\n","torch.Size([120])\n","torch.Size([60, 120])\n","torch.Size([60])\n","torch.Size([10, 60])\n","torch.Size([10])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hw5TMuCRn8_2","colab_type":"code","outputId":"79c3af52-e4f4-4df8-ee19-49c5ea862f90","executionInfo":{"status":"ok","timestamp":1561179811748,"user_tz":-330,"elapsed":7362,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["for name, param in network.named_parameters():\n","    print('Name: {}      Shape: {}'.format(name, param.shape))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Name: conv1.weight      Shape: torch.Size([6, 1, 5, 5])\n","Name: conv1.bias      Shape: torch.Size([6])\n","Name: conv2.weight      Shape: torch.Size([12, 6, 5, 5])\n","Name: conv2.bias      Shape: torch.Size([12])\n","Name: fc1.weight      Shape: torch.Size([120, 192])\n","Name: fc1.bias      Shape: torch.Size([120])\n","Name: fc2.weight      Shape: torch.Size([60, 120])\n","Name: fc2.bias      Shape: torch.Size([60])\n","Name: out.weight      Shape: torch.Size([10, 60])\n","Name: out.bias      Shape: torch.Size([10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TR154PFSVdUr","colab_type":"text"},"source":["### Callable Layers and Neural Networks\n","Instead of calling the forward() method directly, we call the object instance. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. This applies to all PyTorch neural network modules, namely, networks and layers.\n","\n","## Step 3 Forward Pass\n","\n","Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are just pure operations. Neither of these have weights, and this is why we call them directly from the nn.functional API."]},{"cell_type":"code","metadata":{"id":"p09ONW5TWBJr","colab_type":"code","colab":{}},"source":["class Network(nn.Module):\n","    \n","    # constructor function constructs objects\n","    def __init__(self):\n","        super(Network,self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n","        \n","        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n","        self.fc2 = nn.Linear(in_features=120, out_features=60)\n","        self.out = nn.Linear(in_features=60, out_features=10)\n","        \n","    def forward(self,t): \n","        \n","        t = self.conv1(t)\n","        t = F.relu(t)\n","        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n","        \n","        t = self.conv2(t)\n","        t = F.relu(t)\n","        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n","        \n","        t = t.reshape(-1, 12 * 4 * 4)\n","        t = self.fc1(t)\n","        t = F.relu(t)\n","        \n","        t = self.fc2(t)\n","        t = F.relu(t)\n","        \n","        t = self.out(t)\n","        #t = F.softmax(t, dim = 1)\n","       \n","        return t\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5C5kI57aPlU","colab_type":"text"},"source":["The values inside each of the ten components will correspond to the prediction value for each of our prediction classes.\n","\n","Inside the network we usually use relu() as our non-linear activation function, but for the output layer, whenever we have a single category that we are trying to predict, we use softmax(). The softmax function returns a positive probability for each of the prediction classes, and the probabilities sum to 1.\n","\n","\n"," \n","However, in our case, we won't use softmax() because the loss function that we'll use, F.cross_entropy(), implicitly performs the softmax() operation on its input, so we'll just return the result of the last linear transformation.\n","\n","The implication of this is that our network will be trained using the softmax operation but will not need to compute the additional operation when the network is used for inference after the training process is complete.\n","\n","### Predicting with the network: Forward pass\n","Before we being, we are going to turn off PyTorchâ€™s gradient calculation feature. This will stop PyTorch from automatically building a computation graph as our tensor flows through the network.\n","\n","The computation graph keeps track of the network's mapping by tracking each computation that happens. The graph is used during the training process to calculate the derivative (gradient) of the loss function with respect to the networkâ€™s weights.\n","\n","Since we are not training the network yet, we arenâ€™t planning on updating the weights, and so we donâ€™t require gradient calculations. We will turn this back on when training begins.\n","\n","### Turning off the computation graph"]},{"cell_type":"code","metadata":{"id":"LZ6V8_XpaRhO","colab_type":"code","outputId":"990e1cc5-f3c3-47ef-946d-c224d1c11aab","executionInfo":{"status":"ok","timestamp":1561179812311,"user_tz":-330,"elapsed":7912,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# turning off dynamic computational graph\n","torch.set_grad_enabled(False)"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7fae4525f7b8>"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"109b5AxhdL22","colab_type":"code","outputId":"b1f63e9d-0211-4b83-f3a3-5fb77476587d","executionInfo":{"status":"ok","timestamp":1561179812312,"user_tz":-330,"elapsed":7900,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# create an instance of the Network class\n","network = Network()\n","\n","image, label = next(iter(train_set))\n","print(image.shape)\n","\n","# unsqueeze to add a dimension for batch\n","image = image.unsqueeze(0)\n","print(image.shape)\n","\n","pred = network(image)\n","print(pred)\n","\n","print('Prediction: {}'.format(pred.argmax(dim = 1)))\n","print('Target Label: {}'.format(label))\n","\n","# If we want th epredictions as probability distribution\n","print('Prediction: {}'.format(F.softmax(pred,dim = 1)))"],"execution_count":75,"outputs":[{"output_type":"stream","text":["torch.Size([1, 28, 28])\n","torch.Size([1, 1, 28, 28])\n","tensor([[-0.1358, -0.0872,  0.0790,  0.0626,  0.1245, -0.0993,  0.1098,  0.0712, -0.0160, -0.0651]])\n","Prediction: tensor([4])\n","Target Label: 9\n","Prediction: tensor([[0.0866, 0.0909, 0.1073, 0.1056, 0.1123, 0.0898, 0.1107, 0.1065, 0.0976, 0.0929]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MmrY6uopm56R","colab_type":"text"},"source":["The prediction in this case is incorrect, which is what we expect because the weights in the network were generated randomly.\n","\n","### Predicting on a batch of data"]},{"cell_type":"code","metadata":{"id":"NDSDSSNZm7Gy","colab_type":"code","outputId":"b287498e-8f61-429f-bf04-a704e27507a7","executionInfo":{"status":"ok","timestamp":1561179812313,"user_tz":-330,"elapsed":7886,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from torch.utils.data import DataLoader \n","\n","data_loader = DataLoader(train_set, batch_size = 10, shuffle = True)\n","\n","images, labels = next(iter(data_loader))\n","\n","print(images.shape)\n","print(labels.shape)\n","\n","predictions = network(images)\n","\n","print(predictions.argmax(dim = 1))\n","print(labels)\n","\n","# Element wise comparison between labels and predictions\n","# This can be used to calculate accuracy\n","print(predictions.argmax(dim = 1).eq(labels))\n","\n","# Find number of correct predictions\n","# .item() returns the value of a 1x1 tensor (scalar)\n","print(predictions.argmax(dim = 1).eq(labels).sum().item())"],"execution_count":76,"outputs":[{"output_type":"stream","text":["torch.Size([10, 1, 28, 28])\n","torch.Size([10])\n","tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n","tensor([2, 0, 0, 9, 4, 4, 0, 2, 0, 0])\n","tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0], dtype=torch.uint8)\n","2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A-aJzrZ9vutD","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"FUf0femkv0fI","colab_type":"code","outputId":"707fcf79-e2db-43e9-f44d-9d7c2ceea5e3","executionInfo":{"status":"ok","timestamp":1561179812314,"user_tz":-330,"elapsed":7874,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# turn on computation graph\n","torch.set_grad_enabled(True)\n","\n","train_loader = DataLoader(train_set, batch_size = 1000, shuffle = True)\n","\n","images, labels = next(iter(train_loader))\n","\n","preds = network(images)\n","\n","# defining the loss function\n","loss = F.cross_entropy(preds, labels)\n","\n","print('Loss before Training: {}'.format(loss.item()))\n","\n","# define a function to find the number of correct predictions\n","def correct_predictions(pred, label):\n","    return pred.argmax(dim = 1).eq(label).sum().item()"],"execution_count":77,"outputs":[{"output_type":"stream","text":["Loss before Training: 2.3090178966522217\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XntIxex70R2W","colab_type":"text"},"source":["### Calculating the Gradients\n","The computation graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network's weights. The gradients are tensors that are accessible in the grad (short for gradient) attribute of the weight tensor of each layer."]},{"cell_type":"code","metadata":{"id":"5g0GodMF0TZ_","colab_type":"code","outputId":"e19abf1e-d31a-4b08-b8f8-ed2c438bc6ad","executionInfo":{"status":"ok","timestamp":1561179812315,"user_tz":-330,"elapsed":7863,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# since we have not calculated the gradients yet, they are none\n","print(network.conv1.weight.grad)\n","\n","# To calculate the gradients, we call the backward() method on the loss tensor, like so:\n","loss.backward()\n","\n","# print the shape of the gradients of conv1\n","print(network.conv1.weight.grad.shape)"],"execution_count":78,"outputs":[{"output_type":"stream","text":["None\n","torch.Size([6, 1, 5, 5])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yFUd0HLe1vQI","colab_type":"text"},"source":["### Updating the weights (optimize)\n","These gradients are used by the optimizer to update the respective weights. To create our optimizer, we use the torch.optim package that has many optimization algorithm implementations that we can use. We'll use Adam for our example."]},{"cell_type":"code","metadata":{"id":"4joLQMfv15ZA","colab_type":"code","colab":{}},"source":["# create an instance of Adam optimizer\n","# we are passing network.parameters() as it contains all the network's variables\n","optimizer = optim.Adam(network.parameters(), lr=0.01)\n","\n","# Updating the weights\n","optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kXaBO9U2cCb","colab_type":"text"},"source":["When the step() function is called, the optimizer updates the weights using the gradients that are stored in the network's parameters. This means that we should expect our loss to be reduced if we pass the same batch through the network again. Checking this, we can see that this is indeed the case:"]},{"cell_type":"code","metadata":{"id":"MWMM6rfg2dW8","colab_type":"code","outputId":"037a3a07-245c-4273-f142-c674f532895e","executionInfo":{"status":"ok","timestamp":1561179812320,"user_tz":-330,"elapsed":7852,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["preds = network(images)\n","\n","# defining the loss function\n","loss = F.cross_entropy(preds, labels)\n","\n","print('Loss after 1 step of optimization: {}'.format(loss.item()))\n","\n","print(correct_predictions(preds, labels))"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Loss after 1 step of optimization: 2.293449878692627\n","175\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-N4yAomn3uO0","colab_type":"text"},"source":["## Create a training Loop"]},{"cell_type":"code","metadata":{"id":"CiMMalhB3z7h","colab_type":"code","colab":{}},"source":["num_epochs = 5\n","\n","train_steps = int(60000/1000)\n","\n","for epoch in range(num_epochs):\n","    for step in range(train_steps):\n","        \n","        images, labels = next(iter(train_loader))\n","        \n","        preds = network(images)\n","        \n","        loss = F.cross_entropy(preds, labels)\n","        \n","        # calculate the gradients\n","        loss.backward()\n","        \n","        optimizer = optim.Adam(network.parameters(), lr = 0.01)\n","        \n","        # update the parameters\n","        optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yR12EgI3jIWT","colab_type":"text"},"source":["## Save and Restore\n","When it comes to saving and loading models, there are three core functions to be familiar with:\n","\n","* torch.save: Saves a serialized object to disk. This function uses Pythonâ€™s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n","* torch.load: Uses pickleâ€™s unpickling facilities to deserialize pickled object files to memory.\n","* torch.nn.Module.load_state_dict: Loads a modelâ€™s parameter dictionary using a deserialized state_dict. \n","\n","### state_dict\n","In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the modelâ€™s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. Note that only layers with learnable parameters (convolutional layers, linear layers, etc.) and registered buffers (batchnormâ€™s running_mean) have entries in the modelâ€™s state_dict. Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizerâ€™s state, as well as the hyperparameters used."]},{"cell_type":"code","metadata":{"id":"yevWV-oer99N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":6171},"outputId":"4d7c4f53-bcea-4a5f-a415-794e423685cb","executionInfo":{"status":"ok","timestamp":1561181819733,"user_tz":-330,"elapsed":1065,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}}},"source":["network.state_dict()"],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('conv1.weight',\n","              tensor([[[[-1.9264, -4.0183, -4.0680, -4.2739,  4.5970],\n","                        [-2.3058, -4.7468, -4.2884, -4.5651, -4.1111],\n","                        [-2.8358, -4.5300, -4.6306, -4.4916, -4.6153],\n","                        [-4.5992, -4.5430, -4.7504, -4.7498, -4.2877],\n","                        [-4.5218, -5.1768, -4.4291, -4.3926, -4.4767]]],\n","              \n","              \n","                      [[[-6.4682, -6.5975, -6.5501, -6.4855, -6.4625],\n","                        [-6.8183, -6.7827, -6.7119, -6.8149, -6.6983],\n","                        [-6.7099, -6.6931, -6.6333, -6.8842, -7.0120],\n","                        [-7.0095, -7.0339, -6.7584, -7.0696, -6.7994],\n","                        [-6.7933, -7.1006, -6.7979, -7.0769, -6.8590]]],\n","              \n","              \n","                      [[[-2.7384, -7.1502, -6.8969, -6.9200, -7.0639],\n","                        [-2.5411, -3.2767, -6.6180, -4.8523, -6.8291],\n","                        [-3.1953, -5.5631, -6.9079, -5.4340, -7.1285],\n","                        [-0.8541, -0.2039, -3.9739, -2.5284, -5.0527],\n","                        [-0.4909, -0.1068, -1.7849, -1.4876, -5.7828]]],\n","              \n","              \n","                      [[[-6.5969, -6.7152, -6.6907, -6.7927, -6.5790],\n","                        [-6.8855, -6.8361, -6.9318, -6.8575, -7.0943],\n","                        [-6.8254, -6.7502, -6.5142, -6.6249, -6.6973],\n","                        [-7.0067, -6.9484, -7.0257, -6.7103, -6.8687],\n","                        [-6.9149, -6.9770, -6.8273, -7.1193, -6.8387]]],\n","              \n","              \n","                      [[[-6.8239, -7.1242, -6.9788, -6.9414, -6.9781],\n","                        [-6.7972, -7.0662, -7.0249, -6.7863, -6.8673],\n","                        [-6.9260, -6.7502, -6.7992, -6.7510, -7.0785],\n","                        [-6.7605, -6.7407, -7.0005, -6.7693, -7.1076],\n","                        [-6.9408, -6.8917, -6.9944, -6.8771, -6.7477]]],\n","              \n","              \n","                      [[[-6.9592, -6.8691, -6.8657, -6.5408, -6.8161],\n","                        [-6.9488, -6.8766, -6.8160, -6.6090, -6.7509],\n","                        [-6.7926, -6.7391, -6.7804, -6.4630, -6.8692],\n","                        [-6.9808, -6.9035, -6.8402, -6.7443, -6.7097],\n","                        [-6.7836, -7.0567, -6.6873, -6.9307, -6.8651]]]])),\n","             ('conv1.bias',\n","              tensor([-4.9140, -6.5720,  0.3943, -6.8700, -6.9902, -6.7766])),\n","             ('conv2.weight',\n","              tensor([[[[-6.8570, -6.8379, -6.8680, -6.9768, -6.8640],\n","                        [-6.8544, -6.9374, -6.8785, -6.8638, -6.9068],\n","                        [-6.9995, -6.8755, -6.8567, -6.9523, -6.8404],\n","                        [-6.9614, -6.8735, -6.9281, -6.9767, -6.9399],\n","                        [-6.9194, -7.0152, -6.9130, -6.9700, -6.8725]],\n","              \n","                       [[-7.0152, -6.9606, -6.9655, -6.9786, -6.9240],\n","                        [-7.0035, -6.9618, -6.9923, -6.9530, -6.8997],\n","                        [-6.9312, -7.0209, -7.0109, -6.9805, -7.0223],\n","                        [-6.8926, -6.9469, -7.0112, -6.9370, -7.0395],\n","                        [ 6.9344, -6.8814, -6.9410, -6.9053, -6.9468]],\n","              \n","                       [[-6.8929, -6.9325, -6.9337, -6.9196, -7.0313],\n","                        [-6.9216, -6.9795, -6.9981, -7.0125, -7.0206],\n","                        [-6.9635, -6.9220, -6.9998, -6.9097, -6.9434],\n","                        [-6.9445, -6.9375, -7.0412, -7.0104, -7.0194],\n","                        [-7.0047, -7.0094, -6.9969, -6.9500, -6.9623]],\n","              \n","                       [[ 6.8737,  6.9576,  6.8230,  6.9014,  6.7801],\n","                        [ 6.8496,  6.8691,  6.8112,  6.9108,  6.7735],\n","                        [ 6.9547,  6.9227,  6.8452,  6.8789,  6.7160],\n","                        [ 6.8680,  6.9594,  6.8901,  6.8814,  6.8684],\n","                        [ 6.8606,  6.9811,  6.8997,  6.9437,  6.8982]],\n","              \n","                       [[-6.9049, -6.9157, -6.9917, -6.9085, -6.8957],\n","                        [-6.9057, -6.8617, -6.9298, -6.9842, -6.9000],\n","                        [-6.9027, -6.9389, -6.9064, -6.9683, -6.9014],\n","                        [-7.0391, -7.0413, -6.8807, -6.9586, -6.8916],\n","                        [-6.9370, -6.9632, -7.0161, -6.8999, -6.8524]],\n","              \n","                       [[-3.4648, -5.4048, -4.5407, -3.5411,  3.6603],\n","                        [-6.0519, -4.8718, -3.5470,  4.4124, -3.6444],\n","                        [-2.8941, -5.8526, -1.7666,  1.4803, -4.1870],\n","                        [ 0.1256, -3.3077,  0.9609, -2.8393, -6.0886],\n","                        [-4.3540, -5.7675,  4.7764, -5.6848, -6.4623]]],\n","              \n","              \n","                      [[[-6.8607, -6.8417, -6.7558, -6.6519,  6.9509],\n","                        [-6.7805, -6.8000, -6.7560, -6.7776, -6.7629],\n","                        [-6.8204, -6.8665, -6.9113, -6.8815, -6.8921],\n","                        [-0.5408, -7.0408, -6.9895, -6.9025, -7.0166],\n","                        [-7.0585, -6.9717, -6.9371, -7.0596, -7.0316]],\n","              \n","                       [[-6.8990, -6.9229, -6.8994, -6.8739, -6.9834],\n","                        [-6.8599, -6.8805, -6.7598,  6.7035,  6.7623],\n","                        [-6.7591, -6.8033, -6.6085, -6.5637, -6.5782],\n","                        [-6.7444, -6.6102, -6.6176, -6.7529, -6.6558],\n","                        [ 6.9548,  6.9596,  6.9742,  7.0268,  7.0602]],\n","              \n","                       [[-6.8871, -6.9011, -6.9457, -6.9536, -6.8595],\n","                        [-6.9145, -6.8628, -6.9816, -6.9662, -7.0340],\n","                        [-6.9363, -6.9612, -6.8769, -7.0179, -6.9161],\n","                        [-6.9176, -6.9160, -6.9943, -6.8949, -6.9826],\n","                        [-6.9294, -6.9053, -6.9762, -6.8809, -6.9453]],\n","              \n","                       [[ 6.8607,  6.9331,  6.8973,  6.9553,  6.9991],\n","                        [ 6.9538,  6.9485,  6.9394,  6.9217,  7.0129],\n","                        [-6.4923, -6.5662, -6.7343,  6.9990,  6.9932],\n","                        [-6.6863, -6.6879, -6.6672, -6.6351, -6.7278],\n","                        [ 7.0048,  6.9020,  7.0160,  6.9615,  6.9263]],\n","              \n","                       [[-6.9411, -6.8770, -6.9803, -6.9130, -6.9214],\n","                        [-6.9135, -6.9215, -6.9658, -6.8814, -6.8908],\n","                        [-6.8686, -6.9772, -6.9220, -6.9814, -6.9767],\n","                        [-7.0134, -6.8790, -6.9131, -7.0081, -7.0047],\n","                        [-6.9293, -7.0374, -7.0040, -6.9742, -7.0426]],\n","              \n","                       [[-6.9307, -6.9995, -7.0267, -6.8320, -6.8081],\n","                        [-6.9998, -6.9345, -6.9233, -6.7461, -6.9388],\n","                        [-6.9700, -6.9137, -6.9644, -6.9247,  6.7779],\n","                        [-6.9233, -6.9010,  6.5347,  6.9282,  6.7516],\n","                        [-7.0049, -6.8783,  6.6760, -6.6318,  6.8587]]],\n","              \n","              \n","                      [[[-7.0228, -7.0548, -6.9837, -7.0216, -6.9448],\n","                        [-6.8557, -7.0202, -7.0059, -6.9969, -6.9258],\n","                        [-6.9452, -6.9419, -6.9200, -6.9758, -6.8620],\n","                        [-6.9697, -6.9590, -6.8717, -6.8934, -6.9884],\n","                        [-6.9936, -6.9189, -6.9872, -6.9404, -6.9705]],\n","              \n","                       [[-6.8639, -6.9697,  6.8989,  6.9390, -6.8661],\n","                        [-6.9055, -6.9139, -6.8570, -6.8590, -6.9438],\n","                        [-6.8957, -6.8569, -6.8185, -6.8905, -6.9423],\n","                        [-6.8987, -6.8086, -6.8969, -6.8563, -6.9032],\n","                        [-6.8418, -6.8207, -6.8352, -6.8021, -6.8800]],\n","              \n","                       [[-6.8928, -6.8664, -6.8393, -6.8791, -6.9372],\n","                        [-6.9019, -6.9730, -6.9040, -6.9575, -6.9096],\n","                        [-6.9487, -6.9386, -6.9878, -6.9145, -6.8794],\n","                        [-7.0045, -6.9042, -6.9644, -6.9435, -6.9111],\n","                        [-6.9823, -6.9047, -6.8528, -6.8567, -6.8663]],\n","              \n","                       [[-6.8597, -6.9334, -6.7883, -6.7710, -6.8591],\n","                        [-6.9339, -6.7790, -6.9003, -6.8140, -6.8495],\n","                        [-6.8051, -6.8872, -6.7937, -6.8811, -6.7986],\n","                        [-6.8615, -6.8707, -6.8232, -6.8065, -6.7846],\n","                        [-6.9161, -6.9806, -6.9409, -6.8501, -6.8463]],\n","              \n","                       [[-6.8887, -6.9378, -6.9275, -6.9999, -6.9269],\n","                        [-6.8451, -6.9193, -6.9013, -7.0118, -6.9590],\n","                        [-6.9644, -6.9244, -6.8436, -6.8442, -6.8450],\n","                        [-6.9176, -6.9461, -6.9842, -6.9351, -6.8671],\n","                        [-6.8570, -6.8741, -6.9470, -6.9807, -6.8906]],\n","              \n","                       [[ 4.3053,  3.4494, -6.1055,  0.6990, -3.7470],\n","                        [-6.2859,  4.1479, -6.3322,  3.2688,  6.4958],\n","                        [ 5.7651, -5.0859, -6.6890, -5.2050,  6.9145],\n","                        [-6.7209,  6.1418,  3.2554,  4.5019,  6.9733],\n","                        [ 6.2039,  5.1192, -6.5770, -2.8270,  6.9330]]],\n","              \n","              \n","                      ...,\n","              \n","              \n","                      [[[-1.6854, -5.8257, -6.5843,  5.0030, -3.5817],\n","                        [-3.9214,  5.2594,  2.3119, -4.7280, -4.4006],\n","                        [-4.4732, -4.8906, -2.1450, -4.9885, -6.3285],\n","                        [ 4.4385, -2.9077, -3.5878, -5.0402, -5.6950],\n","                        [ 4.8809, -2.5584, -4.0244, -4.2587,  2.4980]],\n","              \n","                       [[ 6.7768,  6.7459,  7.0317,  7.0034,  6.9247],\n","                        [ 6.8253,  6.7640,  6.8970,  6.9250,  6.9871],\n","                        [ 6.8013,  6.7857,  6.8858,  6.8735,  6.9189],\n","                        [ 6.7445,  6.8123,  6.8559,  6.8231,  6.8787],\n","                        [ 6.6404,  6.7310,  6.7592,  6.7564,  6.7905]],\n","              \n","                       [[-1.1969,  0.1686,  0.7430,  2.0874,  1.9915],\n","                        [-0.9859, -0.3499,  0.8765,  0.5739, -0.2717],\n","                        [ 0.4120,  2.3821,  1.4547, -1.1379,  0.3179],\n","                        [-0.2333, -0.3679,  1.2611,  1.2051,  1.9145],\n","                        [-1.9722, -0.7486,  0.8843, -1.0592,  0.4302]],\n","              \n","                       [[ 6.8575,  6.7935,  6.8014,  6.8313,  6.8597],\n","                        [ 6.7982,  6.9009,  6.9158,  6.8838,  6.8609],\n","                        [ 6.9044,  6.9007,  6.9539,  6.8034,  6.9235],\n","                        [ 6.7805,  6.9094,  6.7469,  6.9252,  6.7989],\n","                        [ 6.7799,  6.8438,  6.7664,  6.8655,  6.7667]],\n","              \n","                       [[ 7.0119, -6.9097, -7.0067, -6.9756, -6.9708],\n","                        [ 7.0268,  6.8036, -6.9366, -7.0089, -6.8832],\n","                        [ 6.9669,  6.8558, -6.9639, -7.0014, -6.9915],\n","                        [ 7.0036,  6.9437, -6.9155, -6.9264, -6.9391],\n","                        [ 6.8861,  7.0306,  6.9679, -6.9744, -6.8706]],\n","              \n","                       [[-6.7278, -6.6603, -6.7014, -6.7435, -6.8324],\n","                        [-6.6373, -6.6770, -6.7974, -6.6818,  6.7690],\n","                        [-6.6246, -6.7984, -6.7304, -6.8216,  6.8671],\n","                        [-6.6251, -6.6437, -6.5993, -6.7061, -6.8491],\n","                        [-6.6415, -6.8672, -6.8605, -6.7574, -6.8431]]],\n","              \n","              \n","                      [[[-7.0134, -6.9879, -7.0394, -6.8930, -7.0108],\n","                        [-6.9152, -7.0037, -6.8763, -7.0129, -6.9001],\n","                        [-6.9241, -6.8898, -6.8658, -6.9966, -6.8433],\n","                        [-7.0041, -6.9117, -6.9577, -6.8950, -6.9180],\n","                        [-7.0332, -6.9650, -6.9711, -7.0349, -6.8733]],\n","              \n","                       [[-6.7205, -6.6343, -6.6234, -6.5647, -6.5469],\n","                        [-6.7291, -6.6050, -6.6117, -6.5634, -6.5585],\n","                        [-6.8271, -6.6133, -6.5873, -6.6148, -6.7362],\n","                        [-6.6913,  6.9009,  6.9888,  6.9813,  6.9579],\n","                        [-6.7375, -6.6512,  6.8978,  6.8496,  6.9001]],\n","              \n","                       [[-6.8645, -6.8455, -6.9112, -6.9109, -6.9506],\n","                        [-6.9544, -6.8822, -6.8670, -6.9962, -6.8675],\n","                        [-6.8600, -6.9484, -6.8503, -6.9331, -6.9260],\n","                        [-6.8868, -6.8902, -6.9201, -6.8496, -6.9640],\n","                        [-6.9405, -6.9591, -6.8727, -6.8704, -6.9266]],\n","              \n","                       [[-6.9386, -6.9021, -6.9201,  6.7544,  6.8915],\n","                        [-7.0091, -6.8997, -6.8793,  6.7837,  6.7261],\n","                        [ 6.7972,  6.8463,  6.8434,  6.8983,  6.8599],\n","                        [ 6.7865,  6.8357,  6.7491,  6.9555,  6.9581],\n","                        [ 6.7802,  6.8777,  6.7494,  6.8492,  6.8418]],\n","              \n","                       [[-6.9727, -7.0279, -6.8752, -7.0191, -6.9967],\n","                        [-6.9623, -6.9576, -6.9516, -6.8637, -6.9853],\n","                        [-7.0122, -6.9381, -6.9758, -6.9909, -6.8503],\n","                        [-6.8437, -6.9441, -6.8404, -6.9048, -6.8577],\n","                        [-6.9770, -6.8524, -6.9891, -6.9738, -6.9670]],\n","              \n","                       [[-6.7972, -6.6745, -6.8347, -6.7369, -6.7220],\n","                        [-6.7140, -6.7951, -6.8857, -6.6622, -6.7613],\n","                        [-6.7026, -6.6441, -6.7220, -6.8444, -6.7232],\n","                        [-6.8273, -6.6817, -6.6840, -6.6466, -6.5655],\n","                        [-6.8437, -6.7639, -6.6378, -6.8760, -6.6631]]],\n","              \n","              \n","                      [[[-7.0207, -6.9010, -6.9238, -6.8852, -6.9045],\n","                        [-6.9778, -6.9145, -6.8858, -6.9344, -6.8873],\n","                        [-6.9685, -6.9911, -6.9335, -6.8500, -6.9180],\n","                        [-6.9914, -7.0008, -7.0004, -6.9933, -6.8645],\n","                        [-6.8939, -6.8896, -6.8597, -6.8925, -6.8525]],\n","              \n","                       [[-6.8970, -6.9836, -6.9763, -6.9718, -6.8902],\n","                        [-6.9719, -6.9459, -6.8879, -6.9725, -6.8997],\n","                        [-6.9698, -7.0132, -6.8823, -6.9611, -7.0040],\n","                        [-6.8913, -6.9552, -6.9169, -7.0189, -7.0187],\n","                        [-6.9170, -6.8602, -6.9753, -6.8687, -6.8805]],\n","              \n","                       [[-6.9526, -6.8453, -6.8637, -6.9845, -6.9504],\n","                        [-6.9444, -6.9895, -7.0101, -6.9443, -6.8430],\n","                        [-6.8692, -6.9817, -7.0175, -6.9045, -7.0013],\n","                        [-6.9385, -6.8804, -6.9423, -6.8786, -7.0379],\n","                        [-6.9395, -7.0114, -6.9056, -7.0133, -6.9076]],\n","              \n","                       [[-6.9022, -6.9582, -6.9998, -6.9811, -7.0093],\n","                        [-6.9164, -6.8989, -7.0111, -6.9653, -6.8980],\n","                        [-7.0259, -6.9828, -6.9828, -6.9494, -6.9397],\n","                        [-7.0200, -6.9792, -7.0215, -7.0149, -6.9903],\n","                        [-6.9697, -6.9500, -7.0098, -6.9409, -6.9004]],\n","              \n","                       [[-6.8986, -6.9869, -6.9496, -7.0137, -6.9082],\n","                        [-6.8590, -7.0009, -6.8763, -6.9203, -6.9022],\n","                        [-6.9387, -6.9460, -6.9167, -6.8679, -6.8199],\n","                        [-7.0082, -7.0163, -6.8858, -6.9629, -6.9119],\n","                        [-6.9251, -6.9229, -7.0056, -6.8662, -6.8485]],\n","              \n","                       [[-3.3713,  3.4888, -4.4767, -1.7069,  5.1647],\n","                        [-5.6620,  3.2838,  4.8987,  2.5482, -4.1726],\n","                        [ 6.2650, -5.1463,  4.3028,  3.6448, -3.3800],\n","                        [-5.7477, -4.3226, -0.4897,  4.5678, -1.3086],\n","                        [-6.2008, -6.2988, -5.9246, -5.5358,  4.0482]]]])),\n","             ('conv2.bias',\n","              tensor([-6.8897, -6.8602, -6.9658,  1.6532, -6.9547, -6.8807, -6.9235, -6.9941, -6.8997,  0.2760, -6.9789, -6.9570])),\n","             ('fc1.weight',\n","              tensor([[-6.9693e+00, -6.9840e+00, -6.8929e+00,  ...,  6.8953e+00,  7.0132e+00, -7.0317e+00],\n","                      [-6.9878e+00, -6.8448e+00, -6.9324e+00,  ..., -6.9805e+00, -6.9679e+00, -7.0192e+00],\n","                      [-6.9329e+00, -6.8941e+00, -7.0174e+00,  ..., -6.9861e+00, -6.9810e+00, -6.9551e+00],\n","                      ...,\n","                      [ 6.9537e+00,  7.0185e+00,  6.9058e+00,  ...,  6.9822e+00, -6.9402e+00, -6.9119e+00],\n","                      [-6.9350e+00, -6.9209e+00, -7.0030e+00,  ..., -6.8892e+00, -6.9758e+00, -6.9060e+00],\n","                      [ 3.3874e-02, -1.0253e-03,  3.0656e-03,  ..., -5.0619e-02, -4.9712e-02, -1.9116e-02]])),\n","             ('fc1.bias',\n","              tensor([-6.6506, -5.6475, -4.9867, -6.9261,  6.7208,  1.0257,  3.3111, -7.0070, -5.6867, -7.0174, -6.9651, -6.8492,\n","                      -6.1705,  2.1239,  5.7643, -6.9837,  6.1952, -7.0038, -6.9361, -6.6496, -6.7291, -6.6107, -6.4541, -6.9156,\n","                      -6.9831,  4.7349, -6.9613, -6.9519, -6.8897, -5.8627, -6.9299, -6.9592, -4.5010, -7.0005, -5.3898, -6.9233,\n","                      -6.8582,  6.6638, -7.0127, -6.9622, -7.0133, -2.6859, -5.5911, -6.9241, -6.7678, -6.5110, -6.6993, -6.9000,\n","                      -6.8704, -6.9176,  6.6958, -6.8652, -6.7618, -6.8840, -6.8999, -6.9794, -6.6714, -6.9021, -6.9125, -6.9147,\n","                      -7.0038, -6.9075, -7.0297, -6.9452, -6.9056, -6.9155, -6.9314, -6.8288, -6.8457, -3.6153, -6.8668, -6.8553,\n","                      -6.9413, -6.8123, -6.5153,  0.1215, -5.8391, -6.5432, -6.9722, -6.3705, -6.9459, -6.9759, -6.9697, -6.8088,\n","                      -5.8952, -6.9168, -6.1211, -7.0054, -6.9126, -4.0086, -6.9683, -7.0339, -6.9119, -6.2142, -5.3670, -6.8697,\n","                      -6.8972, -6.9884, -4.0566, -6.9602, -6.9138, -6.9158, -6.8677, -6.8945, -6.3896, -6.9156,  3.5775, -6.8256,\n","                      -6.8706, -3.5770, -6.7348, -6.5448, -6.9287, -6.9621, -6.9593, -7.0068, -6.9675, -4.8565,  1.5921, -6.6663])),\n","             ('fc2.weight',\n","              tensor([[ 6.9953e+00, -6.8442e+00, -6.8744e+00,  ..., -6.9238e+00, -7.0326e+00, -3.6919e-02],\n","                      [-6.9682e+00, -6.9020e+00, -1.5074e+00,  ..., -6.8503e+00, -6.9334e+00,  1.9819e-02],\n","                      [-6.9964e+00, -6.1483e+00, -6.7989e+00,  ..., -4.7510e+00, -4.9256e+00, -5.8802e-02],\n","                      ...,\n","                      [-7.7670e-04, -5.6165e+00, -5.6974e+00,  ..., -6.9676e+00, -6.8223e+00,  1.6853e-02],\n","                      [-7.0186e+00, -7.0454e+00, -6.7589e+00,  ...,  4.6145e+00, -5.6990e+00, -4.6853e-02],\n","                      [-6.9505e+00, -7.0442e+00, -6.8970e+00,  ..., -6.9580e+00, -7.0073e+00,  1.5888e-02]])),\n","             ('fc2.bias',\n","              tensor([-6.8568, -5.8264, -4.7743, -6.9787, -5.7593, -6.9326, -6.6623, -6.8074, -4.5885, -4.5516, -7.0004, -7.0154,\n","                      -6.9647, -6.9331,  0.6179, -6.9806, -6.9467, -6.8697, -4.8366, -6.8247, -6.8602, -4.0225, -6.9995, -6.9733,\n","                      -6.7121, -6.3440, -6.9101, -6.7990, -6.9312, -6.0051, -6.9945, -5.9517, -7.0175, -6.8900, -5.7972, -6.8623,\n","                      -7.0147, -2.1997, -6.9019, -6.8458, -5.1667, -6.9760, -7.0420, -3.1394, -0.0902, -6.8348, -4.8058, -6.0383,\n","                      -2.2295, -0.4625, -6.9134, -4.4610, -4.6571, -7.0396, -6.9085, -6.7687, -6.7016, -6.9997, -6.6420, -6.9780])),\n","             ('out.weight',\n","              tensor([[ 6.6813,  6.8684, -4.9968, -6.8341,  6.7086, -6.8450,  6.5472, -6.8284, -6.9439,  4.3353, -5.2613, -6.8468,\n","                       -6.9492,  5.9708, -4.6257,  6.9532, -6.9374,  6.8844,  4.0044, -6.5422, -7.0398,  4.6345, -6.1390,  6.8998,\n","                        5.0323, -6.9091, -7.0029, -6.9382, -6.8251,  6.9201, -6.6114,  6.8453, -6.9569,  7.0563,  4.6820,  5.3653,\n","                        6.8544,  2.4157,  6.2277, -6.9768,  6.8165,  5.6652, -6.9495, -6.9470,  2.9065, -6.5318,  6.4415,  6.7747,\n","                        5.2521,  0.6838,  6.6615,  3.0486, -5.7808,  6.9576, -6.9780,  6.4997, -6.4839, -6.9895,  4.8314,  6.8391],\n","                      [ 3.4268, -6.8942,  5.6533, -6.7543,  6.7278, -6.9780, -6.9440,  7.0050, -4.7056, -3.9985, -5.4472, -6.8589,\n","                       -6.8834, -6.9662, -7.0256,  6.9621,  7.0325, -6.8793, -6.7574, -6.1166, -6.9054, -7.0202, -6.5897, -6.9364,\n","                       -6.8568, -7.0016, -6.8660,  7.0120,  6.7241, -6.8377, -6.7637, -6.8350, -6.8470, -6.9907,  3.3715, -5.1918,\n","                       -6.8617, -7.0269,  5.1309, -6.9453, -6.8040, -7.0269, -6.8332, -6.7180, -2.2647, -6.8563, -6.6921, -4.8375,\n","                       -3.6069,  4.0358, -6.0688,  1.6936, -6.9298, -6.8995, -6.9831, -6.8152, -7.0503, -6.8635,  0.3988,  6.9199],\n","                      [ 7.0727,  6.8479, -5.1110,  6.8663, -5.2292, -5.7596, -5.7215,  3.3715, -7.0362, -5.8539, -5.5423,  6.9083,\n","                        6.8237, -6.9233, -4.6846, -6.9745,  7.0724,  6.8350, -6.8527, -6.8454, -6.8919, -3.7896,  6.8901,  6.9011,\n","                       -6.8100, -6.6433, -7.0094,  6.9565,  6.9760,  2.2789, -6.5728,  6.7776,  6.8869,  6.8045,  4.9953,  4.6207,\n","                        4.6499,  4.7389, -4.0218,  1.8415,  6.8413, -6.8470,  7.0789,  6.1815, -3.2584,  6.6961,  6.0328,  6.5541,\n","                        6.3830, -3.6368, -6.8956,  0.6168, -6.7257, -4.8319, -6.8738,  2.1341, -6.0269,  6.8656,  3.9483, -6.9886],\n","                      [-4.3333,  7.0377,  6.5257, -7.0605, -5.4635,  7.0475,  6.4911,  6.9242,  2.5843,  5.5526, -5.4868,  6.8415,\n","                       -6.9255,  7.0651, -6.8934, -6.7904,  6.8652,  6.9006, -3.2571, -6.1002, -6.9040, -5.0276, -6.5141,  7.0155,\n","                       -6.5092,  6.2089, -7.0353,  6.9966, -6.7627,  1.9537, -6.6810,  6.8371,  6.8376,  7.0209, -2.0692, -5.6878,\n","                       -6.9291,  2.8876, -4.0765, -6.7250,  6.8540,  5.9709, -6.8606,  6.2699,  0.0481, -7.0521, -6.7397,  6.3890,\n","                        4.9838,  1.8168,  6.1182,  1.1194, -7.0180,  6.9510, -6.8886,  6.8045, -7.0215, -6.8815, -4.6281,  6.9198],\n","                      [ 7.0156, -6.8547,  6.3542,  6.8395, -5.1608, -5.7373,  7.0737,  7.0676, -0.0149, -4.6412, -5.6816,  7.0189,\n","                        6.9492, -7.0298, -6.8630, -7.0236,  6.8546,  6.8093, -4.1810, -6.6997, -6.8166, -4.0359, -6.7515,  6.8481,\n","                        4.3907, -6.2832, -6.9233,  7.0491,  6.8315,  1.8116, -6.8787, -6.7950,  6.9866,  6.9708,  5.3970,  4.5737,\n","                        7.0097,  6.2940, -4.1182, -6.6500, -6.8905,  7.0352,  6.9152, -3.0234, -1.3112, -6.7772,  6.4580,  6.4231,\n","                       -0.5199,  1.7881, -6.8605, -2.7093, -7.0134, -4.6390, -6.6350, -7.0412, -6.6017,  6.9464, -7.0862, -6.8711],\n","                      [ 4.0355, -6.6524,  6.4720,  6.7509, -6.8886, -4.7759,  5.6994,  0.3009, -1.8783, -5.3368,  6.4450, -6.8891,\n","                       -6.7714,  6.9036,  2.5550,  6.8587,  6.9369, -6.9790, -3.3574,  6.6776,  5.8411,  5.0911, -6.5555, -6.8484,\n","                        6.9747,  6.7286,  6.9376, -6.9100,  6.4593, -6.7356,  6.7498, -6.6925,  6.2631, -6.9373, -5.5958, -3.3031,\n","                       -6.9273, -3.4855, -3.7070,  6.0442,  5.4893,  6.6852, -6.8620, -3.4909, -0.8819,  6.4015, -5.7582, -6.8672,\n","                       -6.7642, -4.1377,  6.9267, -6.7747,  1.8575, -6.8638, -4.8369, -6.6651,  6.3832, -6.7986,  6.1663, -2.2026],\n","                      [-3.5989,  6.9388,  6.4556, -6.2840,  6.8015, -5.9013, -5.7216, -6.9656, -1.9290,  4.9931, -5.7529,  6.8281,\n","                        6.8336, -6.8047, -2.5062,  4.8822, -6.7907,  6.9474, -5.8915, -6.8496, -6.5845,  4.3993, -6.7014,  6.9984,\n","                       -6.3933, -5.2685,  6.9032, -6.9464,  6.6907,  1.8844, -6.6535,  6.7754, -6.7849,  6.8876, -6.8715, -5.8168,\n","                       -6.9386,  4.0662, -4.2632,  4.3291,  4.7537, -6.9362,  6.9554,  6.5808, -0.7921, -6.7668, -5.4124, -5.8559,\n","                        4.6607,  0.9918, -6.9342,  1.4172, -6.7940,  4.6923, -6.7854,  4.6724, -6.5729,  6.9487,  4.5001, -7.0516],\n","                      [-6.8135,  6.7698, -6.9196,  6.1378, -6.9686, -6.8781, -6.2750,  6.7735, -6.9912, -1.0957, -5.5058,  6.8925,\n","                       -6.8864,  6.8806,  2.5566, -7.0300,  6.7934,  5.8915,  4.8041,  6.6195,  0.1853, -6.4806,  6.8315, -6.9098,\n","                        6.8722,  6.7083,  6.8727, -7.0128, -6.8920, -6.8841,  6.9404,  6.6543, -7.0243, -6.8760,  5.6712,  5.3817,\n","                       -6.8127,  5.0010, -4.2230,  6.4775, -4.8166, -4.4280,  6.9426, -6.8245, -2.7077, -4.8601,  5.1496, -6.8487,\n","                       -6.9154, -6.9249,  7.0709, -7.0442,  1.6833, -6.9374,  6.5500,  6.8328, -4.5516, -6.7879,  4.9178, -4.3733],\n","                      [ 7.0552, -6.7846,  6.4633, -6.9278, -6.5207,  6.8512,  6.8564, -6.9003,  6.9130,  4.9265,  6.4813,  7.0745,\n","                       -6.9568, -7.0073,  1.5689, -6.9362, -6.9054,  6.8400, -3.7729,  6.7101, -6.7950,  5.0315, -6.5738,  6.8668,\n","                       -6.6959, -5.3361, -6.8285, -6.9839, -6.6636,  6.2769, -6.8611, -6.6578, -7.0514,  7.0597, -4.6904, -4.6763,\n","                       -4.5778,  4.0439, -3.9888, -6.2981, -6.8030,  6.9907,  7.0225, -6.3883,  4.7067, -6.7360,  6.5991,  6.7502,\n","                        2.8191,  0.0505, -7.0821,  1.5887, -1.5486,  4.7805, -6.7627, -6.7027, -6.4694,  6.9636,  4.6054,  4.5317],\n","                      [-7.0161, -6.8962, -6.7012, -5.8460, -6.5943, -7.0204, -6.0799,  7.0556, -2.5270, -6.5996, -5.4400, -6.9421,\n","                       -6.9040,  6.9138,  2.2725, -6.9668,  6.8321, -6.8532,  3.1877, -6.7880, -5.1161, -2.7746, -6.1375, -6.8704,\n","                       -6.9990,  6.6246, -7.0578, -6.8736,  6.9853, -6.8241, -6.9228, -6.8899, -6.8674, -6.9118, -3.1894, -6.9070,\n","                        6.8536, -2.9073, -4.0500, -3.5143, -6.8288,  6.8428,  6.9177, -6.3227, -7.0273,  6.8931, -6.9159, -6.5533,\n","                       -6.8387, -7.1032, -6.9879, -6.8810,  2.5305, -6.8528,  6.3404, -6.9437,  6.8177,  6.8814,  0.8377, -6.8809]])),\n","             ('out.bias',\n","              tensor([ 0.4166,  0.4137,  1.9004,  1.9367,  2.2949, -3.8909,  1.3729, -2.8623,  0.8650, -5.5598]))])"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"fWtQjtyljUDn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"8ae77c22-c6e6-4320-a7ae-5fb8aab7d209","executionInfo":{"status":"ok","timestamp":1561180148311,"user_tz":-330,"elapsed":1000,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}}},"source":["# Print Network's state_dict()\n","print(\"Network's state_dict:\")\n","\n","for param in network.state_dict():\n","    print(param, '\\t', network.state_dict()[param].size())"],"execution_count":88,"outputs":[{"output_type":"stream","text":["Network's state_dict:\n","conv1.weight \t torch.Size([6, 1, 5, 5])\n","conv1.bias \t torch.Size([6])\n","conv2.weight \t torch.Size([12, 6, 5, 5])\n","conv2.bias \t torch.Size([12])\n","fc1.weight \t torch.Size([120, 192])\n","fc1.bias \t torch.Size([120])\n","fc2.weight \t torch.Size([60, 120])\n","fc2.bias \t torch.Size([60])\n","out.weight \t torch.Size([10, 60])\n","out.bias \t torch.Size([10])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GFhPLQoFlxQg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":12847},"outputId":"6b9a9f25-fbed-491c-ea78-95afe4d893ba","executionInfo":{"status":"ok","timestamp":1561180252209,"user_tz":-330,"elapsed":1120,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}}},"source":["# Print optimizer's state_dict\n","print(\"Optimizer's state_dict:\")\n","\n","for var in optimizer.state_dict():\n","    print(var, \"\\t\", optimizer.state_dict()[var])"],"execution_count":92,"outputs":[{"output_type":"stream","text":["Optimizer's state_dict:\n","state \t {140386461173008: {'step': 1, 'exp_avg': tensor([[[[ 1.2894e-01,  1.9232e-01,  1.4679e-01,  1.5089e-01, -9.0329e-02],\n","          [ 1.9770e-01,  1.9655e-01,  6.5381e-02,  9.4888e-02,  2.3755e-02],\n","          [ 1.7709e-01,  1.2936e-01,  3.5401e-02,  2.3413e-02,  1.2030e-01],\n","          [ 1.2532e-01,  1.6179e-01,  9.8208e-02,  1.3321e-01,  2.4340e-01],\n","          [ 9.7097e-02,  6.0117e-02,  8.7895e-02,  1.5455e-01,  1.3766e-01]]],\n","\n","\n","        [[[ 4.2486e-02,  3.0028e-02,  3.1188e-02,  1.4412e-02,  1.9906e-02],\n","          [ 3.5880e-02,  2.8253e-02,  3.5600e-02,  3.0047e-02,  3.5933e-02],\n","          [ 3.0921e-02,  2.8818e-02,  4.5948e-02,  3.9171e-02,  3.9073e-02],\n","          [ 3.0650e-02,  3.1160e-02,  4.9593e-02,  3.9923e-02,  3.8416e-02],\n","          [ 3.0037e-02,  3.0310e-02,  4.4934e-02,  3.4523e-02,  3.0759e-02]]],\n","\n","\n","        [[[-5.3230e-02,  1.0418e-02,  2.6303e-02,  2.0749e-03,  1.2817e-02],\n","          [-2.3137e-02, -9.0293e-03,  1.1383e-03, -1.6971e-02,  4.5364e-03],\n","          [-1.7231e-02, -1.6827e-03,  5.4165e-03, -9.9274e-03,  3.8328e-03],\n","          [-5.8519e-02, -4.2232e-02, -2.0678e-02, -4.4548e-02, -6.2402e-05],\n","          [ 3.8213e-01, -8.0906e-02, -7.3483e-02, -2.8550e-02,  1.4114e-02]]],\n","\n","\n","        [[[ 1.9463e-02,  2.1348e-02,  1.9693e-02,  1.8443e-02,  2.7859e-02],\n","          [ 5.1317e-03,  1.0084e-02,  6.8611e-03,  6.6035e-03,  1.6573e-02],\n","          [ 8.8554e-03,  1.0607e-02,  4.5918e-03,  5.3687e-03,  1.2381e-02],\n","          [ 9.0050e-03,  9.9853e-03,  6.0125e-03,  4.3701e-03,  1.2401e-02],\n","          [ 1.7336e-02,  1.4981e-02,  1.1394e-02,  1.0052e-02,  1.6573e-02]]],\n","\n","\n","        [[[ 1.9520e-02,  1.6818e-02,  1.8655e-02,  2.2043e-02,  2.3649e-02],\n","          [ 1.9588e-02,  1.7620e-02,  2.0166e-02,  2.5945e-02,  2.6392e-02],\n","          [ 2.0972e-02,  2.3116e-02,  2.4830e-02,  2.6918e-02,  2.3963e-02],\n","          [ 2.2376e-02,  2.3041e-02,  2.1398e-02,  2.3469e-02,  2.2046e-02],\n","          [ 1.9706e-02,  1.9574e-02,  1.9067e-02,  2.3058e-02,  2.6284e-02]]],\n","\n","\n","        [[[ 4.7935e-03,  5.1382e-03,  6.1858e-03,  1.1619e-02,  1.1307e-02],\n","          [ 3.9336e-03,  4.1533e-03,  6.1036e-03,  1.2426e-02,  1.1314e-02],\n","          [ 2.6841e-03,  3.0565e-03,  4.8532e-03,  1.0992e-02,  9.7453e-03],\n","          [ 3.2094e-03,  3.9593e-03,  5.8505e-03,  1.1944e-02,  1.2228e-02],\n","          [ 3.5944e-03,  4.5656e-03,  7.0671e-03,  1.1969e-02,  1.2206e-02]]]]), 'exp_avg_sq': tensor([[[[1.6625e-03, 3.6987e-03, 2.1547e-03, 2.2769e-03, 8.1594e-04],\n","          [3.9087e-03, 3.8632e-03, 4.2747e-04, 9.0037e-04, 5.6429e-05],\n","          [3.1360e-03, 1.6733e-03, 1.2532e-04, 5.4817e-05, 1.4472e-03],\n","          [1.5706e-03, 2.6177e-03, 9.6448e-04, 1.7745e-03, 5.9243e-03],\n","          [9.4279e-04, 3.6140e-04, 7.7256e-04, 2.3887e-03, 1.8949e-03]]],\n","\n","\n","        [[[1.8051e-04, 9.0170e-05, 9.7266e-05, 2.0769e-05, 3.9625e-05],\n","          [1.2874e-04, 7.9823e-05, 1.2673e-04, 9.0282e-05, 1.2912e-04],\n","          [9.5613e-05, 8.3049e-05, 2.1112e-04, 1.5344e-04, 1.5267e-04],\n","          [9.3945e-05, 9.7093e-05, 2.4594e-04, 1.5938e-04, 1.4758e-04],\n","          [9.0220e-05, 9.1869e-05, 2.0191e-04, 1.1918e-04, 9.4611e-05]]],\n","\n","\n","        [[[2.8334e-04, 1.0853e-05, 6.9184e-05, 4.3054e-07, 1.6428e-05],\n","          [5.3530e-05, 8.1529e-06, 1.2956e-07, 2.8801e-05, 2.0579e-06],\n","          [2.9691e-05, 2.8313e-07, 2.9339e-06, 9.8554e-06, 1.4690e-06],\n","          [3.4245e-04, 1.7835e-04, 4.2758e-05, 1.9845e-04, 3.8941e-10],\n","          [1.4602e-02, 6.5458e-04, 5.3997e-04, 8.1509e-05, 1.9922e-05]]],\n","\n","\n","        [[[3.7881e-05, 4.5573e-05, 3.8783e-05, 3.4015e-05, 7.7611e-05],\n","          [2.6334e-06, 1.0170e-05, 4.7074e-06, 4.3606e-06, 2.7468e-05],\n","          [7.8417e-06, 1.1250e-05, 2.1084e-06, 2.8823e-06, 1.5329e-05],\n","          [8.1089e-06, 9.9705e-06, 3.6150e-06, 1.9098e-06, 1.5380e-05],\n","          [3.0054e-05, 2.2442e-05, 1.2981e-05, 1.0105e-05, 2.7465e-05]]],\n","\n","\n","        [[[3.8102e-05, 2.8285e-05, 3.4799e-05, 4.8591e-05, 5.5928e-05],\n","          [3.8367e-05, 3.1048e-05, 4.0667e-05, 6.7313e-05, 6.9656e-05],\n","          [4.3984e-05, 5.3434e-05, 6.1652e-05, 7.2458e-05, 5.7420e-05],\n","          [5.0069e-05, 5.3089e-05, 4.5789e-05, 5.5078e-05, 4.8602e-05],\n","          [3.8832e-05, 3.8315e-05, 3.6356e-05, 5.3166e-05, 6.9086e-05]]],\n","\n","\n","        [[[2.2977e-06, 2.6401e-06, 3.8264e-06, 1.3501e-05, 1.2784e-05],\n","          [1.5473e-06, 1.7250e-06, 3.7254e-06, 1.5441e-05, 1.2800e-05],\n","          [7.2045e-07, 9.3424e-07, 2.3553e-06, 1.2082e-05, 9.4972e-06],\n","          [1.0301e-06, 1.5676e-06, 3.4229e-06, 1.4266e-05, 1.4953e-05],\n","          [1.2920e-06, 2.0845e-06, 4.9945e-06, 1.4326e-05, 1.4898e-05]]]])}, 140386461173080: {'step': 1, 'exp_avg': tensor([1.2219, 0.0834, 0.4209, 0.0379, 0.0568, 0.0274]), 'exp_avg_sq': tensor([1.4930e-01, 6.9594e-04, 1.7719e-02, 1.4380e-04, 3.2271e-04, 7.4965e-05])}, 140386461173152: {'step': 1, 'exp_avg': tensor([[[[ 2.2584e-03,  2.6860e-03,  3.2870e-03,  2.9308e-03,  1.8199e-03],\n","          [ 3.4863e-03,  4.0157e-03,  4.8753e-03,  4.6290e-03,  3.0170e-03],\n","          [ 4.3022e-03,  5.0627e-03,  5.7658e-03,  5.2218e-03,  3.8855e-03],\n","          [ 5.6024e-03,  5.7896e-03,  6.1453e-03,  5.6702e-03,  4.0128e-03],\n","          [ 7.0312e-03,  6.9423e-03,  6.7991e-03,  5.7174e-03,  3.9119e-03]],\n","\n","         [[ 8.5209e-04,  8.8080e-04,  7.9228e-04,  8.3734e-04,  1.0741e-03],\n","          [ 7.8408e-04,  8.5716e-04,  8.6312e-04,  9.5504e-04,  1.1722e-03],\n","          [ 1.0173e-03,  1.0607e-03,  1.0589e-03,  1.1935e-03,  1.3992e-03],\n","          [ 9.7143e-04,  1.1402e-03,  1.3994e-03,  1.5659e-03,  1.7340e-03],\n","          [-4.6900e-05,  1.2281e-04,  5.5100e-04,  8.6209e-04,  1.2333e-03]],\n","\n","         [[ 1.8996e-03,  2.4025e-03,  2.5403e-03,  2.7338e-03,  2.8576e-03],\n","          [ 1.9845e-03,  2.6004e-03,  2.6270e-03,  2.6068e-03,  2.9362e-03],\n","          [ 2.1058e-03,  2.5490e-03,  2.2811e-03,  2.4385e-03,  2.8153e-03],\n","          [ 2.1678e-03,  2.4874e-03,  2.1943e-03,  2.3074e-03,  2.5431e-03],\n","          [ 2.0836e-03,  2.5510e-03,  2.2601e-03,  2.2903e-03,  2.5175e-03]],\n","\n","         [[-1.1283e-04, -1.1635e-04, -1.0954e-04, -1.0276e-04, -5.6894e-05],\n","          [-9.0950e-05, -8.4157e-05, -9.9697e-05, -8.7696e-05, -1.5818e-05],\n","          [-5.3580e-05, -6.0392e-05, -4.4712e-05, -4.2976e-05, -2.2138e-06],\n","          [-1.5548e-04, -2.1733e-04, -2.1424e-04, -1.8340e-04, -8.4889e-05],\n","          [-2.8454e-04, -3.0418e-04, -3.3670e-04, -3.6771e-04, -2.6165e-04]],\n","\n","         [[ 1.9878e-03,  2.5083e-03,  3.1277e-03,  2.8529e-03,  1.5849e-03],\n","          [ 2.9834e-03,  3.8719e-03,  4.3480e-03,  3.6995e-03,  2.7415e-03],\n","          [ 4.0319e-03,  4.3306e-03,  4.8733e-03,  4.4230e-03,  3.2437e-03],\n","          [ 4.7052e-03,  4.9502e-03,  5.3519e-03,  4.4174e-03,  3.1064e-03],\n","          [ 6.1348e-03,  6.0363e-03,  5.5718e-03,  4.8232e-03,  3.2813e-03]],\n","\n","         [[ 3.1345e-08,  1.0626e-07,  5.8579e-08,  3.1296e-08, -3.5871e-08],\n","          [ 1.9650e-07,  7.3476e-08,  3.2958e-08, -5.2781e-08,  3.3860e-08],\n","          [ 2.2299e-08,  1.7817e-07,  1.0713e-08, -8.3917e-09,  4.6730e-08],\n","          [-7.7207e-10,  2.9523e-08, -5.6317e-09,  2.2794e-08,  2.2131e-07],\n","          [ 5.1578e-08,  1.4438e-07, -7.0450e-08,  1.3801e-07,  4.2331e-07]]],\n","\n","\n","        [[[ 1.5535e-02,  1.0166e-02,  6.5486e-03,  3.9234e-03, -3.4434e-03],\n","          [ 1.0234e-02,  7.6922e-03,  6.6444e-03,  5.9696e-03,  1.6017e-03],\n","          [ 6.4285e-03,  7.9841e-03,  9.7763e-03,  1.0982e-02,  6.5481e-03],\n","          [ 1.4848e-04,  3.1048e-03,  7.4086e-03,  1.0004e-02,  6.5362e-03],\n","          [ 2.2251e-03,  5.2014e-03,  1.0387e-02,  1.3711e-02,  1.1467e-02]],\n","\n","         [[ 6.0978e-03,  5.7458e-03,  5.7221e-03,  4.1870e-03,  3.1873e-03],\n","          [ 5.1666e-03,  3.2221e-03,  2.4776e-04, -2.7938e-03, -2.9722e-03],\n","          [ 1.8288e-02,  1.3232e-02,  7.4717e-03,  2.5604e-03,  4.3550e-03],\n","          [ 1.5398e-02,  1.2768e-02,  1.2282e-02,  9.8764e-03,  8.7839e-03],\n","          [-7.4303e-03, -5.1815e-03, -3.2735e-03, -4.7315e-03, -7.8615e-03]],\n","\n","         [[ 1.4213e-03,  1.7563e-03,  1.9090e-03,  2.3063e-03,  2.7187e-03],\n","          [ 2.2182e-03,  2.4254e-03,  2.5230e-03,  3.0560e-03,  3.4339e-03],\n","          [ 3.0133e-03,  2.8670e-03,  2.8481e-03,  3.3614e-03,  3.7894e-03],\n","          [ 3.8875e-03,  3.4957e-03,  3.3663e-03,  3.6917e-03,  4.3121e-03],\n","          [ 3.7689e-03,  3.1845e-03,  2.7050e-03,  3.1314e-03,  3.7221e-03]],\n","\n","         [[-7.8116e-04, -6.8055e-04, -1.0142e-03, -1.1875e-03, -1.4367e-03],\n","          [-1.8821e-03, -1.6615e-03, -2.1693e-03, -2.4724e-03, -2.9516e-03],\n","          [ 2.4630e-04,  6.9823e-04,  5.0083e-04, -3.4196e-04, -9.3529e-04],\n","          [ 2.7107e-03,  4.2967e-03,  4.8471e-03,  3.3530e-03,  2.3363e-03],\n","          [-3.7688e-03, -2.2606e-03, -9.2627e-04, -5.4141e-04, -1.9331e-03]],\n","\n","         [[ 3.3579e-03,  3.5720e-03,  3.4905e-03,  3.4538e-03,  2.6407e-03],\n","          [ 2.7251e-03,  3.7133e-03,  4.2335e-03,  4.5191e-03,  3.4856e-03],\n","          [ 4.3151e-03,  5.2667e-03,  6.0376e-03,  6.4182e-03,  5.6272e-03],\n","          [ 7.7328e-03,  8.6194e-03,  9.8000e-03,  1.0179e-02,  9.2987e-03],\n","          [ 9.9896e-03,  1.1088e-02,  1.2233e-02,  1.2866e-02,  1.2292e-02]],\n","\n","         [[ 1.1125e-03,  3.5615e-04,  3.7570e-04,  1.0732e-03,  1.9534e-04],\n","          [ 1.1263e-03,  5.6856e-04,  3.4685e-04,  1.0499e-03,  5.9347e-05],\n","          [ 1.1127e-03,  2.6684e-04,  1.7234e-04,  7.0422e-04, -6.4722e-04],\n","          [ 9.4927e-04,  2.4954e-04, -4.1029e-05, -2.7807e-04, -1.9736e-03],\n","          [ 9.9205e-04,  3.2821e-04, -2.7600e-04,  3.6547e-04, -1.8486e-03]]],\n","\n","\n","        [[[ 1.3679e-02,  1.5363e-02,  1.6239e-02,  1.5572e-02,  1.4286e-02],\n","          [ 1.3861e-02,  1.5421e-02,  1.6360e-02,  1.5475e-02,  1.4437e-02],\n","          [ 1.4010e-02,  1.5310e-02,  1.6246e-02,  1.5694e-02,  1.4683e-02],\n","          [ 1.4082e-02,  1.5550e-02,  1.6484e-02,  1.5872e-02,  1.4830e-02],\n","          [ 1.4398e-02,  1.6341e-02,  1.7133e-02,  1.6700e-02,  1.5336e-02]],\n","\n","         [[ 3.9981e-04,  1.1409e-04, -2.2795e-04, -1.9924e-04,  6.2006e-06],\n","          [ 6.1036e-04,  4.0519e-04,  1.5175e-04,  2.0121e-04,  3.2738e-04],\n","          [ 5.6458e-04,  5.0786e-04,  3.7935e-04,  4.4899e-04,  4.3818e-04],\n","          [ 1.0775e-03,  8.9616e-04,  4.8297e-04,  5.9655e-04,  6.3783e-04],\n","          [ 1.6163e-03,  1.3119e-03,  9.0596e-04,  9.6747e-04,  1.0653e-03]],\n","\n","         [[ 2.7903e-03,  2.8489e-03,  3.1452e-03,  3.8121e-03,  4.1298e-03],\n","          [ 2.6200e-03,  2.7929e-03,  2.8020e-03,  3.4425e-03,  3.7563e-03],\n","          [ 2.6410e-03,  3.0341e-03,  2.9696e-03,  3.4119e-03,  3.4528e-03],\n","          [ 2.4905e-03,  2.9101e-03,  2.8082e-03,  3.4691e-03,  3.7157e-03],\n","          [ 2.4022e-03,  2.7460e-03,  2.9544e-03,  3.7949e-03,  3.9719e-03]],\n","\n","         [[ 1.0555e-03,  1.1703e-03,  1.3140e-03,  1.2878e-03,  1.2489e-03],\n","          [ 1.0675e-03,  1.2439e-03,  1.3428e-03,  1.3741e-03,  1.2983e-03],\n","          [ 1.1702e-03,  1.2461e-03,  1.2883e-03,  1.3391e-03,  1.2948e-03],\n","          [ 1.2455e-03,  1.3807e-03,  1.4612e-03,  1.4459e-03,  1.3675e-03],\n","          [ 9.7568e-04,  1.0137e-03,  1.0814e-03,  1.0987e-03,  1.0483e-03]],\n","\n","         [[ 1.2909e-02,  1.4565e-02,  1.5792e-02,  1.5334e-02,  1.5082e-02],\n","          [ 1.2826e-02,  1.4529e-02,  1.5832e-02,  1.5561e-02,  1.5108e-02],\n","          [ 1.3316e-02,  1.5035e-02,  1.5745e-02,  1.5019e-02,  1.4795e-02],\n","          [ 1.3322e-02,  1.4937e-02,  1.5837e-02,  1.5710e-02,  1.5573e-02],\n","          [ 1.3380e-02,  1.5584e-02,  1.6958e-02,  1.6861e-02,  1.6360e-02]],\n","\n","         [[-4.9316e-08, -3.3382e-08,  2.5010e-07, -3.7379e-09,  3.4943e-08],\n","          [ 3.5897e-07, -4.6720e-08,  3.5447e-07, -2.7038e-08, -6.6421e-07],\n","          [-1.6254e-07,  8.9909e-08,  1.0559e-06,  9.8132e-08, -1.7018e-06],\n","          [ 1.0179e-06, -2.8090e-07, -2.8695e-08, -6.0208e-08, -3.9183e-06],\n","          [-2.7426e-07, -8.9377e-08,  5.0457e-07,  2.2746e-08, -3.7661e-06]]],\n","\n","\n","        ...,\n","\n","\n","        [[[ 2.9812e-03,  1.8663e-02,  6.3907e-02, -1.8324e-02,  2.1631e-02],\n","          [ 1.2525e-02, -3.9414e-02, -1.2970e-03,  9.9115e-03,  8.1937e-02],\n","          [ 3.1115e-03,  1.7784e-03,  1.1989e-02,  3.9555e-02,  1.2419e-01],\n","          [-4.3916e-02,  4.0151e-03,  3.1368e-02,  2.8394e-02,  2.5661e-02],\n","          [-4.0334e-02,  6.6291e-03,  4.9580e-02,  4.5355e-02, -1.1326e-03]],\n","\n","         [[-4.3393e-03, -4.6940e-03, -7.3742e-03, -9.2336e-03, -1.0720e-02],\n","          [-1.4089e-02, -1.3642e-02, -1.5763e-02, -1.6493e-02, -1.9375e-02],\n","          [-1.8819e-02, -1.7881e-02, -1.7879e-02, -1.6255e-02, -1.7608e-02],\n","          [-1.2087e-02, -1.1778e-02, -1.2092e-02, -8.8424e-03, -7.8229e-03],\n","          [-5.3922e-03, -5.9682e-03, -8.2509e-03, -5.4024e-03, -4.0027e-03]],\n","\n","         [[-5.4629e-02,  2.5095e-01, -2.3959e-01, -7.3998e-02,  5.9923e-02],\n","          [ 6.4743e-02,  9.7954e-02, -2.3850e-02,  1.1327e-01,  3.0294e-01],\n","          [ 6.4260e-02, -2.1253e-02, -1.2092e-01, -4.0572e-02, -2.6143e-02],\n","          [ 1.6675e-01,  2.9512e-01, -1.6488e-01, -2.4932e-01, -2.7348e-01],\n","          [ 3.6259e-01,  2.4214e-01, -5.8560e-02, -2.4021e-01,  5.9816e-03]],\n","\n","         [[-4.4646e-04, -1.3422e-04, -8.0690e-04, -1.5912e-03, -2.0794e-03],\n","          [-8.4205e-04, -7.0827e-04, -1.0906e-03, -1.5080e-03, -2.0746e-03],\n","          [-1.8280e-03, -1.6217e-03, -1.5992e-03, -1.3576e-03, -1.6037e-03],\n","          [-1.1612e-03, -9.5287e-04, -5.7279e-04, -3.7505e-04, -4.5614e-04],\n","          [-7.3460e-04, -7.0481e-04, -4.8056e-04, -2.9820e-04, -2.6731e-04]],\n","\n","         [[-1.3966e-03,  7.8637e-05,  2.0010e-03,  3.5210e-03,  4.3668e-03],\n","          [-1.1911e-03, -1.3314e-04,  2.1145e-03,  3.7586e-03,  4.6705e-03],\n","          [-1.0822e-03, -8.6061e-05,  1.6654e-03,  3.4833e-03,  4.1533e-03],\n","          [-1.0801e-03, -7.0095e-04,  7.3478e-04,  2.3571e-03,  3.1071e-03],\n","          [-2.1070e-03, -1.8806e-03, -7.4834e-04,  7.3827e-04,  1.6208e-03]],\n","\n","         [[ 4.1103e-03,  2.0289e-03,  5.2694e-04,  6.1712e-04,  1.0098e-04],\n","          [ 3.5445e-03,  1.3867e-03,  2.8588e-04,  5.5622e-04, -5.8624e-04],\n","          [ 3.5369e-03,  1.8112e-03,  4.1664e-04,  1.7446e-03, -2.8962e-04],\n","          [ 3.6859e-03,  2.5118e-03,  5.1499e-04,  2.5946e-03,  4.1840e-05],\n","          [ 3.7974e-03,  3.1131e-03,  8.7847e-04,  2.6608e-03,  4.5752e-04]]],\n","\n","\n","        [[[ 2.0082e-02,  2.1954e-02,  2.1590e-02,  1.8148e-02,  1.5823e-02],\n","          [ 1.6436e-02,  1.8763e-02,  1.9131e-02,  1.6154e-02,  1.3012e-02],\n","          [ 1.5464e-02,  1.7089e-02,  1.7823e-02,  1.3774e-02,  1.0945e-02],\n","          [ 1.5368e-02,  1.7346e-02,  1.8838e-02,  1.4011e-02,  1.0798e-02],\n","          [ 1.4656e-02,  1.7754e-02,  1.9695e-02,  1.6683e-02,  1.2891e-02]],\n","\n","         [[ 9.1976e-03,  7.6151e-03,  3.6016e-03,  8.5677e-04,  1.7032e-04],\n","          [ 4.0767e-03,  3.5706e-03,  2.0205e-03,  1.4851e-03,  2.1839e-03],\n","          [ 5.7857e-03,  5.0272e-03,  4.1702e-03,  4.6198e-03,  4.4378e-03],\n","          [ 9.3180e-04, -1.3506e-03, -4.0020e-03, -6.3225e-03, -6.8556e-03],\n","          [ 1.7078e-03,  2.8495e-04, -8.7046e-04, -1.9107e-03, -1.6859e-03]],\n","\n","         [[ 8.1593e-04,  9.1007e-04,  2.1606e-03,  2.3267e-03,  3.3267e-03],\n","          [ 5.4761e-04,  5.6525e-04,  1.8830e-03,  1.9215e-03,  2.8047e-03],\n","          [ 4.5096e-04,  5.7178e-04,  1.6309e-03,  1.9357e-03,  3.0813e-03],\n","          [ 3.4055e-04,  3.1235e-04,  1.5621e-03,  1.8111e-03,  3.3609e-03],\n","          [ 4.0251e-04,  1.1531e-04,  9.9323e-04,  1.3336e-03,  2.9729e-03]],\n","\n","         [[ 1.0964e-03,  1.2418e-03,  7.2454e-04, -2.3095e-04, -5.9409e-04],\n","          [ 4.2724e-04,  5.5230e-04,  2.7240e-04, -4.4429e-04, -3.4380e-04],\n","          [-8.9681e-04, -9.5276e-04, -1.4812e-03, -2.0159e-03, -1.8748e-03],\n","          [-5.4619e-04, -2.8282e-04, -9.1807e-04, -1.6976e-03, -1.9645e-03],\n","          [-5.3890e-04, -3.5718e-04, -3.6761e-04, -6.9416e-04, -7.3216e-04]],\n","\n","         [[ 1.7167e-02,  1.8917e-02,  1.9969e-02,  1.8848e-02,  1.6984e-02],\n","          [ 1.6076e-02,  1.8123e-02,  1.8688e-02,  1.7385e-02,  1.6000e-02],\n","          [ 1.5042e-02,  1.6793e-02,  1.7510e-02,  1.6370e-02,  1.4709e-02],\n","          [ 1.4045e-02,  1.6534e-02,  1.7198e-02,  1.6525e-02,  1.5266e-02],\n","          [ 1.4040e-02,  1.6442e-02,  1.7535e-02,  1.7210e-02,  1.6013e-02]],\n","\n","         [[ 1.9346e-03,  2.9547e-03,  1.2689e-03,  1.6269e-03,  1.5968e-03],\n","          [ 1.7143e-03,  3.1239e-03,  1.3755e-03,  1.3198e-03,  1.6146e-03],\n","          [ 2.0527e-03,  3.4733e-03,  1.8228e-03,  1.4023e-03,  2.2496e-03],\n","          [ 2.0240e-03,  3.8993e-03,  2.2307e-03,  1.4843e-03,  2.3519e-03],\n","          [ 2.1035e-03,  4.0584e-03,  2.5950e-03,  1.8701e-03,  2.3674e-03]]],\n","\n","\n","        [[[ 4.7207e-03,  4.8962e-03,  5.0854e-03,  4.6395e-03,  4.7360e-03],\n","          [ 6.0425e-03,  5.9980e-03,  6.2386e-03,  5.5708e-03,  5.5438e-03],\n","          [ 6.9500e-03,  6.8457e-03,  6.8268e-03,  6.4315e-03,  6.2404e-03],\n","          [ 7.5375e-03,  7.2095e-03,  6.6860e-03,  6.2275e-03,  6.4015e-03],\n","          [ 8.1646e-03,  7.6558e-03,  6.9789e-03,  5.9512e-03,  5.8366e-03]],\n","\n","         [[ 9.3915e-04,  1.0179e-03,  1.1605e-03,  1.2807e-03,  1.3810e-03],\n","          [ 8.3332e-04,  8.9331e-04,  1.0795e-03,  1.2105e-03,  1.3408e-03],\n","          [ 1.0316e-03,  1.0559e-03,  1.3156e-03,  1.4019e-03,  1.4559e-03],\n","          [ 1.4423e-03,  1.4386e-03,  1.7571e-03,  1.8001e-03,  1.8083e-03],\n","          [ 1.3485e-03,  1.5527e-03,  1.7816e-03,  2.0261e-03,  2.0961e-03]],\n","\n","         [[ 3.2964e-03,  3.3128e-03,  3.3519e-03,  3.4547e-03,  3.0641e-03],\n","          [ 3.2492e-03,  3.1961e-03,  3.2437e-03,  3.5130e-03,  3.4966e-03],\n","          [ 3.1480e-03,  3.1768e-03,  3.3966e-03,  3.4317e-03,  3.1027e-03],\n","          [ 3.1021e-03,  3.3145e-03,  3.4291e-03,  3.4568e-03,  2.8846e-03],\n","          [ 2.9941e-03,  3.1369e-03,  3.4169e-03,  3.2437e-03,  3.0325e-03]],\n","\n","         [[ 8.9171e-06,  2.5132e-06,  1.8536e-05,  2.3950e-05,  2.1311e-05],\n","          [ 1.4574e-05,  9.5047e-06,  3.5537e-05,  3.6456e-05,  3.2202e-05],\n","          [ 3.7298e-05,  3.2408e-05,  5.6355e-05,  8.4362e-05,  8.4207e-05],\n","          [ 5.9920e-05,  6.5675e-05,  9.8418e-05,  1.0146e-04,  1.4196e-04],\n","          [ 7.8101e-05,  8.0688e-05,  9.9768e-05,  1.5909e-04,  1.5354e-04]],\n","\n","         [[ 5.3706e-03,  5.3068e-03,  5.1749e-03,  4.4151e-03,  4.3745e-03],\n","          [ 6.1353e-03,  5.9812e-03,  6.1029e-03,  5.4459e-03,  5.1143e-03],\n","          [ 6.9145e-03,  6.8891e-03,  6.5468e-03,  5.8276e-03,  5.6907e-03],\n","          [ 7.2849e-03,  7.1362e-03,  6.6710e-03,  5.5064e-03,  5.2328e-03],\n","          [ 7.7176e-03,  7.4168e-03,  6.5487e-03,  5.6262e-03,  5.0466e-03]],\n","\n","         [[ 2.8313e-08, -3.3495e-08,  5.7170e-08,  1.0483e-08, -8.5802e-08],\n","          [ 1.3885e-07, -2.8935e-08, -7.0815e-08, -1.7977e-08,  4.8866e-08],\n","          [-2.6291e-07,  9.2539e-08, -5.3109e-08, -3.3546e-08,  2.8693e-08],\n","          [ 1.3690e-07,  5.2382e-08,  2.1080e-09, -6.0099e-08,  7.4278e-09],\n","          [ 2.7046e-07,  3.0904e-07,  1.6790e-07,  1.1743e-07, -4.5301e-08]]]]), 'exp_avg_sq': tensor([[[[5.1005e-07, 7.2147e-07, 1.0804e-06, 8.5895e-07, 3.3120e-07],\n","          [1.2154e-06, 1.6126e-06, 2.3768e-06, 2.1428e-06, 9.1021e-07],\n","          [1.8509e-06, 2.5631e-06, 3.3244e-06, 2.7267e-06, 1.5097e-06],\n","          [3.1387e-06, 3.3519e-06, 3.7765e-06, 3.2151e-06, 1.6103e-06],\n","          [4.9437e-06, 4.8195e-06, 4.6228e-06, 3.2689e-06, 1.5303e-06]],\n","\n","         [[7.2605e-08, 7.7581e-08, 6.2770e-08, 7.0114e-08, 1.1536e-07],\n","          [6.1478e-08, 7.3472e-08, 7.4498e-08, 9.1210e-08, 1.3741e-07],\n","          [1.0349e-07, 1.1251e-07, 1.1213e-07, 1.4245e-07, 1.9578e-07],\n","          [9.4367e-08, 1.3002e-07, 1.9583e-07, 2.4520e-07, 3.0068e-07],\n","          [2.1996e-10, 1.5081e-09, 3.0361e-08, 7.4319e-08, 1.5211e-07]],\n","\n","         [[3.6086e-07, 5.7719e-07, 6.4532e-07, 7.4739e-07, 8.1658e-07],\n","          [3.9383e-07, 6.7619e-07, 6.9013e-07, 6.7953e-07, 8.6213e-07],\n","          [4.4344e-07, 6.4973e-07, 5.2036e-07, 5.9464e-07, 7.9260e-07],\n","          [4.6992e-07, 6.1873e-07, 4.8148e-07, 5.3239e-07, 6.4674e-07],\n","          [4.3413e-07, 6.5075e-07, 5.1079e-07, 5.2455e-07, 6.3376e-07]],\n","\n","         [[1.2732e-09, 1.3538e-09, 1.2000e-09, 1.0559e-09, 3.2369e-10],\n","          [8.2719e-10, 7.0825e-10, 9.9394e-10, 7.6907e-10, 2.5022e-11],\n","          [2.8708e-10, 3.6472e-10, 1.9991e-10, 1.8469e-10, 4.9009e-13],\n","          [2.4175e-09, 4.7234e-09, 4.5899e-09, 3.3637e-09, 7.2062e-10],\n","          [8.0964e-09, 9.2526e-09, 1.1336e-08, 1.3521e-08, 6.8462e-09]],\n","\n","         [[3.9512e-07, 6.2917e-07, 9.7822e-07, 8.1391e-07, 2.5121e-07],\n","          [8.9007e-07, 1.4991e-06, 1.8906e-06, 1.3687e-06, 7.5156e-07],\n","          [1.6256e-06, 1.8754e-06, 2.3749e-06, 1.9563e-06, 1.0521e-06],\n","          [2.2139e-06, 2.4505e-06, 2.8643e-06, 1.9514e-06, 9.6494e-07],\n","          [3.7635e-06, 3.6437e-06, 3.1045e-06, 2.3263e-06, 1.0767e-06]],\n","\n","         [[9.8248e-17, 1.1292e-15, 3.4315e-16, 9.7945e-17, 1.2867e-16],\n","          [3.8613e-15, 5.3987e-16, 1.0862e-16, 2.7859e-16, 1.1465e-16],\n","          [4.9725e-17, 3.1744e-15, 1.1476e-17, 7.0420e-18, 2.1837e-16],\n","          [5.9609e-20, 8.7159e-17, 3.1716e-18, 5.1957e-17, 4.8977e-15],\n","          [2.6603e-16, 2.0845e-15, 4.9632e-16, 1.9047e-15, 1.7919e-14]]],\n","\n","\n","        [[[2.4133e-05, 1.0335e-05, 4.2884e-06, 1.5393e-06, 1.1857e-06],\n","          [1.0474e-05, 5.9170e-06, 4.4148e-06, 3.5636e-06, 2.5654e-07],\n","          [4.1326e-06, 6.3746e-06, 9.5575e-06, 1.2060e-05, 4.2878e-06],\n","          [2.2045e-09, 9.6398e-07, 5.4887e-06, 1.0007e-05, 4.2722e-06],\n","          [4.9509e-07, 2.7055e-06, 1.0789e-05, 1.8799e-05, 1.3148e-05]],\n","\n","         [[3.7183e-06, 3.3014e-06, 3.2742e-06, 1.7531e-06, 1.0159e-06],\n","          [2.6694e-06, 1.0382e-06, 6.1386e-09, 7.8053e-07, 8.8340e-07],\n","          [3.3445e-05, 1.7508e-05, 5.5827e-06, 6.5556e-07, 1.8966e-06],\n","          [2.3711e-05, 1.6302e-05, 1.5086e-05, 9.7543e-06, 7.7157e-06],\n","          [5.5209e-06, 2.6848e-06, 1.0716e-06, 2.2387e-06, 6.1804e-06]],\n","\n","         [[2.0202e-07, 3.0845e-07, 3.6442e-07, 5.3189e-07, 7.3912e-07],\n","          [4.9205e-07, 5.8827e-07, 6.3655e-07, 9.3391e-07, 1.1792e-06],\n","          [9.0802e-07, 8.2199e-07, 8.1116e-07, 1.1299e-06, 1.4360e-06],\n","          [1.5113e-06, 1.2220e-06, 1.1332e-06, 1.3629e-06, 1.8594e-06],\n","          [1.4204e-06, 1.0141e-06, 7.3171e-07, 9.8054e-07, 1.3854e-06]],\n","\n","         [[6.1021e-08, 4.6315e-08, 1.0286e-07, 1.4101e-07, 2.0641e-07],\n","          [3.5424e-07, 2.7607e-07, 4.7057e-07, 6.1126e-07, 8.7118e-07],\n","          [6.0666e-09, 4.8752e-08, 2.5083e-08, 1.1694e-08, 8.7477e-08],\n","          [7.3480e-07, 1.8461e-06, 2.3495e-06, 1.1243e-06, 5.4585e-07],\n","          [1.4204e-06, 5.1105e-07, 8.5798e-08, 2.9313e-08, 3.7367e-07]],\n","\n","         [[1.1276e-06, 1.2759e-06, 1.2184e-06, 1.1929e-06, 6.9735e-07],\n","          [7.4261e-07, 1.3789e-06, 1.7923e-06, 2.0422e-06, 1.2150e-06],\n","          [1.8620e-06, 2.7738e-06, 3.6452e-06, 4.1193e-06, 3.1666e-06],\n","          [5.9796e-06, 7.4294e-06, 9.6040e-06, 1.0362e-05, 8.6466e-06],\n","          [9.9793e-06, 1.2294e-05, 1.4964e-05, 1.6553e-05, 1.5108e-05]],\n","\n","         [[1.2376e-07, 1.2684e-08, 1.4115e-08, 1.1518e-07, 3.8157e-09],\n","          [1.2685e-07, 3.2326e-08, 1.2031e-08, 1.1022e-07, 3.5220e-10],\n","          [1.2382e-07, 7.1206e-09, 2.9702e-09, 4.9593e-08, 4.1889e-08],\n","          [9.0111e-08, 6.2269e-09, 1.6834e-10, 7.7324e-09, 3.8950e-07],\n","          [9.8416e-08, 1.0772e-08, 7.6178e-09, 1.3357e-08, 3.4173e-07]]],\n","\n","\n","        [[[1.8712e-05, 2.3604e-05, 2.6370e-05, 2.4250e-05, 2.0410e-05],\n","          [1.9213e-05, 2.3779e-05, 2.6766e-05, 2.3949e-05, 2.0843e-05],\n","          [1.9629e-05, 2.3440e-05, 2.6392e-05, 2.4629e-05, 2.1558e-05],\n","          [1.9831e-05, 2.4182e-05, 2.7172e-05, 2.5193e-05, 2.1994e-05],\n","          [2.0730e-05, 2.6704e-05, 2.9353e-05, 2.7887e-05, 2.3521e-05]],\n","\n","         [[1.5985e-08, 1.3017e-09, 5.1963e-09, 3.9698e-09, 3.8448e-12],\n","          [3.7254e-08, 1.6417e-08, 2.3029e-09, 4.0487e-09, 1.0718e-08],\n","          [3.1875e-08, 2.5792e-08, 1.4391e-08, 2.0159e-08, 1.9200e-08],\n","          [1.1610e-07, 8.0310e-08, 2.3326e-08, 3.5587e-08, 4.0683e-08],\n","          [2.6124e-07, 1.7212e-07, 8.2076e-08, 9.3601e-08, 1.1348e-07]],\n","\n","         [[7.7856e-07, 8.1164e-07, 9.8923e-07, 1.4532e-06, 1.7056e-06],\n","          [6.8646e-07, 7.8005e-07, 7.8514e-07, 1.1851e-06, 1.4109e-06],\n","          [6.9751e-07, 9.2058e-07, 8.8183e-07, 1.1641e-06, 1.1922e-06],\n","          [6.2026e-07, 8.4684e-07, 7.8858e-07, 1.2035e-06, 1.3807e-06],\n","          [5.7708e-07, 7.5406e-07, 8.7285e-07, 1.4401e-06, 1.5776e-06]],\n","\n","         [[1.1140e-07, 1.3697e-07, 1.7265e-07, 1.6585e-07, 1.5598e-07],\n","          [1.1395e-07, 1.5473e-07, 1.8030e-07, 1.8881e-07, 1.6856e-07],\n","          [1.3694e-07, 1.5527e-07, 1.6598e-07, 1.7931e-07, 1.6765e-07],\n","          [1.5513e-07, 1.9063e-07, 2.1352e-07, 2.0906e-07, 1.8699e-07],\n","          [9.5196e-08, 1.0275e-07, 1.1694e-07, 1.2071e-07, 1.0988e-07]],\n","\n","         [[1.6664e-05, 2.1213e-05, 2.4938e-05, 2.3513e-05, 2.2747e-05],\n","          [1.6451e-05, 2.1108e-05, 2.5066e-05, 2.4215e-05, 2.2825e-05],\n","          [1.7732e-05, 2.2606e-05, 2.4789e-05, 2.2558e-05, 2.1890e-05],\n","          [1.7747e-05, 2.2311e-05, 2.5082e-05, 2.4682e-05, 2.4253e-05],\n","          [1.7902e-05, 2.4287e-05, 2.8759e-05, 2.8429e-05, 2.6764e-05]],\n","\n","         [[2.4320e-16, 1.1144e-16, 6.2551e-15, 1.3972e-18, 1.2210e-16],\n","          [1.2886e-14, 2.1828e-16, 1.2565e-14, 7.3107e-17, 4.4117e-14],\n","          [2.6420e-15, 8.0837e-16, 1.1149e-13, 9.6299e-16, 2.8961e-13],\n","          [1.0362e-13, 7.8903e-15, 8.2342e-17, 3.6250e-16, 1.5353e-12],\n","          [7.5219e-15, 7.9883e-16, 2.5460e-14, 5.1737e-17, 1.4184e-12]]],\n","\n","\n","        ...,\n","\n","\n","        [[[8.8877e-07, 3.4833e-05, 4.0841e-04, 3.3576e-05, 4.6790e-05],\n","          [1.5687e-05, 1.5535e-04, 1.6822e-07, 9.8237e-06, 6.7136e-04],\n","          [9.6816e-07, 3.1629e-07, 1.4374e-05, 1.5646e-04, 1.5424e-03],\n","          [1.9287e-04, 1.6121e-06, 9.8394e-05, 8.0620e-05, 6.5849e-05],\n","          [1.6268e-04, 4.3945e-06, 2.4582e-04, 2.0571e-04, 1.2828e-07]],\n","\n","         [[1.8829e-06, 2.2034e-06, 5.4379e-06, 8.5259e-06, 1.1491e-05],\n","          [1.9849e-05, 1.8610e-05, 2.4847e-05, 2.7201e-05, 3.7538e-05],\n","          [3.5417e-05, 3.1971e-05, 3.1967e-05, 2.6421e-05, 3.1005e-05],\n","          [1.4609e-05, 1.3872e-05, 1.4622e-05, 7.8187e-06, 6.1197e-06],\n","          [2.9076e-06, 3.5620e-06, 6.8077e-06, 2.9186e-06, 1.6021e-06]],\n","\n","         [[2.9843e-04, 6.2974e-03, 5.7402e-03, 5.4757e-04, 3.5907e-04],\n","          [4.1917e-04, 9.5951e-04, 5.6881e-05, 1.2831e-03, 9.1775e-03],\n","          [4.1293e-04, 4.5170e-05, 1.4622e-03, 1.6461e-04, 6.8345e-05],\n","          [2.7807e-03, 8.7098e-03, 2.7186e-03, 6.2159e-03, 7.4791e-03],\n","          [1.3147e-02, 5.8631e-03, 3.4292e-04, 5.7701e-03, 3.5780e-06]],\n","\n","         [[1.9933e-08, 1.8015e-09, 6.5108e-08, 2.5319e-07, 4.3239e-07],\n","          [7.0906e-08, 5.0164e-08, 1.1893e-07, 2.2740e-07, 4.3040e-07],\n","          [3.3416e-07, 2.6299e-07, 2.5576e-07, 1.8430e-07, 2.5720e-07],\n","          [1.3485e-07, 9.0796e-08, 3.2808e-08, 1.4066e-08, 2.0806e-08],\n","          [5.3964e-08, 4.9675e-08, 2.3093e-08, 8.8926e-09, 7.1456e-09]],\n","\n","         [[1.9506e-07, 6.1838e-10, 4.0038e-07, 1.2397e-06, 1.9069e-06],\n","          [1.4188e-07, 1.7726e-09, 4.4711e-07, 1.4127e-06, 2.1814e-06],\n","          [1.1712e-07, 7.4065e-10, 2.7737e-07, 1.2133e-06, 1.7250e-06],\n","          [1.1665e-07, 4.9133e-08, 5.3990e-08, 5.5560e-07, 9.6544e-07],\n","          [4.4395e-07, 3.5367e-07, 5.6002e-08, 5.4504e-08, 2.6269e-07]],\n","\n","         [[1.6894e-06, 4.1166e-07, 2.7766e-08, 3.8084e-08, 1.0197e-09],\n","          [1.2564e-06, 1.9230e-07, 8.1728e-09, 3.0938e-08, 3.4367e-08],\n","          [1.2509e-06, 3.2805e-07, 1.7359e-08, 3.0438e-07, 8.3877e-09],\n","          [1.3586e-06, 6.3089e-07, 2.6521e-08, 6.7317e-07, 1.7506e-10],\n","          [1.4421e-06, 9.6916e-07, 7.7171e-08, 7.0799e-07, 2.0932e-08]]],\n","\n","\n","        [[[4.0329e-05, 4.8196e-05, 4.6611e-05, 3.2934e-05, 2.5037e-05],\n","          [2.7014e-05, 3.5205e-05, 3.6601e-05, 2.6095e-05, 1.6932e-05],\n","          [2.3914e-05, 2.9202e-05, 3.1767e-05, 1.8971e-05, 1.1980e-05],\n","          [2.3617e-05, 3.0089e-05, 3.5488e-05, 1.9630e-05, 1.1661e-05],\n","          [2.1481e-05, 3.1521e-05, 3.8791e-05, 2.7831e-05, 1.6618e-05]],\n","\n","         [[8.4597e-06, 5.7989e-06, 1.2971e-06, 7.3405e-08, 2.9008e-09],\n","          [1.6620e-06, 1.2749e-06, 4.0824e-07, 2.2055e-07, 4.7695e-07],\n","          [3.3474e-06, 2.5272e-06, 1.7390e-06, 2.1343e-06, 1.9694e-06],\n","          [8.6826e-08, 1.8241e-07, 1.6016e-06, 3.9974e-06, 4.6999e-06],\n","          [2.9165e-07, 8.1197e-09, 7.5771e-08, 3.6507e-07, 2.8423e-07]],\n","\n","         [[6.6574e-08, 8.2822e-08, 4.6683e-07, 5.4133e-07, 1.1067e-06],\n","          [2.9988e-08, 3.1950e-08, 3.5456e-07, 3.6923e-07, 7.8664e-07],\n","          [2.0337e-08, 3.2693e-08, 2.6598e-07, 3.7470e-07, 9.4947e-07],\n","          [1.1598e-08, 9.7566e-09, 2.4401e-07, 3.2800e-07, 1.1295e-06],\n","          [1.6202e-08, 1.3296e-09, 9.8651e-08, 1.7786e-07, 8.8382e-07]],\n","\n","         [[1.2021e-07, 1.5420e-07, 5.2496e-08, 5.3338e-09, 3.5294e-08],\n","          [1.8253e-08, 3.0503e-08, 7.4202e-09, 1.9739e-08, 1.1820e-08],\n","          [8.0426e-08, 9.0775e-08, 2.1939e-07, 4.0639e-07, 3.5151e-07],\n","          [2.9832e-08, 7.9988e-09, 8.4284e-08, 2.8818e-07, 3.8591e-07],\n","          [2.9041e-08, 1.2758e-08, 1.3514e-08, 4.8186e-08, 5.3605e-08]],\n","\n","         [[2.9471e-05, 3.5785e-05, 3.9878e-05, 3.5526e-05, 2.8844e-05],\n","          [2.5845e-05, 3.2843e-05, 3.4923e-05, 3.0222e-05, 2.5599e-05],\n","          [2.2627e-05, 2.8201e-05, 3.0659e-05, 2.6799e-05, 2.1635e-05],\n","          [1.9727e-05, 2.7336e-05, 2.9576e-05, 2.7308e-05, 2.3307e-05],\n","          [1.9713e-05, 2.7035e-05, 3.0748e-05, 2.9617e-05, 2.5641e-05]],\n","\n","         [[3.7425e-07, 8.7305e-07, 1.6100e-07, 2.6468e-07, 2.5498e-07],\n","          [2.9388e-07, 9.7586e-07, 1.8920e-07, 1.7419e-07, 2.6068e-07],\n","          [4.2138e-07, 1.2064e-06, 3.3227e-07, 1.9663e-07, 5.0605e-07],\n","          [4.0964e-07, 1.5204e-06, 4.9761e-07, 2.2031e-07, 5.5315e-07],\n","          [4.4246e-07, 1.6470e-06, 6.7340e-07, 3.4974e-07, 5.6047e-07]]],\n","\n","\n","        [[[2.2285e-06, 2.3972e-06, 2.5862e-06, 2.1525e-06, 2.2430e-06],\n","          [3.6512e-06, 3.5977e-06, 3.8920e-06, 3.1034e-06, 3.0734e-06],\n","          [4.8302e-06, 4.6863e-06, 4.6606e-06, 4.1365e-06, 3.8942e-06],\n","          [5.6814e-06, 5.1977e-06, 4.4702e-06, 3.8781e-06, 4.0979e-06],\n","          [6.6660e-06, 5.8612e-06, 4.8705e-06, 3.5417e-06, 3.4066e-06]],\n","\n","         [[8.8201e-08, 1.0362e-07, 1.3467e-07, 1.6402e-07, 1.9071e-07],\n","          [6.9443e-08, 7.9800e-08, 1.1652e-07, 1.4654e-07, 1.7979e-07],\n","          [1.0641e-07, 1.1148e-07, 1.7308e-07, 1.9654e-07, 2.1198e-07],\n","          [2.0802e-07, 2.0695e-07, 3.0874e-07, 3.2402e-07, 3.2700e-07],\n","          [1.8185e-07, 2.4109e-07, 3.1743e-07, 4.1051e-07, 4.3936e-07]],\n","\n","         [[1.0866e-06, 1.0974e-06, 1.1235e-06, 1.1935e-06, 9.3885e-07],\n","          [1.0557e-06, 1.0215e-06, 1.0521e-06, 1.2341e-06, 1.2226e-06],\n","          [9.9101e-07, 1.0092e-06, 1.1537e-06, 1.1776e-06, 9.6269e-07],\n","          [9.6228e-07, 1.0986e-06, 1.1759e-06, 1.1949e-06, 8.3210e-07],\n","          [8.9648e-07, 9.8402e-07, 1.1675e-06, 1.0521e-06, 9.1958e-07]],\n","\n","         [[7.9515e-12, 6.3160e-13, 3.4358e-11, 5.7359e-11, 4.5416e-11],\n","          [2.1241e-11, 9.0339e-12, 1.2628e-10, 1.3290e-10, 1.0369e-10],\n","          [1.3911e-10, 1.0503e-10, 3.1759e-10, 7.1170e-10, 7.0908e-10],\n","          [3.5904e-10, 4.3132e-10, 9.6860e-10, 1.0295e-09, 2.0151e-09],\n","          [6.0998e-10, 6.5105e-10, 9.9537e-10, 2.5308e-09, 2.3576e-09]],\n","\n","         [[2.8843e-06, 2.8162e-06, 2.6779e-06, 1.9493e-06, 1.9136e-06],\n","          [3.7642e-06, 3.5775e-06, 3.7245e-06, 2.9658e-06, 2.6156e-06],\n","          [4.7810e-06, 4.7460e-06, 4.2861e-06, 3.3961e-06, 3.2384e-06],\n","          [5.3070e-06, 5.0925e-06, 4.4502e-06, 3.0320e-06, 2.7382e-06],\n","          [5.9561e-06, 5.5009e-06, 4.2885e-06, 3.1654e-06, 2.5468e-06]],\n","\n","         [[8.0165e-17, 1.1219e-16, 3.2684e-16, 1.0989e-17, 7.3620e-16],\n","          [1.9279e-15, 8.3726e-17, 5.0147e-16, 3.2317e-17, 2.3879e-16],\n","          [6.9120e-15, 8.5635e-16, 2.8206e-16, 1.1253e-16, 8.2331e-17],\n","          [1.8741e-15, 2.7438e-16, 4.4436e-19, 3.6119e-16, 5.5172e-18],\n","          [7.3147e-15, 9.5503e-15, 2.8190e-15, 1.3789e-15, 2.0522e-16]]]])}, 140386461173224: {'step': 1, 'exp_avg': tensor([ 0.0138,  0.0255,  0.0235, -0.0741,  0.0358,  0.0230,  0.0205,  0.1262,  0.0619,  0.0987,  0.0185,  0.0186]), 'exp_avg_sq': tensor([1.9140e-05, 6.5169e-05, 5.5168e-05, 5.4852e-04, 1.2825e-04, 5.2817e-05, 4.2172e-05, 1.5938e-03, 3.8265e-04,\n","        9.7499e-04, 3.4406e-05, 3.4514e-05])}, 140386461173296: {'step': 1, 'exp_avg': tensor([[ 1.7909e-05,  1.9350e-05,  3.0760e-05,  ..., -1.6686e-05, -4.7368e-05,  1.8494e-05],\n","        [ 1.7187e-04,  4.0742e-05,  7.9219e-05,  ...,  1.0158e-03,  1.0840e-03,  8.7013e-04],\n","        [ 1.1059e-04,  1.7743e-04,  1.2367e-04,  ...,  2.4973e-04,  5.6280e-05,  1.2661e-05],\n","        ...,\n","        [-5.4821e-04, -8.5802e-04, -5.5239e-04,  ..., -2.3316e-04,  3.7336e-04,  8.6274e-04],\n","        [ 5.9050e-04,  5.8845e-04,  5.7978e-04,  ...,  6.7355e-04,  7.4704e-04,  3.8427e-04],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,  0.0000e+00,  0.0000e+00]]), 'exp_avg_sq': tensor([[3.2073e-11, 3.7443e-11, 9.4620e-11,  ..., 2.7843e-11, 2.2437e-10, 3.4201e-11],\n","        [2.9541e-09, 1.6599e-10, 6.2757e-10,  ..., 1.0319e-07, 1.1750e-07, 7.5712e-08],\n","        [1.2229e-09, 3.1482e-09, 1.5294e-09,  ..., 6.2365e-09, 3.1674e-10, 1.6031e-11],\n","        ...,\n","        [3.0053e-08, 7.3620e-08, 3.0513e-08,  ..., 5.4362e-09, 1.3940e-08, 7.4432e-08],\n","        [3.4868e-08, 3.4627e-08, 3.3615e-08,  ..., 4.5366e-08, 5.5807e-08, 1.4766e-08],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]])}, 140386461173368: {'step': 1, 'exp_avg': tensor([ 8.5525e-04,  7.2440e-03,  9.9962e-03,  3.7664e-04, -8.4799e-03,  3.4610e-02, -3.3498e-02,  9.0151e-06,\n","         2.7113e-02,  3.6016e-05,  8.4700e-04,  1.6759e-03,  1.1039e-02,  9.2138e-04, -6.6724e-03,  1.5702e-03,\n","        -1.0292e-03,  1.2388e-05,  1.3496e-02,  1.4128e-02,  1.3650e-03,  1.8361e-04,  2.9016e-03,  1.7130e-06,\n","         2.7373e-03, -1.0345e-01,  1.2492e-05,  8.4014e-05,  2.2377e-03,  8.4482e-03,  1.4942e-03,  3.0985e-04,\n","         2.0437e-02,  1.4718e-03,  2.8276e-02,  2.6316e-06,  2.3469e-03, -2.1125e-03,  1.1271e-04,  1.1491e-03,\n","         3.1505e-04,  3.5636e-03,  3.8896e-02,  3.9138e-04,  6.2365e-03,  9.4342e-03,  1.2425e-04,  2.1235e-06,\n","         3.2777e-06,  3.7775e-05, -3.9022e-03,  2.2713e-03,  1.3147e-03,  2.1193e-03,  9.8553e-04,  2.4157e-04,\n","         3.3141e-02,  1.3684e-03,  2.3348e-04,  3.2450e-04,  1.9175e-05,  1.5244e-03,  5.8522e-05,  1.3050e-05,\n","         1.5358e-03,  1.5939e-03,  1.1571e-03,  2.0637e-02,  3.6530e-06,  6.2775e-02,  7.5820e-04,  8.0333e-02,\n","         2.6401e-04,  2.0348e-04,  2.4743e-02,  1.6070e-03,  9.4172e-03,  6.1924e-03,  7.9723e-04,  7.8765e-03,\n","         3.3219e-03,  1.9219e-04,  1.4789e-04,  4.8932e-03,  4.0864e-03,  7.6233e-03,  2.2426e-07,  1.2483e-04,\n","         2.0389e-03,  1.1699e-03,  3.1952e-05,  3.6460e-05,  7.1807e-03,  1.4964e-02,  1.1230e-02,  7.3971e-03,\n","         1.5758e-03,  1.1419e-02,  1.7063e-03,  1.4879e-03,  2.5113e-03,  2.0889e-04,  1.3639e-03,  2.3221e-03,\n","         4.7662e-02,  3.1818e-04, -4.3124e-04,  3.3016e-02,  1.9914e-03, -1.1253e-01,  1.5338e-03,  2.2734e-03,\n","         2.0344e-02,  2.6226e-04,  1.1590e-03,  8.8056e-06,  2.3764e-04,  1.8047e-02,  3.3408e-03,  8.5127e-07]), 'exp_avg_sq': tensor([7.3145e-08, 5.2476e-06, 9.9925e-06, 1.4186e-08, 7.1908e-06, 1.1978e-04, 1.1221e-04, 8.1271e-12, 7.3512e-05,\n","        1.2972e-10, 7.1741e-08, 2.8085e-07, 1.2187e-05, 8.4894e-08, 4.4521e-06, 2.4654e-07, 1.0592e-07, 1.5347e-11,\n","        1.8215e-05, 1.9959e-05, 1.8633e-07, 3.3712e-09, 8.4192e-07, 2.9344e-13, 7.4927e-07, 1.0703e-03, 1.5605e-11,\n","        7.0584e-10, 5.0074e-07, 7.1373e-06, 2.2327e-07, 9.6004e-09, 4.1767e-05, 2.1663e-07, 7.9952e-05, 6.9252e-13,\n","        5.5079e-07, 4.4628e-07, 1.2705e-09, 1.3205e-07, 9.9260e-09, 1.2699e-06, 1.5129e-04, 1.5318e-08, 3.8894e-06,\n","        8.9004e-06, 1.5438e-09, 4.5091e-13, 1.0743e-12, 1.4269e-10, 1.5227e-06, 5.1590e-07, 1.7285e-07, 4.4915e-07,\n","        9.7128e-08, 5.8356e-09, 1.0983e-04, 1.8724e-07, 5.4515e-09, 1.0530e-08, 3.6768e-11, 2.3238e-07, 3.4248e-10,\n","        1.7029e-11, 2.3587e-07, 2.5406e-07, 1.3389e-07, 4.2589e-05, 1.3344e-12, 3.9407e-04, 5.7486e-08, 6.4533e-04,\n","        6.9704e-09, 4.1404e-09, 6.1220e-05, 2.5825e-07, 8.8684e-06, 3.8346e-06, 6.3557e-08, 6.2039e-06, 1.1035e-06,\n","        3.6939e-09, 2.1871e-09, 2.3943e-06, 1.6699e-06, 5.8115e-06, 5.0292e-15, 1.5582e-09, 4.1571e-07, 1.3688e-07,\n","        1.0209e-10, 1.3293e-10, 5.1563e-06, 2.2392e-05, 1.2610e-05, 5.4717e-06, 2.4833e-07, 1.3039e-05, 2.9113e-07,\n","        2.2140e-07, 6.3067e-07, 4.3635e-09, 1.8601e-07, 5.3922e-07, 2.2717e-04, 1.0124e-08, 1.8597e-08, 1.0901e-04,\n","        3.9655e-07, 1.2663e-03, 2.3525e-07, 5.1685e-07, 4.1389e-05, 6.8782e-09, 1.3432e-07, 7.7538e-12, 5.6474e-09,\n","        3.2568e-05, 1.1161e-06, 7.2467e-14])}, 140386461173440: {'step': 1, 'exp_avg': tensor([[-5.8865e-05,  6.2945e-04,  4.3325e-05,  ...,  3.7450e-03,  6.4853e-04,  0.0000e+00],\n","        [ 7.5500e-06,  4.0532e-03,  5.3044e-03,  ...,  2.2261e-03,  1.7476e-03,  0.0000e+00],\n","        [ 1.0025e-04,  1.0000e-02,  6.3756e-03,  ...,  1.0218e-01,  3.7650e-03,  0.0000e+00],\n","        ...,\n","        [ 0.0000e+00,  1.0086e-06,  2.5663e-07,  ...,  1.0988e-04,  2.5046e-06,  0.0000e+00],\n","        [ 5.8282e-05,  5.2022e-05,  3.3072e-04,  ..., -3.9759e-03,  1.4084e-04,  0.0000e+00],\n","        [ 2.7971e-06,  4.1200e-04,  5.4887e-04,  ...,  1.1127e-04,  2.3134e-05,  0.0000e+00]]), 'exp_avg_sq': tensor([[3.4650e-10, 3.9621e-08, 1.8771e-10,  ..., 1.4025e-06, 4.2059e-08, 0.0000e+00],\n","        [5.7003e-12, 1.6429e-06, 2.8137e-06,  ..., 4.9555e-07, 3.0540e-07, 0.0000e+00],\n","        [1.0050e-09, 1.0001e-05, 4.0649e-06,  ..., 1.0440e-03, 1.4175e-06, 0.0000e+00],\n","        ...,\n","        [0.0000e+00, 1.0172e-13, 6.5859e-15,  ..., 1.2073e-09, 6.2731e-13, 0.0000e+00],\n","        [3.3968e-10, 2.7062e-10, 1.0938e-08,  ..., 1.5808e-06, 1.9836e-09, 0.0000e+00],\n","        [7.8240e-13, 1.6975e-08, 3.0126e-08,  ..., 1.2382e-09, 5.3519e-11, 0.0000e+00]])}, 140386461173512: {'step': 1, 'exp_avg': tensor([ 1.5600e-03,  1.5168e-03,  2.5252e-02,  1.2532e-02,  1.2182e-02,  1.5209e-03,  1.1546e-02,  1.3845e-02,\n","         4.0390e-03,  3.1932e-02,  4.0253e-05,  4.5540e-04,  2.1176e-04,  8.5898e-04, -1.9479e-01,  5.6687e-04,\n","         2.6688e-03,  1.6889e-03,  2.6454e-02,  3.0643e-04,  3.8984e-02,  1.0746e-01,  5.8446e-05,  2.8689e-03,\n","         5.5048e-03,  1.1465e-02,  5.0623e-05,  4.0629e-03,  9.9003e-03,  2.1280e-02,  3.1873e-05,  5.0679e-03,\n","         1.5090e-04,  2.2453e-03,  1.4887e-02,  5.6922e-03,  1.0453e-03,  1.8192e-01,  1.9198e-06,  2.5450e-03,\n","         7.2679e-04,  1.2628e-02,  9.3840e-04,  7.4866e-02,  2.8748e-02,  1.7751e-02,  8.0489e-03,  1.2250e-02,\n","         1.0245e-01, -8.2293e-03,  1.1953e-03,  7.4570e-02,  1.7651e-02,  6.4389e-03,  1.1249e-02,  2.4614e-03,\n","         1.4991e-02,  1.0543e-03,  9.5808e-03,  5.2042e-03]), 'exp_avg_sq': tensor([2.4337e-07, 2.3006e-07, 6.3767e-05, 1.5704e-05, 1.4840e-05, 2.3131e-07, 1.3332e-05, 1.9169e-05, 1.6314e-06,\n","        1.0197e-04, 1.6203e-10, 2.0739e-08, 4.4844e-09, 7.3785e-08, 3.7942e-03, 3.2134e-08, 7.1223e-07, 2.8524e-07,\n","        6.9983e-05, 9.3899e-09, 1.5198e-04, 1.1548e-03, 3.4160e-10, 8.2305e-07, 3.0303e-06, 1.3145e-05, 2.5626e-10,\n","        1.6507e-06, 9.8016e-06, 4.5285e-05, 1.0159e-10, 2.5684e-06, 2.2771e-09, 5.0413e-07, 2.2162e-05, 3.2401e-06,\n","        1.0927e-07, 3.3096e-03, 3.6858e-13, 6.4771e-07, 5.2822e-08, 1.5945e-05, 8.8059e-08, 5.6049e-04, 8.2643e-05,\n","        3.1509e-05, 6.4785e-06, 1.5007e-05, 1.0497e-03, 6.7721e-06, 1.4287e-07, 5.5607e-04, 3.1157e-05, 4.1460e-06,\n","        1.2655e-05, 6.0584e-07, 2.2472e-05, 1.1115e-07, 9.1791e-06, 2.7084e-06])}, 140386461173584: {'step': 1, 'exp_avg': tensor([[-8.1235e-03, -1.2821e-02,  1.2016e-02,  7.1180e-03, -5.5989e-02,  1.3612e-03, -4.2760e-02,  2.6043e-02,\n","          3.2900e-05, -2.0028e-02,  1.0436e-07,  3.5704e-05,  6.1571e-06, -1.0772e-05,  5.3544e-03, -9.5398e-05,\n","          3.0651e-02, -2.8501e-03, -1.4927e-03,  1.2267e-06,  1.5151e-03, -1.3333e-03,  2.2840e-07, -7.5283e-04,\n","         -2.9259e-02,  2.4090e-02,  7.7870e-06,  3.1618e-03,  9.2104e-03, -2.5080e-02,  6.4327e-07, -1.2468e-02,\n","          7.3628e-06, -1.7442e-03, -3.2430e-03, -1.2283e-03, -1.9577e-05, -1.9619e-02, -2.6738e-07,  9.5357e-03,\n","         -4.2071e-03, -1.8461e-02,  1.5063e-04,  2.2678e-03, -8.0221e-03,  5.3886e-04, -1.2146e-02, -1.8251e-02,\n","         -8.7717e-02,  2.8749e-01, -1.1946e-04, -2.1723e-02,  9.2751e-04, -5.4342e-05,  5.1612e-03, -1.9311e-03,\n","          7.4312e-03,  2.9624e-05, -6.5946e-03, -4.0511e-05],\n","        [-3.2536e-04,  6.4320e-03, -4.1996e-02,  1.3015e-03, -3.8210e-02,  4.6259e-04,  4.9021e-03, -9.7820e-03,\n","          6.0521e-04,  1.7825e-02,  1.0808e-07,  1.1927e-04,  7.4131e-06,  2.8421e-05,  1.5896e-02, -3.0759e-05,\n","         -1.0073e-02,  6.8915e-03,  4.7122e-03,  3.7878e-07,  4.5157e-03,  2.6572e-02,  7.3688e-07,  6.8960e-04,\n","          7.2410e-05,  4.9657e-02,  3.7695e-05, -8.0646e-03, -1.6751e-02,  2.8051e-03,  3.3375e-06,  8.4760e-03,\n","          1.4156e-05,  4.2152e-03, -4.1194e-05,  4.2209e-06,  4.0036e-05,  4.4310e-02, -8.6949e-08,  5.1828e-03,\n","          1.1975e-02,  1.1248e-02,  3.2020e-04,  2.1896e-02, -2.4905e-04,  1.9041e-02,  4.0802e-05,  3.4425e-05,\n","          9.0389e-02, -6.7552e-01,  2.2519e-04, -9.2912e-02,  1.1977e-02,  2.8516e-05,  5.0023e-03,  4.6651e-03,\n","          3.3596e-02,  1.3373e-04,  4.1351e-03, -2.2673e-04],\n","        [-3.3216e-03, -3.2196e-03,  2.7000e-02, -1.7561e-03,  7.3438e-03,  1.4881e-05,  3.8920e-03, -1.0746e-03,\n","          3.1837e-03,  7.8798e-02,  1.2859e-07, -2.1612e-05, -2.2706e-05,  5.6601e-05,  5.1044e-02,  4.4675e-05,\n","         -1.1657e-03, -2.8209e-03,  8.8285e-03,  3.5654e-04,  7.6712e-03,  3.4478e-03, -2.9267e-06, -1.0422e-03,\n","          3.3633e-05,  6.9979e-03,  2.0607e-05, -3.1576e-04, -1.2223e-02,  6.4593e-03,  8.4548e-07, -4.3353e-03,\n","         -9.5753e-06, -2.4827e-03, -6.9311e-03, -4.4509e-04, -4.1212e-05, -1.3483e-02,  4.6011e-08,  1.7599e-03,\n","         -1.7800e-03,  3.2893e-03, -1.5021e-04, -2.2250e-02,  6.1920e-04, -8.4853e-03, -2.3449e-02, -1.1737e-02,\n","         -2.3003e-02,  5.0108e-02,  1.0062e-04, -7.6925e-02,  2.5317e-02,  3.3908e-04,  1.3891e-02,  1.0151e-03,\n","          2.6210e-03, -1.5807e-04, -8.1787e-04,  1.3590e-04],\n","        [ 6.9631e-03, -8.7299e-03, -2.5652e-03,  3.3919e-03,  3.5566e-03, -7.3689e-03, -4.0991e-03, -4.7662e-03,\n","         -1.2420e-04, -7.6459e-02,  1.2536e-07, -1.4853e-04,  1.4711e-05, -1.4720e-05,  5.5443e-03,  5.1522e-05,\n","         -3.1990e-03, -3.5467e-03,  2.3130e-02,  3.2783e-07,  7.6176e-03, -1.8434e-02,  4.2243e-07, -6.9717e-04,\n","          7.8478e-03, -5.0214e-03,  3.5005e-05, -1.1747e-02,  1.0770e-02,  1.0501e-02,  8.9145e-07, -5.5394e-03,\n","         -1.5619e-05, -1.0138e-03,  2.2481e-03,  1.0071e-04,  2.8733e-05, -2.5506e-02,  4.4928e-08,  3.3293e-03,\n","         -4.8266e-03, -3.3933e-03,  2.2880e-04, -8.2612e-03,  1.9047e-03,  1.0570e-02,  9.1874e-03, -2.6693e-02,\n","         -6.9473e-02, -6.6448e-02, -2.1696e-04,  5.7790e-02,  2.6148e-02, -5.2377e-05,  7.6520e-03, -1.7309e-03,\n","          1.0558e-02,  1.4092e-04,  1.7981e-02, -1.5969e-04],\n","        [-3.3126e-04,  1.0475e-03, -4.6148e-02, -4.4130e-03,  1.1574e-02,  2.3057e-03, -2.1069e-03, -1.7737e-02,\n","          9.1597e-04,  1.1376e-01,  1.3319e-07, -1.0560e-04, -2.9632e-05,  4.2023e-05,  9.2830e-03,  6.2991e-05,\n","         -1.8757e-02, -6.6694e-05,  1.0897e-04,  2.9495e-04,  8.5325e-03,  4.9420e-03,  9.0875e-07, -8.2318e-04,\n","         -1.9639e-04,  2.0791e-02,  5.7992e-05, -1.5039e-02, -2.2645e-03,  2.9904e-02,  1.5571e-06,  7.7690e-04,\n","         -5.2849e-05, -7.5501e-04, -4.1268e-04, -7.4130e-04, -4.4619e-05, -1.6171e-02,  4.8373e-08,  3.9709e-03,\n","          2.4119e-03, -6.2115e-04, -1.7106e-04,  1.2463e-02,  1.6247e-02,  1.3566e-02, -4.9111e-03, -1.0459e-03,\n","          1.0425e-02,  1.8925e-01,  2.8275e-04,  6.3675e-02,  1.7315e-02,  6.2488e-04,  1.4439e-02,  6.9256e-04,\n","          1.0931e-02, -1.8446e-04,  1.0908e-04,  1.5807e-04],\n","        [-7.6440e-04,  2.6657e-02, -3.2014e-02, -4.5229e-03,  1.1014e-02, -7.3907e-04, -7.1480e-03, -4.0853e-04,\n","          1.1766e-03,  3.2870e-03, -4.0702e-07,  1.9648e-04,  4.5616e-06, -8.3294e-05,  7.5222e-01, -9.3054e-05,\n","         -1.7307e-03,  7.9791e-03,  6.0277e-04, -2.3549e-03, -6.0324e-02,  1.6958e-02,  3.7901e-07,  2.0798e-03,\n","         -6.4796e-05, -6.0075e-02, -7.9864e-05,  9.0228e-03, -4.7660e-03,  3.2208e-03, -1.2560e-06,  7.7018e-03,\n","         -2.6982e-05,  3.0567e-03,  4.9070e-04,  6.3162e-04,  2.6945e-05,  4.9368e-02,  3.8526e-08, -2.1579e-02,\n","         -2.4856e-02, -8.5186e-03,  2.3139e-04, -1.4306e-03, -5.3471e-02, -4.7087e-02,  2.9604e-02,  2.5148e-02,\n","          1.7648e-02,  5.4130e-02, -3.5080e-04,  2.0109e-02,  5.5005e-01,  3.8904e-05,  1.1079e-02,  1.4498e-02,\n","         -2.2665e-02,  1.1201e-04, -1.9313e-02, -4.3048e-03],\n","        [ 3.7230e-03, -3.3878e-03, -2.0201e-02,  2.7540e-03, -3.0713e-02,  2.2026e-03,  4.4064e-02,  1.0357e-02,\n","         -3.2026e-03, -1.1975e-01,  1.3169e-07, -1.1734e-04, -1.5331e-05,  2.9213e-06,  7.0827e-02, -4.7091e-05,\n","          1.1104e-02, -1.9269e-03,  1.2666e-02,  1.0281e-04,  7.1162e-03, -1.2812e-02,  8.9186e-07, -6.8652e-04,\n","          1.9960e-02,  2.1932e-03, -6.0884e-05,  2.5547e-03, -9.2387e-03,  9.4194e-03,  8.4905e-07, -6.9295e-04,\n","          1.8604e-06, -2.9789e-03,  3.5518e-03,  3.1300e-03,  7.0004e-06, -3.5732e-02,  4.7784e-08,  1.0198e-02,\n","          2.4376e-04,  3.0569e-02, -1.1259e-04, -2.8528e-02,  2.0622e-02,  1.6115e-02,  8.3971e-03,  5.5625e-03,\n","         -4.9856e-04,  2.7319e-03,  1.5369e-04,  6.6618e-02,  1.2778e-02, -4.5411e-04,  7.1751e-03,  1.2954e-03,\n","          7.9587e-03, -4.6702e-05, -6.0182e-04,  4.7504e-03],\n","        [ 2.1321e-03, -2.3351e-02,  7.2596e-02, -2.3474e-02,  5.3588e-02,  3.5731e-03,  1.3314e-03, -1.8589e-03,\n","          6.0400e-04,  3.6959e-04,  1.2540e-07, -3.9742e-05,  4.1986e-06, -1.1523e-05,  1.7472e-02,  6.6757e-05,\n","         -4.7889e-03, -3.7614e-03, -5.7798e-02, -7.8686e-04,  9.2536e-03,  6.4286e-04, -1.5987e-06,  7.8495e-04,\n","         -1.4654e-04, -3.7418e-02, -2.6428e-05,  1.1424e-02,  1.4680e-02,  6.4162e-03, -1.1193e-05, -4.8083e-03,\n","          3.1277e-05,  4.3566e-03, -7.7479e-04, -3.8465e-03,  3.6410e-06, -1.4629e-02,  4.5934e-08, -1.9299e-02,\n","          5.3202e-03,  1.3563e-03, -1.3412e-04,  9.4836e-03, -5.8826e-03,  3.7527e-02, -1.0780e-02,  2.9607e-02,\n","          6.2300e-02,  7.0514e-02, -1.9818e-04,  1.2088e-02, -7.2768e-01,  5.2458e-05, -2.8398e-02, -3.2570e-02,\n","          2.7271e-03,  1.3781e-04, -3.7327e-03,  2.3170e-04],\n","        [-3.8859e-03,  7.0751e-03, -2.0935e-02,  9.9018e-04,  7.1364e-03, -7.8238e-03, -4.6873e-03,  4.0681e-03,\n","         -3.3356e-03, -8.4740e-03, -5.6035e-07, -1.8894e-04,  1.4882e-05,  4.0607e-05, -1.1983e-01,  1.4694e-05,\n","          3.0850e-03, -4.6496e-03,  2.2650e-02, -6.9591e-04,  3.1515e-03, -1.1686e-02,  7.1225e-07, -1.5542e-04,\n","          1.7266e-03,  2.7711e-03,  2.3606e-06,  4.2804e-03,  1.1619e-02, -4.7417e-02,  1.4720e-06,  6.1123e-03,\n","          6.5266e-06, -3.3433e-03,  4.3243e-03,  2.3640e-03,  1.0921e-05, -3.2946e-02,  4.2141e-08,  2.4202e-03,\n","          1.0443e-02, -8.2179e-03, -9.4304e-05,  7.6986e-03,  9.6342e-03,  9.2066e-03, -2.2248e-02, -2.9801e-03,\n","         -2.7108e-02,  1.1051e-02,  1.1939e-04, -3.3772e-02, -3.1995e-02, -5.4574e-04,  1.6604e-02,  1.1899e-02,\n","          3.4559e-03, -1.2738e-04, -5.0708e-03, -7.2410e-04],\n","        [ 3.9339e-03,  1.0298e-02,  5.2248e-02,  1.8611e-02,  3.0700e-02,  6.0116e-03,  6.6119e-03, -4.8405e-03,\n","          1.4405e-04,  1.0673e-02,  1.1069e-07,  2.7031e-04,  1.5745e-05, -5.0265e-05, -8.0781e-01,  2.5663e-05,\n","         -5.1256e-03,  4.7517e-03, -1.3408e-02,  3.0814e-03,  1.0950e-02, -8.2970e-03,  2.4586e-07,  6.0294e-04,\n","          2.6318e-05, -3.9858e-03,  5.7291e-06,  4.7237e-03, -1.0369e-03,  3.7719e-03,  2.8536e-06,  4.7767e-03,\n","          4.3843e-05,  6.8942e-04,  7.8782e-04,  3.0656e-05, -1.1869e-05,  6.4410e-02,  4.0633e-08,  4.4807e-03,\n","          5.2753e-03, -7.2501e-03, -2.6873e-04,  6.6611e-03,  1.8598e-02, -5.0991e-02,  2.6305e-02,  3.5352e-04,\n","          2.7038e-02,  7.6694e-02,  3.7596e-06,  5.0519e-03,  1.1516e-01,  2.2727e-05, -5.2605e-02,  2.1667e-03,\n","         -5.6613e-02, -3.7493e-05,  1.3906e-02,  1.7983e-04]]), 'exp_avg_sq': tensor([[6.5991e-06, 1.6438e-05, 1.4438e-05, 5.0666e-06, 3.1348e-04, 1.8530e-07, 1.8285e-04, 6.7822e-05, 1.0824e-10,\n","         4.0112e-05, 1.0892e-15, 1.2748e-10, 3.7910e-12, 1.1603e-11, 2.8670e-06, 9.1008e-10, 9.3948e-05, 8.1233e-07,\n","         2.2283e-07, 1.5049e-13, 2.2957e-07, 1.7777e-07, 5.2165e-15, 5.6675e-08, 8.5608e-05, 5.8034e-05, 6.0637e-12,\n","         9.9970e-07, 8.4831e-06, 6.2902e-05, 4.1380e-14, 1.5544e-05, 5.4210e-12, 3.0422e-07, 1.0517e-06, 1.5088e-07,\n","         3.8325e-11, 3.8492e-05, 7.1492e-15, 9.0929e-06, 1.7700e-06, 3.4080e-05, 2.2690e-09, 5.1428e-07, 6.4355e-06,\n","         2.9037e-08, 1.4752e-05, 3.3309e-05, 7.6943e-04, 8.2651e-03, 1.4270e-09, 4.7188e-05, 8.6028e-08, 2.9530e-10,\n","         2.6638e-06, 3.7291e-07, 5.5222e-06, 8.7759e-11, 4.3488e-06, 1.6411e-10],\n","        [1.0586e-08, 4.1371e-06, 1.7637e-04, 1.6938e-07, 1.4600e-04, 2.1399e-08, 2.4031e-06, 9.5688e-06, 3.6628e-08,\n","         3.1773e-05, 1.1682e-15, 1.4225e-09, 5.4954e-12, 8.0774e-11, 2.5267e-05, 9.4609e-11, 1.0146e-05, 4.7493e-06,\n","         2.2205e-06, 1.4347e-14, 2.0392e-06, 7.0606e-05, 5.4299e-14, 4.7555e-08, 5.2432e-10, 2.4658e-04, 1.4209e-10,\n","         6.5037e-06, 2.8059e-05, 7.8688e-07, 1.1139e-12, 7.1842e-06, 2.0038e-11, 1.7768e-06, 1.6970e-10, 1.7816e-12,\n","         1.6029e-10, 1.9634e-04, 7.5602e-16, 2.6861e-06, 1.4341e-05, 1.2651e-05, 1.0253e-08, 4.7945e-05, 6.2028e-09,\n","         3.6256e-05, 1.6648e-10, 1.1851e-10, 8.1701e-04, 4.5632e-02, 5.0713e-09, 8.6327e-04, 1.4345e-05, 8.1319e-11,\n","         2.5023e-06, 2.1763e-06, 1.1287e-04, 1.7883e-09, 1.7099e-06, 5.1408e-09],\n","        [1.1033e-06, 1.0366e-06, 7.2900e-05, 3.0839e-07, 5.3932e-06, 2.2146e-11, 1.5148e-06, 1.1548e-07, 1.0136e-06,\n","         6.2092e-04, 1.6535e-15, 4.6707e-11, 5.1556e-11, 3.2037e-10, 2.6055e-04, 1.9959e-10, 1.3589e-07, 7.9575e-07,\n","         7.7943e-06, 1.2712e-08, 5.8847e-06, 1.1887e-06, 8.5657e-13, 1.0862e-07, 1.1312e-10, 4.8971e-06, 4.2466e-11,\n","         9.9706e-09, 1.4939e-05, 4.1722e-06, 7.1483e-14, 1.8795e-06, 9.1685e-12, 6.1639e-07, 4.8040e-06, 1.9811e-08,\n","         1.6984e-10, 1.8180e-05, 2.1170e-16, 3.0974e-07, 3.1686e-07, 1.0820e-06, 2.2564e-09, 4.9508e-05, 3.8341e-08,\n","         7.2000e-06, 5.4987e-05, 1.3775e-05, 5.2916e-05, 2.5108e-04, 1.0124e-09, 5.9174e-04, 6.4097e-05, 1.1498e-08,\n","         1.9296e-05, 1.0304e-07, 6.8694e-07, 2.4985e-09, 6.6891e-08, 1.8470e-09],\n","        [4.8485e-06, 7.6211e-06, 6.5805e-07, 1.1505e-06, 1.2650e-06, 5.4301e-06, 1.6802e-06, 2.2716e-06, 1.5426e-09,\n","         5.8459e-04, 1.5716e-15, 2.2060e-09, 2.1643e-11, 2.1666e-11, 3.0740e-06, 2.6545e-10, 1.0234e-06, 1.2579e-06,\n","         5.3499e-05, 1.0747e-14, 5.8027e-06, 3.3982e-05, 1.7845e-14, 4.8605e-08, 6.1588e-06, 2.5214e-06, 1.2254e-10,\n","         1.3800e-05, 1.1600e-05, 1.1027e-05, 7.9468e-14, 3.0685e-06, 2.4394e-11, 1.0277e-07, 5.0541e-07, 1.0142e-09,\n","         8.2558e-11, 6.5056e-05, 2.0185e-16, 1.1084e-06, 2.3296e-06, 1.1515e-06, 5.2348e-09, 6.8248e-06, 3.6278e-07,\n","         1.1173e-05, 8.4408e-06, 7.1250e-05, 4.8265e-04, 4.4154e-04, 4.7074e-09, 3.3397e-04, 6.8370e-05, 2.7433e-10,\n","         5.8553e-06, 2.9959e-07, 1.1147e-05, 1.9860e-09, 3.2331e-05, 2.5501e-09],\n","        [1.0973e-08, 1.0973e-07, 2.1297e-04, 1.9475e-06, 1.3395e-05, 5.3162e-07, 4.4389e-07, 3.1459e-05, 8.3900e-08,\n","         1.2941e-03, 1.7741e-15, 1.1152e-09, 8.7806e-11, 1.7660e-10, 8.6174e-06, 3.9679e-10, 3.5183e-05, 4.4481e-10,\n","         1.1875e-09, 8.6996e-09, 7.2803e-06, 2.4423e-06, 8.2582e-14, 6.7762e-08, 3.8571e-09, 4.3226e-05, 3.3630e-10,\n","         2.2618e-05, 5.1281e-07, 8.9424e-05, 2.4244e-13, 6.0357e-08, 2.7930e-10, 5.7004e-08, 1.7030e-08, 5.4952e-08,\n","         1.9909e-10, 2.6151e-05, 2.3400e-16, 1.5768e-06, 5.8175e-07, 3.8583e-08, 2.9260e-09, 1.5532e-05, 2.6397e-05,\n","         1.8403e-05, 2.4119e-06, 1.0939e-07, 1.0869e-05, 3.5814e-03, 7.9950e-09, 4.0545e-04, 2.9980e-05, 3.9047e-08,\n","         2.0849e-05, 4.7964e-08, 1.1949e-05, 3.4024e-09, 1.1898e-09, 2.4986e-09],\n","        [5.8431e-08, 7.1058e-05, 1.0249e-04, 2.0457e-06, 1.2131e-05, 5.4622e-08, 5.1094e-06, 1.6689e-08, 1.3843e-07,\n","         1.0804e-06, 1.6566e-14, 3.8605e-09, 2.0808e-12, 6.9379e-10, 5.6584e-02, 8.6591e-10, 2.9955e-07, 6.3666e-06,\n","         3.6333e-08, 5.5454e-07, 3.6389e-04, 2.8758e-05, 1.4365e-14, 4.3257e-07, 4.1985e-10, 3.6090e-04, 6.3782e-10,\n","         8.1412e-06, 2.2714e-06, 1.0374e-06, 1.5776e-13, 5.9317e-06, 7.2804e-11, 9.3432e-07, 2.4079e-08, 3.9894e-08,\n","         7.2605e-11, 2.4372e-04, 1.4842e-16, 4.6565e-05, 6.1780e-05, 7.2566e-06, 5.3543e-09, 2.0465e-07, 2.8591e-04,\n","         2.2172e-04, 8.7638e-05, 6.3243e-05, 3.1145e-05, 2.9301e-04, 1.2306e-08, 4.0437e-05, 3.0256e-02, 1.5135e-10,\n","         1.2274e-05, 2.1020e-05, 5.1372e-05, 1.2546e-09, 3.7300e-05, 1.8532e-06],\n","        [1.3860e-06, 1.1477e-06, 4.0809e-05, 7.5846e-07, 9.4328e-05, 4.8516e-07, 1.9417e-04, 1.0726e-05, 1.0257e-06,\n","         1.4340e-03, 1.7342e-15, 1.3769e-09, 2.3503e-11, 8.5337e-13, 5.0165e-04, 2.2175e-10, 1.2330e-05, 3.7130e-07,\n","         1.6043e-05, 1.0569e-09, 5.0641e-06, 1.6415e-05, 7.9542e-14, 4.7131e-08, 3.9840e-05, 4.8102e-07, 3.7069e-10,\n","         6.5267e-07, 8.5353e-06, 8.8725e-06, 7.2089e-14, 4.8017e-08, 3.4610e-13, 8.8739e-07, 1.2615e-06, 9.7969e-07,\n","         4.9005e-12, 1.2768e-04, 2.2833e-16, 1.0400e-05, 5.9417e-09, 9.3444e-05, 1.2677e-09, 8.1385e-05, 4.2526e-05,\n","         2.5969e-05, 7.0511e-06, 3.0942e-06, 2.4857e-08, 7.4634e-07, 2.3622e-09, 4.4379e-04, 1.6329e-05, 2.0622e-08,\n","         5.1483e-06, 1.6782e-07, 6.3340e-06, 2.1811e-10, 3.6218e-08, 2.2566e-06],\n","        [4.5458e-07, 5.4525e-05, 5.2702e-04, 5.5104e-05, 2.8716e-04, 1.2767e-06, 1.7726e-07, 3.4554e-07, 3.6481e-08,\n","         1.3660e-08, 1.5725e-15, 1.5795e-10, 1.7628e-12, 1.3278e-11, 3.0526e-05, 4.4565e-10, 2.2933e-06, 1.4148e-06,\n","         3.3406e-04, 6.1915e-08, 8.5630e-06, 4.1327e-08, 2.5559e-13, 6.1615e-08, 2.1473e-09, 1.4001e-04, 6.9844e-11,\n","         1.3050e-05, 2.1549e-05, 4.1168e-06, 1.2529e-11, 2.3119e-06, 9.7824e-11, 1.8980e-06, 6.0031e-08, 1.4795e-06,\n","         1.3257e-12, 2.1402e-05, 2.1099e-16, 3.7243e-05, 2.8305e-06, 1.8396e-07, 1.7989e-09, 8.9939e-06, 3.4605e-06,\n","         1.4082e-04, 1.1621e-05, 8.7658e-05, 3.8813e-04, 4.9722e-04, 3.9275e-09, 1.4613e-05, 5.2952e-02, 2.7518e-10,\n","         8.0645e-05, 1.0608e-04, 7.4373e-07, 1.8992e-09, 1.3933e-06, 5.3684e-09],\n","        [1.5100e-06, 5.0057e-06, 4.3828e-05, 9.8046e-08, 5.0928e-06, 6.1211e-06, 2.1971e-06, 1.6550e-06, 1.1126e-06,\n","         7.1809e-06, 3.1399e-14, 3.5698e-09, 2.2146e-11, 1.6490e-10, 1.4360e-03, 2.1592e-11, 9.5171e-07, 2.1619e-06,\n","         5.1303e-05, 4.8429e-08, 9.9319e-07, 1.3656e-05, 5.0730e-14, 2.4157e-09, 2.9810e-07, 7.6791e-07, 5.5724e-13,\n","         1.8322e-06, 1.3500e-05, 2.2484e-04, 2.1668e-13, 3.7360e-06, 4.2596e-12, 1.1178e-06, 1.8699e-06, 5.5885e-07,\n","         1.1928e-11, 1.0855e-04, 1.7759e-16, 5.8575e-07, 1.0905e-05, 6.7534e-06, 8.8932e-10, 5.9268e-06, 9.2819e-06,\n","         8.4762e-06, 4.9498e-05, 8.8807e-07, 7.3487e-05, 1.2213e-05, 1.4253e-09, 1.1406e-04, 1.0237e-04, 2.9783e-08,\n","         2.7569e-05, 1.4160e-05, 1.1943e-06, 1.6226e-09, 2.5713e-06, 5.2432e-08],\n","        [1.5475e-06, 1.0604e-05, 2.7298e-04, 3.4636e-05, 9.4251e-05, 3.6140e-06, 4.3718e-06, 2.3431e-06, 2.0751e-09,\n","         1.1391e-05, 1.2252e-15, 7.3066e-09, 2.4792e-11, 2.5266e-10, 6.5256e-02, 6.5858e-11, 2.6272e-06, 2.2578e-06,\n","         1.7976e-05, 9.4951e-07, 1.1991e-05, 6.8841e-06, 6.0450e-15, 3.6354e-08, 6.9263e-11, 1.5887e-06, 3.2823e-12,\n","         2.2314e-06, 1.0752e-07, 1.4228e-06, 8.1431e-13, 2.2817e-06, 1.9222e-10, 4.7530e-08, 6.2066e-08, 9.3982e-11,\n","         1.4088e-11, 4.1487e-04, 1.6510e-16, 2.0077e-06, 2.7829e-06, 5.2564e-06, 7.2217e-09, 4.4370e-06, 3.4587e-05,\n","         2.6001e-04, 6.9198e-05, 1.2498e-08, 7.3107e-05, 5.8820e-04, 1.4134e-12, 2.5521e-06, 1.3262e-03, 5.1650e-11,\n","         2.7673e-04, 4.6945e-07, 3.2050e-04, 1.4057e-10, 1.9337e-05, 3.2339e-09]])}, 140386461173656: {'step': 1, 'exp_avg': tensor([-0.2643,  0.1474, -0.1534,  0.0580,  0.0905,  0.1919,  0.0102, -0.0355, -0.0360, -0.0087]), 'exp_avg_sq': tensor([6.9850e-03, 2.1714e-03, 2.3519e-03, 3.3599e-04, 8.1928e-04, 3.6837e-03, 1.0364e-05, 1.2607e-04, 1.2995e-04,\n","        7.6395e-06])}}\n","param_groups \t [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [140386461173008, 140386461173080, 140386461173152, 140386461173224, 140386461173296, 140386461173368, 140386461173440, 140386461173512, 140386461173584, 140386461173656]}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4x602O_zmk8v","colab_type":"text"},"source":["### Saving and Loading the Parameters of a network\n","When saving a model for inference, it is only necessary to save the trained modelâ€™s learned parameters. Saving the modelâ€™s state_dict with the torch.save() function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\n","\n","A common PyTorch convention is to save models using either a .pt or .pth file extension.\n","\n","**Note:**  You must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results."]},{"cell_type":"markdown","metadata":{"id":"uwZOeTtnoKjz","colab_type":"text"},"source":["#### Saving the model's state_dict()"]},{"cell_type":"code","metadata":{"id":"H7pEiqcxmuj7","colab_type":"code","colab":{}},"source":["path = 'network_weights.pth'\n","\n","# Save the parameters\n","torch.save(network.state_dict(), path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSb9qaf9oTRH","colab_type":"text"},"source":["#### Loading the model's state_dict()\n"]},{"cell_type":"code","metadata":{"id":"4lph-8B0oIYw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"f3f03b3b-67db-44b6-cbf9-49287a8322a2","executionInfo":{"status":"ok","timestamp":1561180930559,"user_tz":-330,"elapsed":7992,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}}},"source":["# create a new model object \n","new_network = Network()\n","\n","# move the network to GPU\n","new_network.to(device)\n","\n","# load the network's parameters\n","new_network.load_state_dict(torch.load(path))\n","\n","# set the network into evaluate mode\n","new_network.eval()"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=192, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=60, bias=True)\n","  (out): Linear(in_features=60, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"markdown","metadata":{"id":"e3tEChF9ofQG","colab_type":"text"},"source":["**Note:**  You must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n","\n","### Saving and Loading entire Network\n","This save/load process uses the most intuitive syntax and involves the least amount of code. Saving a model in this way will save the entire module using Pythonâ€™s pickle module. The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. The reason for this is because pickle does not save the model class itself. Rather, it saves a path to the file containing the class, which is used during load time. Because of this, your code can break in various ways when used in other projects or after refactors."]},{"cell_type":"code","metadata":{"id":"C2B29orBogDu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"36164f48-a729-4117-c444-2271edb8b173","executionInfo":{"status":"ok","timestamp":1561181463367,"user_tz":-330,"elapsed":906,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}}},"source":["path = 'full_network.pth'\n","\n","# save the model\n","torch.save(network, path)\n","\n","# load the model\n","new_model = torch.load(path)\n","new_model.eval()"],"execution_count":97,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Network(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=192, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=60, bias=True)\n","  (out): Linear(in_features=60, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"7i3rymCIqwDF","colab_type":"text"},"source":["### Saving & Loading a General Checkpoint for Inference or to  Resuming Training\n","Here, we save different values as dictionary and later access them using keys"]},{"cell_type":"code","metadata":{"id":"iHJPfpn6q0iA","colab_type":"code","colab":{}},"source":["path = 'checkpoint.tar'\n","\n","checkpoint_dict = {\n","            'epoch': epoch,\n","            'model_state_dict': network.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }\n","\n","torch.save(checkpoint_dict, path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2julfkDVtHD-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"c92bff50-9ab5-4cb2-c993-b1098e43e3da","executionInfo":{"status":"ok","timestamp":1561182560153,"user_tz":-330,"elapsed":874,"user":{"displayName":"Abdur Rahman","photoUrl":"","userId":"05342928391784981660"}}},"source":["# create instances of model and optimizer\n","model = Network()\n","optimizer = optim.Adam(network.parameters(), lr = 0.01)\n","\n","checkpoint = torch.load(path)\n","\n","# access each parameter using keys\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","\n","# set the model into train mode for further training\n","model.train()\n","\n","#### or ####\n","\n","# set the model in evaluate mode\n","model.eval()"],"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=192, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=60, bias=True)\n","  (out): Linear(in_features=60, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"markdown","metadata":{"id":"7sgrOiDivp1L","colab_type":"text"},"source":["### Partial Restoring of weights as used in transfer learning\n","Whether you are loading from a partial state_dict, which is missing some keys, or loading a state_dict with more keys than the model that you are loading into, you can set the strict argument to False in the load_state_dict() function to ignore non-matching keys."]},{"cell_type":"code","metadata":{"id":"s2EWWPJDvpFK","colab_type":"code","colab":{}},"source":["PATH = 'network.pth'\n","\n","model_A = Network()\n","\n","torch.save(model_A.state_dict(), PATH)\n","\n","model_B = Network()\n","model_B.load_state_dict(torch.load(PATH), strict=False)"],"execution_count":0,"outputs":[]}]}